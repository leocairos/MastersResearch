evaluating robustness of cloud based systems 2015 chauvel et al various services are now available in the cloud ranging from turnkey databases and application servers to high level services such as continuous integration or source version control to stand out of this diversity robustness of service compositions is an important selling argument but which remains difficult to understand and estimate as it does not only depend on services but also on the underlying platform and infrastructure yet choosing a specific service composition may fail to deliver the expected robustness but reverting early choices may jeopardise the success of any cloud project inspired by existing models used in biology to quantify the robustness of ecosystems we show how to tailor them to obtain early indicators of robustness for cloud based deployments this technique helps identify weakest parts in the overall architecture and in turn mitigates the risk of having to revert key architectural choices we illustrate our approach by comparing the robustness of four alternative deployments of the sensapp application which includes a mongodb database four rest services and a graphical web front end cloud based systems failure sequences robustness indicators robustness metric sensitive components software deployment systems architectures continuous delivery of rhbmp2 and rhvegf165 at a certain ratio enhances bone formation in mandibular defects over the delivery of rhbmp2 alone an experimental study in rats 2015 published by elsevier b v the aim of the present study was to test the hypothesis that different amounts of vascular endothelial growth factor and bone morphogenic protein differentially affect bone formation when applied for repair of non healing defects in the rat mandible porous composite pdlla caco 3 carriers were fabricated as slow release carriers and loaded with rhbmp2 and rhvegf 165 in 10 different dosage combinations using gas foaming with supercritical carbon dioxide they were implanted in non healing defects of the mandibles of 132 adult wistar rats with additional lateral augmentation bone formation was assessed both radiographically bone volume and by histomorphometry bone density the use of carriers with a ratio of delivery of vegf bmp between 0 7 and 1 2 was significantly related to the occurrence of significant increases in radiographic bone volume and or histologic bone density compared to the use of carriers with a ratio of delivery of ≤ 0 5 when all intervals and all outcome parameters were considered moreover simultaneous delivery at this ratio helped to save rhbmp2 as both bone volume and bone density after 13 weeks were reached surpassed using half the dosage required for rhbmp2 alone it is concluded that the combined delivery of rhvegf 165 and rhbmp2 for repair of critical size mandibular defects can significantly enhance volume and density of bone formation over delivery of rhbmp2 alone it appears from the present results that continuous simultaneous delivery of rhvegf 165 and rhbmp2 at a ratio of approximately 1 is favourable for the enhancement of bone formation angiogenesis bone formation bone morphogenic protein polymer retarded delivery vascular endothelial growth factor the quality of your code is the quality of your brand and it s time to pay attention to software testing 2015 ieee competition in the automotive industry is intense and successful companies must constantly innovate by introducing new technology to differentiate and improve their brands as a result today s vehicles have evolved from a mechanical device into an integrated machine with embedded software powering performance in all major systems including engine control power train suspension braking and entertainment consider a few of these incredible statistics today s vehicles have more computer processing power than nasa s early spacecraft an average modern high end car has more lines of code than an f 35 joint strike fighter and finally many automobiles actually have an addressable ip address these technological improvements are driving brand success stories as modern consumers experiences are shaped more by the software than the hardware a strong brand can create significant value in the automotive industry as a focus on integrated technology helps to drive brand loyalty and value according to interbrand s 15th annual best global brands report the collective value of the automotive brands appearing on the global brands ranking increased 14 6 percent three out of the five top risers listed were from the automotive sector making the 15th annual report a record breaking one for the auto industry however statistics also show that more than 50 percent of auto recalls are now due to software bugs not mechanical issues with an industry average of 5 10 bugs per thousand lines of code the errors can pile up fast as automobiles evolve from mechanical to software devices automakers must rethink fundamental product development principles including moving from a sequential compartmentalized design process to a more agile approach with higher degrees of collaboration between self directed cross functional teams this paper will outline why software quality needs to be at the top of the list for automotive oems looking to preserve and elevate their brand status for quality to improve continuous integration and continuous software testing are a necessity while software testing has traditionally been viewed as a development expense we will explain that when developed properly software tests are an asset similar to source code that should provide value over the entire product life cycle well designed tests allow regressions to be caught prior to product release and lead to a reduction in branch damage and costs associated with product recalls road vehicles software quality testing software reliability software testing vehicle safety agile learning through continuous assessment 2015 ieee this work explores the impact of teaching and learning if the rate of learner engagement outside the classroom is continuously measured and available to the instructor and students we describe an ongoing implementation of a monitoring tool built within a software engineering continuous integration and testing ci test platform that integrates multiple streams of student activity and performance on yearlong junior software engineering projects the ci test platform allows for continuous and instantaneous feedback which we will use to inform student behavior change in the work in progress we describe the technology its impact on the teaching process for the instructor and preliminary results observing impacts on student engagement behavior agile continuous assessment project learning software engineering project courses with industrial clients copyright 2015 acm there is an acknowledged need for teaching realistic software development in project courses the design space for such courses is wide ranging from single semester to two semester courses from single client to multicustomer courses from local to globally distributed courses and from toy projects to projects with real clients the challenge for a nontrivial project course is how to make the project complex enough to enrich students software engineering experience yet realistic enough to have a teaching environment that does not unduly burden students or the instructor we describe a methodology for project courses that is realizable for instructors improves students skills and leads to viable results for industry partners in particular recent advances in release management and collaboration workflows reduce the effort of students and instructors during delivery and increase the quality of the deliverables to enable release and feedback management we introduce rugby an agile process model based on scrum that allows reacting to changing requirements to improve early communication we use tornado a scenario based design approach that emphasizes the use of informalmodels for the interaction between clients and students the combination of rugby and tornado allows students to deal with changing requirements produce multiple releases and obtain client feedback through the duration of the course we describe our experience with more than 300 students working on 40 projects with external clients over a 4 year period in the latest instance of our course the students have produced more than 7000 builds with 600 releases for eleven clients in an evaluation of the courses we found that the introduction of rugby and tornado significantly increased students technical skills especially with respect to software engineering usability engineering and configuration management as well as their nontechnical skills such as communication with the client teamwork presentation and demo management finally we discuss how other instructors can adapt the course concept agile methods communication models continuous delivery continuous integration executable prototypes feedback informal modeling prototyping release management scenario based design scrum unified process user involvement version control system a modular approach to collaborative development in an openstack testbed 2015 ieee cloudwave is an integrated project funded by the european commission whose aim is to provide a new powerful foundation for the development deployment and management of cloud based services cloudwave will develop a set of software components that have to be integrated into a single multi layer cloud stack based on openstack during the project a testbed environment allows project partners to exert full control over deployed componentry and collaborate on development among testbed requirements the most interesting ones include providing a flexible infrastructure capable of emulating several multi node cloud environments and enabling the automatic deployment of cloudwave artifacts into such environment during integration phases this paper provides a high level description of the testbed design and implementation focusing on the technologies exploited to meet such requirements cloud computing containers continuous integration integration testbed openstack virtual infrastructure infrastructure as runtime models towards model driven resource management 2015 ieee the importance of continuous delivery and the emergence of tools allowing to treat infrastructure configurations programmatically have revolutionized the way computing resources and software systems are managed however these tools keep lacking an explicit model representation of underlying resources making it difficult to introspect verify or reconfigure the system in response to external events in this paper we outline a novel approach that treats system infrastructure as explicit runtime models a key benefit of using such models run time representation is that it provides a uniform semantic foundation for resources monitoring and reconfiguration adopting models at runtime allows one to integrate different aspects of system management such as resource monitoring and subsequent verification into an unified view which would otherwise have to be done manually and require to use different tools it also simplifies the development of various self adaptation strategies without requiring the engineers and researchers to cope with low level system complexities biological system modeling cloud computing computational modeling monitoring object oriented modeling runtime virtual machining intent tests and release dependencies pragmatic recipes for source code integration 2015 ieee continuous integration of source code changes for example via pull request driven contribution channels has become standard in many software projects however the decision to integrate source code changes into a release is complex and has to be taken by a software manager in this work we identify a set of three pragmatic recipes plus variations to support the decision making of integrating code contributions into a release these recipes cover the isolation of source code changes contribution of test code and the linking of commits to issues we analyze the development history of 21 open source software projects to evaluate whether and to what extent those recipes are followed in open source projects the results of our analysis showed that open source projects largely follow recipes on a compliance level of 75 hence we conclude that the identified recipes plus variations can be seen as wide spread relevant best practices for source code integration evaluating the monolithic and the microservice architecture pattern to deploy web applications in the cloud 2015 ieee cloud computing provides new opportunities to deploy scalable application in an efficient way allowing enterprise applications to dynamically adjust their computing resources on demand in this paper we analyze and test the microservice architecture pattern used during the last years by large internet companies like amazon netflix and linkedin to deploy large applications in the cloud as a set of small services that can be developed tested deployed scaled operated and upgraded independently allowing these companies to gain agility reduce complexity and scale their applications in the cloud in a more efficient way we present a case study where an enterprise application was developed and deployed in the cloud using a monolithic approach and a microservice architecture using the play web framework we show the results of performance tests executed on both applications and we describe the benefits and challenges that existing enterprises can get and face when they implement microservices in their applications cloud computing continuous delivery iaas infrastructure as a services microservice architecture microservices paas platform as a service scalable applications service oriented architectures soa software architecture software engineering efficient regression testing based on test history an industrial evaluation 2015 ieee due to changes in the development practices at axis communications towards continuous integration faster regression testing feedback is needed the current automated regression test suite takes approximately seven hours to run which prevents developers from integrating code changes several times a day as preferred therefore we want to implement a highly selective yet accurate regression testing strategy traditional code coverage based techniques are not applicable due to the size and complexity of the software under test instead we decided to select tests based on regression test history we developed a tool the difference engine which parses and analyzes results from previous test runs and outputs regression test recommendations the difference engine correlates code and test cases at package level and recommends test cases that are strongly correlated to recently changed packages we evaluated the technique with respect to correctness precision recall and efficiency our results are promising on average the tool manages to identify 80 of the relevant tests while recommending only 4 of the test cases in the full regression test suite continuous integration industrial evaluation regression testing social testing a framework to support adoption of continuous delivery by small medium enterprises 2015 ieee continuous delivery cd represents a challenge for software test teams because of the continuous introduction of new features and feedback from customers we consider testing in a framework where users are encouraged to report defects through social or other incentive schemes using an enterprise dataset we address the question of which types of defects can best be found in the field allowing in house test resources to be refocused validation of these touch points ultimately interweaves both customer and business needs the proposed framework is one which can help small to medium software businesses which typically have limited resources to test and release software via cd 2015 2nd international conference on computer science computer engineering and social media cscesm 2015 the proceedings contain 38 papers the topics discussed include social media guidelines for maritime transport crises situations in the eu area modeling communicational and informational content in facebook with context specific methods the usage of social media in crisis communication grammatical inference for the construction of opening books a study on facility maintenance information composition and sophisticated technology utilization measures automated color image arrangement method for multiple peak image how a real time video solution can affect to the level of preparedness in situation centers fun academic cloud computing a framework to learn and play using cloud computing and social testing a framework to support adoption of continuous delivery by small medium enterprises continuously delivered periodically updated never changed studying an open source project s releases of code requirements and trace matrix 2015 ieee many open source software projects deliver code continuously how are the project s requirements updated what about the traceability information of those requirements to answer these questions this paper reports our initial analyses of the itrust medical care project s all publicly accessible releases the results show that as itrust releases two versions per year the code growth is smooth but the requirements growth experiences periodic mass updates the asynchronous evolving paces cause the rtm stagnant outdated and inaccurate our work provides concrete insights into what updates should be applied to the requirements and the rtm in the face of the code changes and illustrates the need for new ways to automatically keep requirements in sync over continuous release cycles continuous delivery itrust itrust just in time requirements software evolution trace matrix traceability oats an automated test system for openocd 2015 ieee this paper describes an industrial experience about the realization of an automated test system oats currently used during the openocd development process the experience has involved concepts about debuggers and software testing and the exploitation of continuous integration techniques in order to automatically build and test software releases in an industrial context continuous integration debugger intel galileo oats openocd software testing continuous architecture sustainable architecture in an agile and cloud centric world 2016 murat erder and pierre pureur published by elsevier inc all rights reserved continuous architecture provides a broad architectural perspective for continuous delivery and describes a new architectural approach that supports and enables it as the pace of innovation and software releases increases it departments are tasked to deliver value quickly and inexpensively to their business partners with a focus on getting software into end users hands faster the ultimate goal of daily software updates is in sight to allow teams to ensure that they can release every change to the system simply and efficiently this book presents an architectural approach to support modern application delivery methods and provide a broader architectural perspective taking architectural concerns into account when deploying agile or continuous delivery approaches the authors explain how to solve the challenges of implementing continuous delivery at the project and enterprise level and the impact on it processes including application testing software deployment and software architecture covering the application of enterprise and software architecture concepts to the agile and continuous delivery models explains how to create an architecture that can evolve with applications incorporates techniques including refactoring architectural analysis testing and feedback driven development provides insight into incorporating modern software development when structuring teams and organizations refactoring a shot in the dark ï¿½ 2015 ieee a study performed semistructured interviews of 12 seasoned software architects and developers at nine finnish companies its main goals were to find out how the practitioners viewed the role and importance of refactoring and how and when they refactored another goal was to see whether shortened cycle times and especially continuous deployment practices affected how and when refactoring was done the results paint a multifaceted picture with some common patterns the respondents considered refactoring to be valuable but had difficulty explaining and justifying it to management and customers refactoring often occurred in conjunction with the development of new features because it seemed to require a clear business need the respondents didn t use measurements to quantify the need for or impact of refactoring this article is part of a special issue on refactoring metrics refactoring software architecture software development software engineering the autologous platelet rich fibrin a novel approach in osseous regeneration after cystic enucleation a pilot study 2015 indian journal of dental research published by wolters kluwer medknow context the platelet rich fibrin prf is second generation platelet concentrate that has been widely used and researched for stimulation and acceleration of soft tissue and osseous healing its continuous delivery of growth factors and proteins mimic the need of physiological wound healing and regenerative tissue processes aims and objectives the aim of this study was to evaluate the efficacy of prf in osseous regeneration after enucleation of cystic lesions the objectives of this study were 1 to evaluate osseous regeneration radiographically with the use of prf in intrabony defects after cystic enucleation 2 to evaluate the degree of bone density in intrabony defects with the use of prf postoperatively after 1 st 3 rd and 6 th months subjects and methods 10 cases of cystic lesions were treated using prf after cystic enucleation follow up radiographs orthopantomogram were taken 1 st 3 rd and 6 th months postoperatively bone density was measured with grayscale histogram using adobe photoshop 7 0 software results the subsequent follow up examinations revealed progressive predictable and significant radiographic osseous regeneration conclusion the use of prf in management of cystic lesions seems to be a novel therapeutic approach promoting faster osseous regeneration within 6 months postoperatively however further study is required with larger sample size and with a control group cystic lesions histogram osseous regeneration platelet rich fibrin workshop preview of the 15th workshop on domain specific modeling dsm 2015 domain specific languages provide a viable and time tested solution for continuing to raise the level of abstraction and thus productivity beyond coding making systems development faster and easier when accompanied with suitable automated modeling tools and generators it delivers to the promises of continuous delivery and devops in domain specific modeling dsm the models are constructed using concepts that represent things in the application domain not concepts of a given programming language the modeling language follows the domain abstractions and semantics allowing developers to perceive themselves as working directly with domain concepts together with frameworks and platforms dsm can automate a large portion of software production this paper introduces domain specific modeling and describes the splash 2015 workshop to be held on 27 th of october in pittsburgh pa which is the 15 th anniversary of the event code generation domain specific languages metamodeling modeling languages runtime metric meets developer building better cloud applications using feedback 2015 acm a unifying theme of many ongoing trends in software engineering is a blurring of the boundaries between building and operating software products in this paper we explore what we consider to be the logical next step in this succession integrating runtime monitoring data from production deployments of the software into the tools developers utilize in their daily workflows i e ides to enable tighter feedback loops we refer to this notion as feedback driven development fdd this more abstract fdd concept can be instantiated in various ways ranging from ide plugins that implement feedback driven refactoring and code optimization to plugins that predict performance and cost implications of code changes prior to even deploying the new version of the software we demonstrate existing proof of concept realizations of these ideas and illustrate our vision of the future of fdd and cloud based software development in general further we discuss the major challenges that need to be solved before fdd can achieve mainstream adoption cloud computing continuous delivery feedback driven development software development synthesizing continuous deployment practices used in software development 2015 ieee continuous deployment speeds up the process of existing agile methods such as scrum and extreme programming xp through the automatic deployment of software changes to end users upon passing of automated tests continuous deployment has become an emerging software engineering process amongst numerous software companies such as facebook github netflix and rally software a systematic analysis of software practices used in continuous deployment can facilitate a better understanding of continuous deployment as a software engineering process such analysis can also help software practitioners in having a shared vocabulary of practices and in choosing the software practices that they can use to implement continuous deployment the goal of this paper is to aid software practitioners in implementing continuous deployment through a systematic analysis of software practices that are used by software companies we studied the continuous deployment practices of 19 software companies by performing a qualitative analysis of internet artifacts and by conducting follow up inquiries in total we found 11 software practices that are used by 19 software companies we also found that in terms of use eight of the 11 software practices are common across 14 software companies we observe that continuous deployment necessitates the consistent use of sound software engineering practices such as automated testing automated deployment and code review agile continuous delivery continuous deployment follow up inquiries industry practices internet artifacts proceedings 2015 agile conference agile 2015 the proceedings contain 16 papers the topics discussed include synthesizing continuous deployment practices used in software development stakeholder perceptions of the adoption of continuous integration a case study understanding digital cardwall usage visual management and blind software developers the prevalence of ux design in agile development processes in industry managing technical debt in software projects using scrum an action research explaining agility with a process theory of change lean cmmi an iterative and incremental approach to cmmi based process improvement agile communicators cognitive apprenticeship to prepare students for communication intensive software development predicting release time for open source software based on the generalized software reliability model and is agile portfolio management following the principles of large scale agile case study in finnish broadcasting company yle stakeholder perceptions of the adoption of continuous integration a case study 2015 ieee continuous integration is an important support mechanism for fast delivery of new features however its adoption in industry has often been problematic partly due to social challenges however there is little knowledge of the exact nature of the challenges and how different stakeholders perceive the need for and adoption of continuous integration in this paper we describe how the introduction of continuous integration was perceived by different stakeholders in a r amp d program at ericsson the case provided a rare opportunity to study the adoption of continuous integration in a large distributed organization we interviewed 27 stakeholders and found differing perceptions of continuous integration how suitable it is for the organization how adoption should be organized and whether it is possible to achieve sufficient quality through automated testing these differences of perception were mainly consequences of the geographic distribution based on the case study we propose three guidelines first understand that the product architecture has a significant effect on the adoption however do not let architectural problems keep you from implementing continuous integration second give the team members sufficient time to overcome the initial learning phase in the adoption third avoid centralizing competencies to individual sites and invest in cross site communication adoption case study continuous integration architecting for devops and continuous deployment development and operations devops in the context of continuous deployment cd have emerged as an attractive software development movement which tries to establish a strong connection between development and operations teams cd is defined as the ability to quickly put new releases into production we believe that devops cd brings new challenges for architects which considerably impacts both on their architectural design decisions and their organizational responsibilities we assert that there is an important and urgent need of sufficient research work to gain a deep understanding of how devops cd adoption can influence architecting architectural decision making processes and their outcomes in an organization this phd research is aimed at understanding and addressing new challenges for designing architectures for supporting devops in the context of cd continuous deployment devops software architecture multi perspective regression test prioritization for time constrained environments 2015 ieee test case prioritization techniques are widely used to enable reaching certain performance goals during regression testing faster a commonly used goal is high fault detection rate where test cases are ordered in a way that enables detecting faults faster however for optimal regression testing there is a need to take into account multiple performance indicators as considered by different project stakeholders in this paper we introduce a new optimal multi perspective approach for regression test case prioritization the approach is designed to optimize regression testing for faster fault detection integrating three different perspectives business perspective performance perspective and technical perspective the approach has been validated in regression testing of industrial mobile device systems developed in continuous integration the results show that our proposed framework efficiently prioritizes test cases for faster and more efficient regression fault detection maximizing the number of executed test cases with high failure frequency high failure impact and cross functional coverage compared to manual practice regression testing software testing test case prioritization model based performance evaluations in continuous delivery pipelines in order to increase the frequency of software releases and to improve their quality continuous integration ci systems became widely used in recent years unfortunately it is not easy to evaluate the performance of a software release in such systems one of the main reasons for this difficulty is often the lack of a test environment that is comparable to a production system performance models can help in this scenario by eliminating the need for a production sized environment building upon these capabilities of performance models we have introduced a model based performance change detection process for continuous delivery pipelines in a previous work this work presents an implementation of the process as plug in for the ci system jenkins continuous delivery palladio component model performance change detection performance evaluation 1st international workshop on quality aware devops qudos 2015 proceedings the proceedings contain 10 papers the topics discussed include a devops approach to integration of software components in an eu research project devops meets formal modeling in high criticality complex systems modeling multi tier enterprise applications behavior with design of experiments technique a proactive approach for runtime self adaptation based on queuing network fluid analysis model based performance evaluations in continuous delivery pipelines continuous deployment of multi cloud systems space4cloud a devops environment for multi cloud applications and filling the gap a tool to automate parameter estimation for software performance models devops meets formal modelling in high criticality complex systems quality is the cornerstone of high criticality systems since in case of failure not only major financial losses are at stake but also human lives formal methods that support model based development are one of the methodologies used to achieve correct by construction systems however these are often heavyweight and need a dedicated development process in our work we combine formal and agile software engineering approaches in particular we use event b and scrum to assure the quality and more rapid and flexible development since we identified that there are more prerequisites for a successful it project we use devops to embrace the development quality assurance and it operations in this paper we show how formal modelling can function within devops and thus promote vanous dimensions of quality and continuous delivery agile devops event b formal modelling scrum vehicle level continuous integration in the automotive industry 2015 acm embedded systems are omnipresent in the modern world this naturally includes the automobile industry where electronic functions are becoming prevalent in the automotive domain embedded systems today are highly distributed systems and manufactured in great numbers and variance to ensure correct functionality systematic integration and testing on the system level is key in software engineering continuous integration has been used with great success in the automotive industry though system tests are still performed in a big bang integration style which makes tracing and fixing errors very expensive and time consuming thus i want to investigate whether and how continuous integration can be applied to the automotive industry on the system level doing so i present an adapted process of continuous integration including methods for test case specification and selection i will apply this process as a pilot project in a production environment at bmw and evaluate the effectiveness by gathering both qualitative and quantitative data from the gained experience i will derive possible improvements to the process for future implementations and requirements on test hardware used for continuous integration automotive continuous integration embedded testing quality and productivity outcomes relating to continuous integration in github 2015 acm software processes comprise many steps coding is followed by building integration testing system testing deployment operations among others software process integration and automation have been areas of key concern in software engineering ever since the pioneering work of osterweil market pressures for agility and open decentralized software development have provided additional pressures for progress in this area but do these innovations actually help projects given the numerous confounding factors that can influence project performance it can be a challenge to discern the effects of process integration and automation software project ecosystems such as github provide a new opportunity in this regard one can readily find large numbers of projects in various stages of process integration and automation and gather data on various influencing factors as well as productivity and quality outcomes in this paper we use large historical data on process metrics and outcomes in github projects to discern the effects of one specific innovation in process automation continuous integration our main finding is that continuous integration improves the productivity of project teams who can integrate more outside contributions without an observable diminishment in code quality continuous integration github pull requests localising faults in test execution traces with the advent of agile processes and their emphasis on continuous integration automated tests became the promi nent driver of the development process when one of the thousands of tests fails the corresponding fault should be localised as quickly as possible as development can only pro ceed when the fault is repaired in this paper we propose a heuristic named speqtra which mines the execution traces of a series of passing and failing tests to localise the class which contains the fault speqtra produces ranking of classes that indicates the likelihood of classes to be at fault we compare our spectrum based fault localisation heuristic with the state of the art ample and demonstrate on a small yet representative case nanoxml that the ranking of classes proposed by speqtra is significantly better than the one of ample automated developer tests replication different heuristic amp same data spectrum based fault localisation distributed software engineering in collaborative research projects 2015 ieee collaborative research projects involve distributed construction of software prototypes as part of the project methodology a major challenge thereby is the need to establish a developer community that shall effectively and efficiently align development efforts with requirements offered by researchers and other stakeholders these projects are inherently different in nature compared to commercial software projects the literature offers little research on this aspect of software engineering in this paper we outline the challenges in this context and present a methodology for distributed software engineering in collaborative research projects the methodology covers all major aspects of the software engineering process including requirements engineering architecture issue tracking and social aspects of developer community building in collaborative projects the methodology can be tailored to different project contexts and may provide support in planning software engineering work in future projects collaborative research projects continuous integration development infrastructure distributed software engineering methodology open source software requirements engineering dyn tail dynamically tailored deployment engines for cloud applications 2015 ieee shortening software release cycles increasingly becomes a critical competitive advantage not exclusively for software vendors in the field of web applications mobile apps and the internet of things today s users customers and other stakeholders expect quick responses to occurring issues and feature requests devops and cloud computing are two key paradigms to enable rapid continuous deployment and delivery of applications utilizing automated software delivery pipelines however it is a highly complex and sophisticated challenge to implement such pipelines by installing configuring and integrating corresponding general purpose deployment automation tooling therefore we present a method in conjunction with a framework and implementation to dynamically generate tailored deployment engines for specific application stacks to deploy corresponding applications generated deployment engines are packaged in a portable manner to run them on various platforms and infrastructures the core of our work is based on generating apis for arbitrary deployment executables such as scripts and plans that perform different tasks in the automated deployment process as a result deployment tasks can be triggered through generated api endpoints abstracting from lower level technical details of different deployment automation tooling apification application topology cloud computing deployment deployment engine devops provisioning rondo a tool suite for continuous deployment in dynamic environments 2015 ieee driven by the emergence of new computing environments dynamically evolving software systems makes it impossible for developers to deploy software with human centric processes instead there is an increasing need for automation tools that continuously deploy software into execution in order to push updates or adapt existing software regarding contextual and business changes existing solutions fall short on providing fault tolerant reproducible deployments that can scale on heterogeneous environments in this paper we present rondo a tool suite that enables continuous deployment for dynamic service oriented applications at the center of these tools we propose a deterministic and idem potent deployment process we provide with rondo a deployment manager that implements this process and capable of conducting deployments and continuously adapting applications according to the changes in the current target platform the tool suite also includes a domain specific language for describing deployment requests we validate our approach in multiple projects for provisioning the platform as well as for installing applications and continuous reconfigurations continuous deployment dynamism service oriented computing poster improving cloud based continuous integration environments 2015 ieee we propose a novel technique for improving the efficiency of cloud based continuous integration development environments our technique identifies repetitive expensive and time consuming setup activities that are required to run integration and system tests in the cloud and consolidates them into preconfigured testing virtual machines such that the overall costs of test execution are minimized we create such testing machines by reconfiguring and opportunistically snapshotting the virtual machines already registered in the cloud cost flow flow constraints integration test linear programming snapshot system tests test driven development codeaware sensor based fine grained monitoring and management of software artifacts 2015 ieee current continuous integration ci tools although extensible can be limiting in terms of flexibility in particular artifact analysis capabilities available through plug in mechanisms are both coarse grained and centralized to address this limitation this paper introduces a new paradigm code aware for distributed and fine grained artifact analysis code aware is an ecosystem inspired by sensor networks consisting of monitors and actuators aimed at improving code quality and team productivity code ware s vision entails a the ability to probe software artifacts of any granularity and localization from variables to classes or files to entire systems b the ability to perform both static and dynamic analyses on these artifacts and c the ability to describe targeted remediation actions for example to notify interested developers through automated actuators we provide motivational examples for the use of code aware that leverage current ci solutions sketch the architecture of its underlying ecosystem and outline research challenges 3rd international workshop on release engineering releng 2015 2015 ieee release engineering deals with all activities inbetween regular development and actual usage of asoftware product by the end user i e integration build testexecution packaging and delivery of software although re search on this topic goes back for decades the increasing heterogeneity and variability of software products along withthe recent trend to reduce the release cycle to days or even hoursstarts to question some of the common beliefs and practicesof the field for example a project like mozilla firefox releasesevery 6 weeks generating updates for dozens of existing fire fox versions on 5 desktop 2 mobile and 3 mobile desktopplatforms each of which for more than 80 locales in this con text the international workshop on release engineering releng aims to provide a highly interactive forum for re searchers and practitioners to address the challenges of findsolutions for and share experiences with release engineering and to build connections between the various communities build system continuous delivery deployment integration packaging release engineering test execution architecting to ensure requirement relevance keynote twinpeaks workshop 2015 ieee research has shown that up to two thirds of features in software systems are hardly ever used or not even used at all this represents a colossal waste of r amp d resources and occurs across the industry on the other hand product management and many others work hard at interacting with customers building business cases and prioritizing requirements a fundamentally different approach to deciding what to build is required requirements should be treated as hypothesis throughout the development process and constant feedback from users and systems in the field should be collected to dynamically reprioritize and change requirements this requires architectural support beyond the current state of practice as continuous deployment split testing and data collection need to be an integral part of the architecture in this paper we present a brief overview of our research and industry collaboration to address this challenge data driven development requirements engineering software architecture crying wolf and meaning it reducing false alarms in monitoring of sporadic operations through pod monitor 2015 ieee when monitoring complex applications in cloud systems a difficult problem for operators is receiving false positive alarms this becomes worse when the system is sporadically being changed and upgraded due to the emerging continuous deployment practice other legitimate but sporadic maintenance operations such as log compression garbage collection and data reconstruction in distributed systems can also trigger false alarms consequently traditional baseline based anomaly detection and monitoring is less effective a normal but dangerous practice is to turn off normal monitoring during sporadic operations such as upgrade and maintenance in this paper we report on the use of the process context information of sporadic operations to suppress false positive alarms we use the context information both directly and in machine learning our experimental evaluation shows that 1 using process context directly improves the alarm precision up to 0 226 36 1 improvement 2 using process context trained machine learning models improves the precision rate up to 0 421 84 7 improvement alarm monitoring operation wait for it determinants of pull request evaluation latency on github 2015 ieee the pull based development model enabled by git and popularised by collaborative coding platforms like bit bucket gitorius and github is widely used in distributed software teams while this model lowers the barrier to entry for potential contributors since anyone can submit pull requests to any repository it also increases the burden on integrators i e members of a project s core team responsible for evaluating the proposed changes and integrating them into the main development line who struggle to keep up with the volume of incoming pull requests in this paper we report on a quantitative study that tries to resolve which factors affect pull request evaluation latency in github using regression modeling on data extracted from a sample of github projects using the travis ci continuous integration service we find that latency is a complex issue requiring many independent variables to explain adequately automatic testing complexity theory computational modeling data mining software engineering software quality experiences virtualizing a large scale test platform for multimedia applications copyright 2015 icst testing is an essential part of software development and many test platforms exist to facilitate the process test systems are scarce because especially scalability tests require many computational resources in this paper we show that these limitations can be overcome by migrating the test infrastructure into cloud environments concrete virtualization concepts for large scale testbeds are discussed using the example of nessee an emulation environment for testing distributed audio video conferencing applications furthermore we describe how the cloud migration allows us to better integrate the test runs of the platform into the work ow of software development cloud migration continuous integration nessee network emulation software test 5th international conference on innovative computing technology intech 2015 the proceedings contain 28 papers the topics discussed include hospital dietary planning system using constraint programming toward a student information system for sebha university libya distribution based ensemble for class imbalance learning the impact of social media networks on enhancing students performance in online learning systems extending postgresql to handle olxp workloads software components selection using the fuzzy set theory an application of augmented reality ar in the manipulation of fanuc 200ic robot cerebral abnormalities detection by region growing segmentation and knn classification scheduling in hybrid cloud to maintain data privacy understanding devops bridging the gap from continuous integration to continuous delivery point triangulation using graham s scan separable convolution gaussian smoothing filters on a xilinx fpga platform accurate and fast multi rate multicast scheme in wireless networks and ramses a robotic assistant and a mobile support environment for speech and language therapy enabling collaborative development in an open stack testbed the cloud wave use case 2015 ieee the cloud wave project embodies a challenging set of goals including the development of software components that have to be integrated into a single multi layer cloud stack based on open stack while cutting across the infrastructure as a service platform as a service and software as a service levels by targeting layer spanning issues such as feedback driven development and coordinated adaptation a devops ready test bed environment should allow project partners to exert full control over deployed compo entry and collaborate on development goals include providing a flexible infrastructure capable of emulating several multi node cloud environments as well as enabling the automatic deployment of cloud wave artifacts into such environment in order to simplify integration activities this paper takes a snapshot of the current situation with regards to the design and implementation of such a setup trying to gain relevant insight out of this effort cloud computing continuous integration devops integration testbed openstack virtual infrastructure proceedings 2nd international workshop on rapid continuous software engineering rcose 2015 the proceedings contain 8 papers the topics discussed include build waiting time in continuous integration an initial interdisciplinary literature review introducing continuous delivery of mobile apps in a corporate environment a case study continuous experimentation in the b2b domain a case study supporting continuous integration by code churn based test selection mashing up software issue management development and usage data continuous api design for software ecosystems towards post agile development practices through productized development infrastructure and rolling out a mission critical system in an agilish way reflections on building a large scale dependable information system for public sector variability middleware for multi tenant saas applications a research roadmap for service lines 2015 acm software product line engineering sple and variability enforcement techniques have been applied to run time adaptive systems for quite some years also in the context of multitenant software as a service saas applications the focus has been mainly on 1 the pre deployment phases of the development life cycle and 2 fine grained tenant level run time activation of specific variants however with upcoming trends such as devops and continuous delivery and deployment operational aspects become increasingly important in this paper we present our integrated vision on the positive interplay between sple and adaptive middleware for multi tenant saas applications focusing on the operational aspects of running and maintaining a successful saas offering this vision called service lines is based on and motivated by our experience and frequent interactions with a number of belgian saas providers we concretely highlight and motivate a number of operational use cases that require advanced variability support in middleware and have promising added value for the economic feasibility of saas offerings in addition we provide a gap analysis of what is currently lacking from the perspectives of variability modeling and management techniques and middleware support and as such sketch a concrete roadmap for continued research in this area models at run time multi tenant saas operational support run time variability service lines variability middleware ptfe membrane flow reactor for aerobic oxidation reactions and its application to alcohol oxidation 2015 american chemical society a tube in shell membrane flow reactor has been developed for aerobic oxidation reactions that permits continuous delivery of o inf 2 inf to a liquid phase reaction along the entire length of the flow path the reactor uses inexpensive o inf 2 inf permeable ptfe teflon tubing that is compatible with elevated pressures and temperatures and avoids hazardous mixtures of organic vapor and oxygen several polymeric materials were tested and ptfe exhibits a useful combination of low cost chemical stability and gas diffusion properties reactor performance is demonstrated in the aerobic oxidation of several alcohols with homogeneous cu tempo and cu abno catalysts tempo 2 2 6 6 tetramethyl 1 piperidinyl n oxyl and abno 9 azabicyclo 3 3 1 nonane n oxyl kinetic studies demonstrate regimes where the overall rate is controlled by the kinetics of the reaction or the transport of oxygen through the tube wall near quantitative product yields are achieved with residence times as low as 1 min a parallel multitube reactor enables higher throughput while retaining good performance finally the reactor is demonstrated with a heterogeneous ru oh inf x inf al inf 2 inf o inf 3 inf catalyst packed in the tubing microservices validation mjolnirr platform case study 2015 mipro microservice architecture is a cloud application design pattern that implies that the application is divided into a number of small independent services each of which is responsible for implementing of a certain feature the need for continuous integration of developed and or modified microservices in the existing system requires a comprehensive validation of individual microservices and their co operation as an ensemble with other microservices in this paper we would provide an analysis of existing methods of cloud applications testing and identify features that are specific to the microservice architecture based on this analysis we will try to propose a validation methodology of the microservice systems cloud computing microservices paas services oriented architecture testing validation officefloor using office patterns to improve software design copyright 2015 acm officefloor is a middleware framework that bases its design on the patterns occurring within an office re using office patterns within software provides two improvements to software design the first is improved performance tuning of applications operating within complex enterprise environments complex enterprise environments such as service oriented architecture require applications to interact with multiple downstream systems any one of these downstream systems may impact the application s performance the improved performance tuning is a result of the office patterns enabling the use of multiple thread pools to isolate the performance impacts of each of these downstream systems the second improvement is a middleware framework that enables early and continuous delivery of working code for agile methodologies the early and continuous delivery of working code is a result of the office patterns enabling inversion of control for methods the inversion of control for methods enables building applications bottom up this bottom up approach to building applications reduces the lead times involved with top down software designs and also reduces the refactoring due to top down software designs the reduced lead times and reduced refactoring enables early and continuous delivery of working code for agile methodologies inversion of control responsible thread pool task orchestration software metrics in students software development projects copyright 2015 acm a software metric is the measurement of a particular characteristic of a software or the measurement of a software project and process in this paper we study how student project managers and team members observe metrics like working hours number of test cases requirement statuses regular reporting and number of code commits and which metrics they consider most important the metrics that the teams reacted most often were team s working hours and requirements statuses these metrics were also considered the most useful by the project managers we propose a method to calculate defect rate of reporting metrics and combine this defect rate with the way the project teams used the metrics it seems that when the teams were motivated to use metrics observed them and reacted on base of the metrics they also had less defects on reporting the metrics continuous integration project management project metrics software metrics student projects unit testing a reusable automated acceptance testing architecture for microservices in behavior driven development 2015 ieee cloud computing and mobile cloud computing are reshaping the way applications are being developed and deployed due to their unique needs such as massive scalability guaranteed fault tolerance near zero downtime etc and also daunting challenges such as security reliability continuous deployment and update capability microservices architecture where application is composed of a set of independently deployable services is increasingly becoming popular due to its capability to address most of these needs and challenges in recent years the behavior driven development bdd has become one of the most popular agile software development processes and frequently used in microservices development the key to success of bdd is the executable acceptance tests that describe the expected behavior of a feature and its acceptance criteria in the form of scenarios using simple and business people readable syntax the reusability auditability and maintainability become some of the major concerns when bdd test framework is applied for each microservice repository and no previous research addresses these concerns in this paper we present a reusable automated acceptance testing architecture to address all these concerns behavior driven development executable automated acceptance testing functional testing gherkin microservice continuous delivery of composite solutions a case for collaborative software defined paas environments to help drive top line growth of their businesses the development and it organizations are under increasing pressure to create and deliver applications at ever faster paces the advent of cloud computing has not only lowered the cost of it operations but also enabled the notion of continuous delivery which promises to radically reduce frictions in devops processes and speed up the product delivery cycle with increased demand on functionality and feature we have also seen these applications becoming more sophisticated often integrating multiple modern programming models and techniques with the traditional n tier web application into a composite application this paper proposes an architectural blueprint for improved continuous delivery of these complex composite applications it treats a solution as a holistic entity comprised of application logic and software defined environment that the logic relies on it also proposes a collaborative approach to software defined platform as a service environment building this being an ongoing research project this paper also briefly describes prototype work in progress and thoughts on future directions cloud computing continuous delivery platform as a service software defined environment solution lifecycle bigsystem 2015 proceedings of the 2nd international workshop on software defined ecosystems part of hpdc 2015 the proceedings contain 5 papers the topics discussed include continuous delivery of composite solutions a case for collaborative software defined paas environments a framework for realizing software defined federations for scientific workflows redefining data locality for cross data center storage xos an extensible cloud operating system and virtual fabric based approach for virtual data center network online tracking of interventional devices for endovascular aortic repair 2015 cars purpose the continuous integration of innovative imaging modalities into conventional vascular surgery rooms has led to an urgent need for computer assistance solutions that support the smooth integration of imaging within the surgical workflow in particular endovascular interventions performed under 2d fluoroscopic or angiographic imaging only require reliable and fast navigation support for complex treatment procedures such as endovascular aortic repair despite the vast variety of image based guide wire and catheter tracking methods an adoption of these for detecting and tracking the stent graft delivery device is not possible due to its special geometry and intensity appearance methods in this paper we present for the first time the automatic detection and tracking of the stent graft delivery device in 2d fluoroscopic sequences on the fly the proposed approach is based on the robust principal component analysis and extends the conventional batch processing towards an online tracking system that is able to detect and track medical devices on the fly results the proposed method has been tested on interventional sequences of four different clinical cases in the lack of publicly available ground truth data we have further initiated a crowd sourcing strategy that has resulted in 200 annotations by unexperienced users 120 of which were used to establish a ground truth dataset for quantitatively evaluating our algorithm in addition we have performed a user study amongst our clinical partners for qualitative evaluation of the results conclusions although we calculated an average error in the range of nine pixels the fact that our tracking method functions on the fly and is able to detect stent grafts in all unfolding stages without fine tuning of parameters has convinced our clinical partners and they all agreed on the very high clinical relevance of our method computer assisted interventions frangi filter image guided interventions instrument tracking online tracking robust pca stent graft tracking a distributed test system architecture for open source iot software 2015 acm in this paper we discuss challenges that are specific to testing of open iot software systems the analysis reveals gaps compared to wireless sensor networks as well as embedded software we propose a testing framework which a supports continuous integration techniques b allows for the integration of project contributors to volunteer hardware and software resources to the test system and c can function as a permanent distributed plugtest for network interoperability testing the focus of this paper lies in open source iot development but many aspects are also applicable to closed source projects interoperability open source iot test system architecture simplified negative pressure wound therapy clinical evaluation of an ultraportable no canister system 2013 the authors the aim of this study was to evaluate a prototype negative pressure wound therapy npwt system that has been developed to simplify npwt for wounds at the lower end of the acuity scale the new device has a single preset pressure of 80mmhg is single use and operates without an exudate canister the disposable npwt system pico™ was tested in a prospective non comparative multicentre clinical trial to assess device functionality and clinical acceptance twenty patients were recruited for a maximum treatment period of 14days the npwt devices were fitted with data log chips to enable longitudinal assessment of negative pressure and leak rates during therapy sixteen 80 patients had closed surgical wounds two 10 patients had traumatic wounds and two 10 patients received meshed split thickness skin grafts the mean study duration was 10·7days range 5 14days and the mean dressing wear time per individual patient was 4·6days range 2 11 fifty five percent of wounds had closed by the end of the 14 day study or earlier with a further 40 of wounds progressing to closure real time pressure monitoring showed continuous delivery of npwt three cases are discussed representing different wound locations and different patient factors that can increase the risk of post surgical complications clinical studies of the disposable npwt system confirmed the ability of the simplified single use device to function consistently over the expected wear time the anticipated reduced costs ease of use and increased mobility of patients using this system may enable npwt benefits to be available to a greater proportion of patients closed surgical incision negative pressure wound therapy portable postoperative care wound healing the highways and country roads to continuous deployment 2015 ieee as part of a finnish research program researchers interviewed 15 information and communications technology companies to determine the extent to which the companies adopted continuous deployment they also aimed to find out why continuous deployment is considered beneficial and what the obstacles are to its full adoption the benefits mentioned the most often were the ability to get faster feedback the ability to deploy more often to keep customers satisfied and improved quality and productivity despite understanding the benefits none of the companies adopted a fully automatic deployment pipeline the companies also had higher continuous deployment capability than what they practiced in many cases they consciously chose to not aim for full continuous deployment obstacles to full adoption included domain imposed restrictions resistance to change customer desires and developers skill and confidence continuous delivery continuous deployment continuous integration software development software engineering thematic analysis achieving reliable high frequency releases in cloud environments 2015 ieee continuous delivery and deployment are dramatically shortening release cycles from months to hours cloud applications with high frequency releases often rely heavily on automated tools and cloud infrastructure apis to deploy new software versions the authors report on reliability issues and how these tools and apis contribute to them they also analyze the trade offs between using heavily baked and lightly baked virtual image approaches on the basis of experiments with amazon web service opsworks apis and the chef configuration management tool finally they propose error handling practices for continuous delivery facilities continuous delivery continuous deployment devops release engineering software engineering system administration the practice and future of release engineering a roundtable with three release engineers 2015 ieee three release engineers share their perspectives on quality metrics for releases and on continuous delivery s benefits and limitations they also discuss release engineering job skills the required mind set the role of education and cultural change and they recommend future research areas the web extra at http youtu be o3cjqtzxai8 is an audio recording of davide falessi speaking with guest editors bram adams and foutse khomh about release engineering and its value to the software industry continuous delivery release engineering software development software engineering continuous delivery huge benefits but challenges too 2015 ieee continuous delivery cd has emerged as an auspicious alternative to traditional release engineering promising to provide the capability to release valuable software continuously to customers paddy power has been implementing cd for the past two years this article explains why paddy power decided to adopt cd describes the resulting cd capability and reports the huge benefits and challenges involved these experiences can provide fellow practitioners with insights for their adoption of cd and the identified challenges can provide researchers valuable input for developing their research agendas continuous delivery continuous software engineering devops release engineering software deployment software engineering the modern cloud based platform 2015 ieee in this excerpt from software engineering radio stefan tilkov talks with adrian cockcroft about architecture development and operations that make the most out of cloud based offerings with cockcroft sharing his experience at netflix adrian cockcroft cloud continuous delivery devops infrastructure paas platform as a service se radio software engineering software engineering radio windows principles for engineering iot cloud systems 2014 ieee engineering internet of things iot and cloud services to provide a coherent software layer for continuous deployment provision and execution of applications for various domains is complex the authors consider whether iot cloud systems could provide a uniform layer to enable continuous execution of complex applications consisting of diverse types of software components although these systems are built by integrating and blending iot infrastructures with cloud based datacenters the authors analyze requirements and perspectives for engineering such iot cloud systems and discuss the main engineering principles that should be supported they then highlight seven main principles covering different development and operation phases of iot cloud systems to show the importance and feasibility of these principles they present some of their recent work in providing concepts and tools for iot cloud systems cloud cloud computing elasticity internet of things software engineering nephila clavipes spiders araneae nephilidae keep track of captured prey counts testing for a sense of numerosity in an orb weaver 2014 springer verlag berlin heidelberg nephila clavipes golden orb web spiders accumulate prey larders on their webs and search for them if they are removed from their web spiders that lose larger larders i e spiders that lose larders consisting of more prey items search for longer intervals indicating that the spiders form memories of the size of the prey larders they have accumulated and use those memories to regulate recovery efforts when the larders are pilfered here we ask whether the spiders represent prey counts i e numerosity or a continuous integration of prey quantity mass in their memories we manipulated larder sizes in treatments that varied in either prey size or prey numbers but were equivalent in total prey quantity mass we then removed the larders to elicit searching and used the spiders searching behavior as an assay of their representations in memory searching increased with prey quantity larder size and did so more steeply with higher prey counts than with single prey of larger sizes thus nephila spiders seem to track prey quantity in two ways but to attend more to prey numerosity we discuss alternatives for continuous accumulator mechanisms that remain to be tested against the numerosity hypothesis and the evolutionary and adaptive significance of evidence suggestive of numerosity in a sit and wait invertebrate predator cognitive ecology counting food hoarding nephila web spider research opportunities in continuous delivery reflections from two years experiences in a large bookmaking company 2015 ieee we have been implementing continuous delivery in paddy power a large organization in the bookmaking industry for more than two years in this talk i will reflect on our journey to continuous delivery and discuss the research opportunities i see agile software development continuous delivery continuous deployment continuous software engineering devops release engineering shorter feedback loops by means of continuous deployment springer international publishing switzerland 2015 gathering early feedback on features is critical to many projects many agile methodologies define feedback loops often the feedback loop for completed features only closes after the iteration finishes in this paper we will introduce a way of closing this feedback loop early by means of continuous deployment this also lowers the deployment effort for developers increasing their happiness bdd continuous deployment continuous integration hot compatibility short feedback loops tdd testing zero downtime deployment model based continuous integration testing of responsiveness of web applications proceedings 3rd international workshop on release engineering releng 2015 the proceedings contain 9 papers the topics discussed include release engineering as a force multiplier research opportunities in continuous delivery towards definitions for release engineering and devops securing a deployment pipeline performance of defect prediction in rapidly evolving software predicting field reliability continuous deployment and schema evolution in sql databases extracting configuration knowledge from build files with symbolic analysis and continuous delivery with jenkins understanding devops amp bridging the gap from continuous integration to continuous delivery 2015 ieee as part of agile transformation in past few years we have seen it organizations adopting continuous integration principles in their software delivery lifecycle which has improved the efficiency of development teams with the time it has been realized that this optimization as part of continuous integration alone is just not helping to make the entire delivery lifecycle efficient or is not driving the organization efficiency unless all the pieces of a software delivery lifecycle work like a well oiled machine efficiency of organization to optimize the delivery lifecycle can not be met this is the problem which devops tries to address this paper tries to cover all aspects of devops applicable to various phases of sdlc and specifically talks about business need ways to move from continuous integration to continuous delivery and its benefits continuous delivery transformation in this paper is explained with a real life case study that how infrastructure can be maintained just in form of code iaac finally this paper touches upon various considerations one must evaluate before adopting devops and what kind of benefits one can expect continuous delivery continuous integration devops infrastructure as a code iaac towards architecting for continuous delivery 2015 ieee continuous delivery cd has emerged as an auspicious software development discipline with the promise of providing organizations the capability to release valuable software continuously to customers our organization has been implementing cd for the last two years thus far we have moved 22 software applications to cd i observed that cd has created a new context for architecting these applications in this paper i will try to characterize such a context of cd explain why we need to architect for cd describe the implications of architecting for cd and discuss the challenges this new context creates this information can provide insights to other practitioners for architecting their software applications and provide researchers with input for developing their research agendas to further study this increasingly important topic architecturally significant requirements continuous delivery continuous deployment continuous software engineering devops non functional requirements quality attributes software architecture automated testing in the continuous delivery pipeline a case study of an online company 2015 ieee companies running an online business need to be able to frequently push new features and bug fixes from development into production successful high performance online companies deliver code changes often several times per day their continuous delivery model supports the business needs of the online world at the same time however such practices increase the risk of introducing quality issues and unwanted side effects rigorous test automation is therefore a key success factor for continuous delivery in this paper we describe how automated testing is used in the continuous delivery pipeline of an austrian online business company the paper illustrates the complex technical and organizational challenges involved and summarizes the lessons from more than six years of practical experience in establishing and maintaining an effective continuous delivery pipeline automated testing continuous delivery continuous integration continusous deployment dev ops paradigm shift from large releases to continuous deployment of software designing a reference model for continuous deployment springer international publishing switzerland 2015 continuous deployment cd is an essential method as software development companies move towards real time business and continuous experiments powered by the lean and agile methods cd aims for continuous deployment of valuable software this doctoral research investigates what it will take to enable cd the findings will be collected to generate a cd reference model the research is initiated by studying existing literature and models for organisational assessment in relation to lean and agile approaches next the focus is sharpened to capabilities that are required for enabling cd in information and communication technologies ict industry the research will apply literature reviews case studies and the design science research dsr framework agile assessment continuous deployment lean real time business reference model software development introducing continuous delivery of mobile apps in a corporate environment a case study 2015 ieee software development is conducted in increasingly dynamic business environments organizations need the capability to develop release and learn from software in rapid parallel cycles the abilities to continuously deliver software to involve users and to collect and prioritize their feedback are necessary for software evolution in 2014 we introduced rugby an agile process model with workflows for continuous delivery and feedback management and evaluated it in university projects together with industrial clients based on rugby s release management workflow we identified the specific needs for project based organizations developing mobile applications varying characteristics and restrictions in projects teams in corporate environments impact both process and infrastructure we found that applicability and acceptance of continuous delivery in industry depend on its adaptability to address issues in industrial projects with respect to delivery process infrastructure neglected testing and continuity we extended rugby s workflow and made it tailor able eight projects at cap gemini a global provider of consulting technology and outsourcing services applied a tailored version of the workflow the evaluation of these projects shows anecdotal evidence that the application of the workflow significantly reduces the time required to build and deliver mobile applications in industrial projects while at the same time increasing the number of builds and internal deliveries for feedback agile methods configuration management continuous delivery continuous integration release management software evolution user feedback user involvement test orchestration a framework for continuous integration and continuous deployment 2015 ieee theenterprises follow agile software development methodology to because business requirements changes frequently inagile software developmentmethodology it is essential to continuously integrate the component into a main trunk of a project to test the new component of the system then test all the component of the project this happens frequently so it needs to streamlineprocesses to orchestrate the tests so it is difficult to manage the software development life cycle for those changes and maintain the software code quality to maintain the product quality it is essential to integrate the product component and need to deploy a product on pre production environment and test the product hence the need for continuous integration and continuous delivery process for software product the popularization of devops and cloud computing has revolutionized the software delivery process making it faster and affordable for business to release their software continuously hence enterprises need for reliable and predictable delivery process of software the objective of the paper is to design an effective framework for automated testing and deployment to help to automate the code analysis test selection test scheduling environment provisioning test execution results analysis and deployment pipeline test orchestration framework typically very complicated to develop such pipeline to make software reliable and bug free for environment provisioning can be provided through virtualization and cloud computing agile software development cloud computing continious deployment cd continuousintegration ci delivery pipeline devops test oracle test orchestration mashing up software issue management development and usage data 2015 ieee modern software development approaches rely extensively on tools motivated by practices such as continuous integration deployment and delivery these tools are used in a fashion where data are automatically accumulated in different databases as a side effect of everyday development activities in this paper we introduce an approach for software engineering data visualization as a mash up that combines data from issue management software development and production use the visualization can show to all stake holders how well continuous delivery is realized in the project the visualization clearly shows the time spent to specify and develop the features as well the length of the delivery cycle further more the visualization shows how much work is unfinished and waiting for delivery this can help the development team to decrease the amount of unfinished work and by that help them to keep up in continuous delivery mind set in addition to development data usage of the features is also visualized continuous delivery information visualization software analytics defining metrics for continuous delivery and deployment pipeline continuous delivery is a software development practice where new features are made available to end users as soon as they have been implemented and tested in such a setting a key technical piece of infrastructure is the development pipeline that consists of various tools and databases where features flow from development to deployment and then further to use metrics unlike those conventionally used in software development are needed to help define the performance of the development pipeline in this paper we address metrics that are suited for supporting continuous delivery and deployment through a descriptive and exploratory single case study on a project of a mid sized finnish software company solita plc as concrete data we use data from project lupapiste a web site for managing municipal authorizations and permissions agile measurements continuous deployment lean software development a practical approach to software continuous delivery focused on application lifecycle management copyright 2015 by ksi research inc and knowledge systems institute graduate school to deliver quality software continuously is a challenge for many organizations it is due to factors such as configuration management source code control peer review delivery planning audits compliance continuous integration testing deployments dependency management databases migration creation and management of testing and production environments traceability and data post fact integrated management process standardization among others to overcome these challenges this paper presents a continuous delivery process that promotes artefacts produced by developers in a managed fashion to production environment allowing bidirectional traceability between requirements and executables and integrating all activities of software development using the concepts of application lifecycle management as a result we obtained an ecosystem of tools and techniques put into production in order to support this process 1 application lifecycle management continuous delivery process quality transitioning towards continuous delivery in the b2b domain a case study springer international publishing switzerland 2015 delivering value to customers in real time requires companies to utilize real time deployment of software to expose features to users faster and to shorten the feedback loop this allows for faster reaction and helps to ensure that the development is focused on features providing real value continuous delivery is a development practice where the software functionality is deployed continuously to customer environment although this practice has been established in some domains such as b2c mobile software the b2b domain imposes specific challenges this article presents a case study that is conducted in a medium sized software company operating in the b2b domain the objective of this study is to analyze the challenges and benefits of continuous delivery in this domain the results suggest that technical challenges are only one part of the challenges a company encounters in this transition the company must also address challenges related to the customer and procedures the core challenges are caused by having multiple customers with diverse environments and unique properties whose business depends on the software product some customers require to perform manual acceptance testing while some are reluctant towards new versions by utilizing continuous delivery it is possible for the case company to shorten the feedback cycles increase the reliability of new versions and reduce the amount of resources required for deploying and testing new releases b2b case study continuous delivery continuous deployment development process continuous delivery with jenkins jenkins solutions to implement continuous delivery 2015 ieee this paper illustrates how jenkins evolved from being a pure continuous integration platform to a continuous delivery one embracing the new design tendency where not only the build but also the release and the delivery process of the product is automated in this scenario jenkins becomes the orchestrator tool for all the teams roles involved in the software lifecycle thanks to which development quality assurance and operations teams can work closely together goal of this paper is not only to position jenkins as hub for cd but also introduce the challenges that still need to be solved in order to strengthen jenkins tracking capabilities continuous integration using cloud computing 2015 aisti nowadays systems can evolve quickly and to this growth is associated for example the addiction of new features or even the change of system perspective required by the stakeholders consequently these cause an increase in the number of developed tests run a large battery of tests sequentially can take hours however tests can run faster in a distributed environment with rapid availability of pre configured systems such as cloud computing this paper pretends to demonstrate some scenarios on the implementation of the practice continuous integration ci in information systems with software tests performed on cloud computing the main goal is explore the most of capacities that ci practice gives for automating the build and testing in a information system architecture build cloud computing continuous integration deployment software testing continuous delivery with visual studio alm 2015 2015 by mathias olausson and jakob ehn this book is the authoritative source on implementing continuous delivery practices using microsoft s visual studio and tfs 2015 microsoft mvp authors mathias olausson and jakob ehn translate the theory behind thismethodology and show step by step how to implement continuous delivery in a real world environment building good software is challenging building high quality software on a tight schedule can be close to impossible continuous delivery is an agile and iterative technique that enables developers to deliver solid working software in every iteration continuous delivery practices help it organizations reduce risk and potentially become as nimble agile and innovative as startups in this book you ll learn • what continuous delivery is and how to use it to create better software more efficiently using visual studio 2015 • how to use team foundation server 2015 and visual studio online to plan design and implement powerful and reliable deployment pipelines • detailed step by step instructions for implementing continuous delivery on a real project improved software production for the lhc tunnel cryogenics control system 2015 published by elsevier b v the software development for the control system of the cryogenics in the lhc is partially automatized however every single modification requires a sequence of consecutive and interdependent tasks to be executed manually by software developers a large number of control system consolidations and the evolution of the used it technologies lead to reviewing the software production methodology as a result an open source continuous integration server has been employed integrating all development tasks tools and technologies this paper describes the main improvements that have been made to fully automate the process of software production and the achieved results cern continuous integration control cryogenics development industrial lhc software virtual to the near end using virtual platforms for continuous integration 2015 acm continuous integration ci is a hot topic in software development today ci is a critical enabler for agile methods and higher software development velocity and productivity however adopting the practice of continuous integration can be difficult especially when developing software for embedded systems practices such as agile and continuous integration are designed to enable engineers to constantly improve and update their products however these processes can break down without access to the target system a way to collaborate with other teams and team members and the ability to automate tests this paper outlines how simulation can enable teams to more effectively manage their integration and test practice using virtual platforms as a key part of the test setup and simulation as a key part of the test strategy agile continuous integration simulated hardware simulation simulator integration tlm transactionlevel simulation virtual platform model based approach for implementation of software configuration management process copyright 2015 scitepress science and technology publications software configuration management is a discipline that controls software evolution process nowadays this process is not only challenge to choose the best version control system o branching strategy for particular project together with source code management the following tasks should be solved continuous integration continuous delivery release management build management etc usually software development companies already have a set of tools to support mentioned processes the main challenge is to adopt this solutions to new projects as soon as possible with minimum efforts of manual steps the article provides new model driven approach to increase reuse of existing solutions in configuration management area in order to illustrate the approach there were developed new meta models that are purposed for development of different configuration management models in context of a model driven approach this article provides a simplified example to illustrate models and defines further researches model driven approach models software configuration management a quality framework for software continuous integration 2015 the authors the research in this paper combines two main areas the first one is software quality and the second is the agile practices of continuous integration software quality has been an important topic since the beginning of the software development and production many researches have been conducted to discuss how the quality of software is a critical factor to its success 1–5 because software became an important part of almost every task in our daily life having high quality software that meets the users expectations is important 6 software integration is a stage in every software development lifecycle it is defined as the process to assemble the software components and produce a single product it has been shown that software integration and integration testing can make more than 40 of the overall project cost so it is important that they are done efficiently and easily to be able to manage the involved risks 7 a software engineering practice called continuous integration ci was introduced by kent beck and ron jeffries to mitigate the risks of software integration enhance its process and improve its quality 8 in this research the principles of ci are identified and applied to a case study in order to analyze their impact on the software development process quality factors agile continuous integration extreme programming iso software development software quality framework continuous delivery – from concept to product trade offs in effectiveness and efficiency springer international publishing switzerland 2015 the implementation and release of software products has progressed from a lengthy delivery cycle – the methodical sequential path of big bang waterfall product delivery – to the rapid iterative release cycle supported by agile practices recently continuous delivery has emerged as a strategy to accelerate product availability however only a systematic automation of the build test and deployment processes in concert with superbly coordinated teams of software practitioners and business partners makes make this possible however trade offs in the optimization of process may act to limit the innovativeness of product output this panel will discuss approaches challenges risks and strategies for using continuous delivery to competitive advantage agile automation delivery innovation processes waterfall applying continuous integration for reducing web applications development risks 2015 ieee in order to project resource manage and time control large or complex software system need be modularized software system is decomposed into subsystems functional modules and basis components finally all tested components have to integrate to be the complete system applying iid interactive incremental development mechanism agile development model becomes the practical method to reduce software project failure rate continuous integration ci is an iid implementation concept which can effectively reduce software development risk web app with high change characteristic is suitable to use agile development model as the development methodology the paper depth surveys ci operating environment and advantages introducing ci concept can make up the development change impacts of web app for this the paper proposes a web applications continuous integration procedure wacip to assist the system integration operating wacip with ci makes web app can be deployed quickly increase stakeholder communication frequency improve staff morale and effectively reduce web app development risks agile process continuous integration development risks integration test web app patterns for continuous integration builds in cross platform agile software development cross platform software development poses challenges to agile development teams in practicing continuous integration ci builds not only because such builds take a longer time to complete and are more likely to fail but also because builds of different lengths and scopes must be available depending on the working circumstances to deal with this situation three aspects of build automation in ci the structuring of source code modules the management of intermediate and final build artifacts and the execution of builds must be re considered to account for the cross platform characteristics this paper discovers and documents a collection of ten patterns of ci builds for use in developing cross platform software in the three aspects re considered these patterns are distilled from known uses of builds in existing software and from our experience in building commercial and open source cross platform software as illustrated with an example adapted from the development of a real world commercial cross platform software product the patterns can be effectively applied to solve many commonly encountered problems in applying ci for agile cross platform software development agile development continuous integration cross platform software pattern pattern language software build build waiting time in continuous integration an initial interdisciplinary literature review 2015 ieee in this position paper we present and demonstrate the idea of using an interdisciplinary literature review to accelerate the research on continuous integration practice a common suggestion has been that build waiting time in continuous integration cycle should be less than 10 minutes this guideline is based on practitioners opinion and has not been further investigated the objective of this study is to understand the effects of build waiting time in software engineering and to get input from waiting time research in other disciplines the objective is met by performing two literature reviews first on build waiting time and second on waiting times in the contexts of service operation web use and computer use the found effects of build waiting time were categorized into continuous integration specific cognitive and emotional two minute build waiting time was considered optimal but under 10 minutes was considered acceptable insight from other waiting time research suggests that the perceptions of waiting time are important and the perceptions can be lowered by providing feedback and giving developers other activities during the integration build waiting time continuous integration literature review dimensions of devops springer international publishing switzerland 2015 devops has been identified as an important aspect in the continuous deployment paradigm in practitioner communities and academic research circles however little has been presented to describe and formalize what it constitutes the absence of such understanding means that the phenomenon will not be effectively communicated and its impact not understood in those two communities this study investigates the elements that characterize the devops phenomenon using a literature survey and interviews with practitioners actively involved in the devops movement four main dimensions of devops are identified collaboration automation measurement and monitoring an initial conceptual framework is developed to communicate the phenomenon to practitioners and the scientific community as well as to facilitate input for future research agile continuous deployment devops software deployment financial measuring of incremental deliveries in software projects finding a model that can answer how much is worth to split a project into iterations 2015 ieee in this article it is presented a mathematical model for the additional financial value generated in software projects as a function of the number of incremental deliveries during the project based on this model it was presented the theoretical asymptotic limit for infinite deliveries during the project this practice is called continuous deployment cd already used by many companies such as google and facebook as financial evaluation measuring it was used the net present value npv it was also measured how the use of incremental deliveries can create additional npv for software projects which have short duration maximum of six months median duration one year and long duration two years at the end it is argued that this additional value can be strategic for maintaining a sustainable portfolio of innovation projects continuous delivery incremental delivery net present value npv project portfolio supporting continuous integration by code churn based test selection 2015 ieee continuous integration promises advantages in large scale software development by enabling software development organizations to deliver new functions faster however implementing continuous integration in large software development organizations is challenging because of organizational social and technical reasons one of the technical challenges is the ability to rapidly prioritize the test cases which can be executed quickly and trigger the most failures as early as possible in our research we propose and evaluate a method for selecting a suitable set of functional regression tests on system level the method is based on analysis of correlations between test case failures and source code changes and is evaluated by combining semi structured interviews and workshops with practitioners at ericsson and axis communications in sweden the results show that using measures of precision and recall the test cases can be prioritized the prioritization leads to finding an optimal test suite to execute before the integration 16th international conference on agile software development xp 2015 the proceedings contain 49 papers the special focus in this conference is on agile processes in software engineering and extreme programming the topics include an industrial case study on test cases as requirements key challenges in early stage software startups current practices and future needs functional size measures and effort estimation in agile development software development as an experiment system coordinating expertise outside agile teams a definition and perceived adoption impediments scaling kanban for software development in a multisite organization the two faces of uncertainty management ambidexterity towards introducing agile architecting in large companies agile and the global software leaders shorter feedback loops by means of continuous deployment applying agile and lean elements to accelerate innovation culture in a large organization organizational culture aspects of an agile transformation building learning organization through peer hands on support community and gamification learning from disaster and experience practical applications of the agile fluency model improving processes by integrating agile practices assurance case integration with an agile development method paradigm shift from large releases to continuous deployment of software agility in dynamic environments applying randori style kata and agile practices to an undergraduate level programming class continuous strategy process in the context of agile and lean software development and automatizing android unit and user interface testing extending software repository hosting to code review and testing we will describe how cern s services around issue tracking and version control have evolved and what the plans for the future are we will describe the services main design integration and structure giving special attention to the new requirements from the community of users in terms of collaboration and integration tools and how we address this challenge when defining new services based on gitlab for collaboration to replace our current gitolite service and code review and jenkins for continuous integration these new services complement the existing ones to create a new global development tool stack where each working group can place its particular development work flow a prologue of jenkins with comparative scrutiny of various software integration tools 2015 ieee software configuration tools are becoming popular day by day in this paper we describe an open source continuous integration tool jenkins which is on the whole a server oriented arrangement that runs in a servlet like container like apache tomcat it supports various source control management scm tools including subversion mercurial perforce clear case and rational team concert rtc the design functionality and usage of jenkins are presented in this paper the aim of this research paper is to emphasize on the jenkins integration development environment ide and evaluate and compare five software integration tools to determine their usability and effectiveness integration tools jenkins software configuration towards post agile development practices through productized development infrastructure 2015 ieee modern software is developed to meet evolving customer needs in a timely fashion the need for a rapid time to market together with changing requirements has led software intensive companies to utilize agile development where each iteration aims at producing end user value and change is embraced in today s post agile software development world there is a need for processes and tools that deliver new software to the end user as fast as possible the level of adoption of these continuous software engineering practices depends on the product customers and the business domain in this paper we investigate the benefits gained from implementing a completely continuous delivery workflow using a domain specific productized development infrastructure through a descriptive single case study embracing the continuous delivery mindset throughout the development pipeline allows the case customer company to gain fast insight on new business directions and lends the services to live experimentation which in turn adds to end user value up to date feedback cycles between all stakeholders all the way from concept design to end users are offered continuous delivery continuous software engineering development infrastructure journal of physics conference series the proceedings contain 68 papers the topics discussed include improved atlas hammercloud monitoring for local site administration scaling up atlas production system for the lhc run 2 and beyond project prodsys2 root 6 and beyond tobject c 14 and many cores implementing a domain specific language to configure and run lhcb continuous integration builds the gridpp dirac project implementation of a multi vo dirac service recent developments in user job management with ganga the atlas software installation system v2 a highly available system to install and validate grid and cloud sites via panda optimizing cms build infrastructure via apache mesos multicore job scheduling in the worldwide lhc computing grid extending software repository hosting to code review and testing monitoring system for the belle ii distributed computing geant4 computing performance benchmarking and monitoring and improvements to the user interface for lhcb s software continuous integration system ceur workshop proceedings the proceedings contain 21 papers the topics discussed include towards proactive management of technical debt by software metrics defining metrics for continuous delivery and deployment pipeline metrics for gerrit code review test suite evaluation using code coverage based metrics accounting testing in software cost estimation a case study of the current practice and impacts icdo integrated cloud based development tool for devops internal marketplace as a mechanism for promoting software reuse lean startup meets software product lines survival of the fittest or letting products bloom model based technology of software development in large requirements management in github with a lean approach and preventing malicious attacks by diversifying linux shell commands software management for the noνaexperiment the novasoftware noνasoft is written in c and built on the fermilab computing division s art framework that uses root analysis software noνasoftmakes use of more than 50 external software packages is developed by more than 50 developers and is used by more than 100 physicists from over 30 universities and laboratories in 3 continents the software builds are handled by fermilab s custom version of software release tools srt a unix based software management system for large collaborative projects that is used by several experiments at fermilab the system provides software version control with svn configured in a client server mode and is based on the code originally developed by the babar collaboration in this paper we present efforts towards distributing the nova software via the cernvm file system distributed file system we will also describe our recent work to use a cmake build system and jenkins the open source continuous integration system for noνasoft enabling devops collaboration and continuous delivery using diverse application environments springer international publishing switzerland 2015 aiming to provide the means for efficient collaboration between development and operations personnel the devops paradigm is backed by an increasingly growing collection of tools and reusable artifacts for application management continuous delivery pipelines are established based on these building blocks by implementing fully automated end to end application delivery processes which significantly shorten release cycles to reduce risks and costs as well as gaining a critical competitive advantage diverse application environments need to be managed along the pipeline such as development build test and production environments in this work we address the need for systematically specifying and maintaining diverse application environment topologies enriched with environment specific requirements in order to implement continuous delivery pipelines beside the representation of such requirements we focus on their systematic and collaborative resolution with respect to the individual needs of the involved application environments continuous delivery devops pipeline requirements topology continuous test generation on guava springer international publishing switzerland 2015 search based testing can be applied to automatically generate unit tests that achieve high levels of code coverage on object oriented classes however test generation takes time in particular if projects consist of many classes like in the case of the guava library to allow search based test generation to scale up and to integrate it better into software development continuous test generation applies test generation incrementally during continuous integration in this paper we report on the application of continuous test generation with evosuite at the ssbse 15 challenge on the guava library our results show that continuous test generation reduces the time spent on automated test generation by 96 while increasing code coverage by 13 9 on average automated unit test generation continuous integration continuous test generation search based testing continuously delivering your network 2015 ieee softwarization and cloudification of networks through software defined networking and network functions virtualisation promise a new degree of flexibility and agility by moving logic from device firmware into software applications and applying software development mechanisms innovation can be introduced with less effort concrete ways how to operate and orchestrate such systems are not yet defined the process of making changes to a controller software or a virtualized network function in a production network without the risk of network disruption is not covered by literature complexity of systems brings the risk of unexpected side effects and has so long been a show stopper for administrators applying changes to networking devices this paper suggests the adaption of the successful concept of continuous delivery into the software defined networking world test driven development and automatic acceptance tests demonstrate that the software engineering community already found ways to ensure that changes do not break applied to network engineering the adaption of continuous delivery can be seen as an enabler for risk free and frequent changes in production infrastructure through push button deployments models testing models in continuous integration of model driven development we propose to test software models with software models model driven software development proposes that software is to be constructed by developing high level models that directly execute or generate most of the code on the other hand test driven development proposes to produce tests that validate the functionality of the code this paper brings both together by using logic labeled finite state machines to deploy executable models of embedded systems and also to configure the corresponding tests the advantage is a much more efficient validation of the models with more robust and durable representations that ensure effective and efficient quality assurance throughout the development process saving the costly exercise of formal model checking until the system is complete enough to meet all requirements finite state machines model driven development real time systems software models test driven development validation and model checking on the journey to continuous deployment technical and social challenges along the way 2014 elsevier b v all rights reserved context continuous deployment cd is an emerging software development process with organisations such as facebook microsoft and ibm successfully implementing and using the process the cd process aims to immediately deploy software to customers as soon as new code is developed and can result in a number of benefits for organisations such as new business opportunities reduced risk for each release and prevent development of wasted software there is little academic literature on the challenges organisations face when adopting the cd process however there are many anecdotal challenges that organisations have voiced on their online blogs objective the aim of this research is to examine the challenges faced by organisations when adopting cd as well as the strategies to mitigate these challenges method an explorative case study technique that involves in depth interviews with software practitioners in an organisation that has adopted cd was conducted to identify these challenges results this study found a total of 20 technical and social adoption challenges that organisations may face when adopting the cd process the results are discussed to gain a deeper understanding of the strategies employed by organisations to mitigate the impacts of these challenges conclusion while a number of individual technical and social adoption challenges were uncovered by the case study in this research most challenges were not faced in isolation the severity of these challenges were reduced by a number of mitigation strategies employed by the case study organisation it is concluded that organisations need to be well prepared to handle technical and social adoption challenges with their existing expertise processes and tools before adopting the cd process for practitioners knowing how to address the challenges an organisation may face when adopting the cd process provides a level of awareness that they previously may not have had agile software development challenges and mitigation strategies continuous deployment lean software development implementing a domain specific language to configure and run lhcb continuous integration builds the new lhcb nightly build system described at chep 2013 was limited by the use of json files for its configuration json had been chosen as a temporary solution to maintain backward compatibility towards the old xml format by means of a translation function modern languages like python leverage on meta programming techniques to enable the development of domain specific languages dsls in this contribution we will present the advantages of such techniques and how they have been used to implement a dsl that can be used to both describe the configuration of the lhcb nightly builds and actually operate them sqa mashup a mashup framework for continuous integration 2014 elsevier b v all rights reserved context continuous integration ci has become an established best practice of modern software development its philosophy of regularly integrating the changes of individual developers with the master code base saves the entire development team from descending into integration hell a term coined in the field of extreme programming in practice ci is supported by automated tools to cope with this repeated integration of source code through automated builds and testing one of the main problems however is that relevant information about the quality and health of a software system is both scattered across those tools and across multiple views objective this paper introduces a quality awareness framework for ci data and its conceptional model used for the data integration and visualization the framework called sqa mashup makes use of the service based mashup paradigm and integrates information from the entire ci toolchain into a single service method the research approach followed in our work consists out of i a conceptional model for data integration and visualization ii a prototypical framework implementation based on tool requirements derived from literature and iii a controlled user study to evaluate its usefulness results the results of the controlled user study showed that sqa mashup s single point of access allows users to answer questions regarding the state of a system more quickly 57 and accurately 21 6 than with standalone ci tools conclusions the sqa mashup framework can serve as one stop shop for software quality data monitoring in a software development project it enables easy access to ci data which otherwise is not integrated but scattered across multiple ci tools our dynamic visualization approach allows for a tailoring of integrated ci data according to information needs of different stakeholders such as developers or testers continuous integration controlled user study information needs software quality tool integration using simulation to evaluate error detection strategies a case study of cloud based deployment processes 2015 elsevier inc all rights reserved the processes for deploying systems in cloud environments can be the basis for studying strategies for detecting and correcting errors committed during complex process execution these cloud based processes encompass diverse activities and entail complex interactions between cloud infrastructure application software tools and humans many of these processes such as those for making release decisions during continuous deployment and troubleshooting in system upgrades are highly error prone unlike the typically well tested deployed software systems these deployment processes are usually neither well understood nor well tested errors that occur during such processes may require time consuming troubleshooting undoing and redoing steps and problem fixing consequently these processes should ideally be guided by strategies for detecting errors that consider trade offs between efficiency and reliability this paper presents a framework for systematically exploring such trade offs to evaluate the framework and illustrate our approach we use two representative cloud deployment processes a continuous deployment process and a rolling upgrade process we augment an existing process modeling language to represent these processes and model errors that may occur during process execution we use a process aware discrete event simulator to evaluate strategies and empirically validate simulation results by comparing them to experiences in a production environment our evaluation demonstrates that our approach supports the study of how error handling strategies affect how much time is taken for task completion and error fixing deployment process process modeling simulation fresh atomic consistent and durable facd data integration guarantees in this paper i investigate the problem of providing guarantees for continuous data integration i propose a set of strong properties facd which when held in the integration transaction will deliver correct results the strong properties support an integration technique called ccetl continuous consistent extract translate and load ccetl consumes uml class diagrams to identify transactional membership of the data elements that make up the integration ccetl transforms the hierarchical relationships using a version of the topological sort the sort maintains a navigation path from the original uml classes the ccetl approach guarantees acid properties up to the level of snapshot isolation between systems supporting a continuous integration consistency data integration distributed database model semantics modeling hardware development agile and co design 2015 ieee the development of devices that combine hardware and software has created new challenges the new built devices have a short life cycle and frequently require upgrading the software industry attends to these requests with agile methods such as scrum agile methods apply quick iterations and continuous preplanning based on feedback and past iterations enabling a quick and continuous delivery for those requests this scenario is being analyzed when applied to hardware or software development and used along with agile methodologies agile methods co desing hardware agile hardware and software agile scrum creating smart tests from recorded automated test cases springer international publishing switzerland 2015 in order to shorten time to market many software development teams have adopted continuous integration and automated testing although user interface test automation is a suitable solution for agile development the resulting frequently changing application gives rise to challenging task especially from the point of view of maintenance in this paper we present an approach bypassing those drawbacks through test recording enhanced by postprocessing that creates smart tests that are easy to maintain we have analyzed recorded tests and created step signatures that we then use to find a sequence of common steps based on this we identify reusable parts which we consequently optimize using algorithms that are introduced in this paper to remove inefficient duplications in tests automated testing common sequences reusable objects test recording test set optimization mob programming what works what doesn t springer international publishing switzerland 2015 at unruly we are constantly trying to turn up the dial on our xp practices and in the second half of 2014 we started to take the step from pair programming on all production code to mob programming with the entire team this report shares experiences that unruly has gained in pushing the boundaries of extreme programming continuous delivery extreme programming groupthink mob programming xp tackling exascale software challenges in molecular dynamics simulations with gromacs springer international publishing switzerland 2015 gromacs is a widely used package for biomolecular simulation and over the last two decades it has evolved from small scale efficiency to advanced heterogeneous acceleration and multi level parallelism targeting some of the largest supercomputers in the world here we describe some of the ways we have been able to realize this through the use of parallelization on all levels combined with a constant focus on absolute performance release 4 6 of gromacs uses simd acceleration on a wide range of architectures gpu offloading acceleration and both openmp and mpi parallelism within and between nodes respectively the recent work on acceleration made it necessary to revisit the fundamental algorithms of molecular simulation including the concept of neighborsearching and we discuss the present and future challenges we see for exascale simulation in particular a very fine grained task parallelism we also discuss the software management code peer review and continuous integration testing required for a project of this complexity testing robot controllers using constraint programming and continuous integration 2014 elsevier b v all rights reserved context testing complex industrial robots cirs requires testing several interacting control systems this is challenging especially for robots performing process intensive tasks such as painting or gluing since their dedicated process control systems can be loosely coupled with the robot s motion control objective current practices for validating cirs involve manual test case design and execution to reduce testing costs and improve quality assurance a trend is to automate the generation of test cases our work aims to define a cost effective automated testing technique to validate cir control systems in an industrial context method this paper reports on a methodology developed at abb robotics in collaboration with simula for the fully automated testing of cirs control systems our approach draws on continuous integration principles and well established constraint based testing techniques it is based on a novel constraintbased model for automatically generating test sequences where test sequences are both generated and executed as part of a continuous integration process results by performing a detailed analysis of experimental results over a simplified version of our constraint model we determine the most appropriate parameterization of the operational version of the constraint model this version is now being deployed at abb robotics s cir testing facilities and used on a permanent basis this paper presents the empirical results obtained when automatically generating test sequences for cirs at abb robotics in a real industrial setting the results show that our methodology is not only able to detect reintroduced known faults but also to spot completely new faults conclusion our empirical evaluation shows that constraint based testing is appropriate for automatically generating test sequences for cirs and can be faithfully deployed in an industrial context agile development constraint programming continuous integration distributed real time systems robotized painting software testing proceedings of the iasted international symposium on software engineering and applications sea 2015 the proceedings contain 15 papers the topics discussed include performance analysis for a cloud computing application access control mechanisms in big data processing iteractive 3d sound generation visualization toolkit for linux kernel tracing and performance verification spider mscontrol a tool for support to the measurement process using gqim approach an improved implementation of parallel selection on gpus vulnerability based information security risk assessment using attack tree design and implementation of an integrated information system for korean women s education programs and models testing models in continuous integration of model driven development sqa profiles rule based activity profiles for continuous integration environments 2015 ieee continuous integration ci environments cope with the repeated integration of source code changes and provide rapid feedback about the status of a software project however as the integration cycles become shorter the amount of data increases and the effort to find information in ci environments becomes substantial in modern ci environments the selection of measurements e g build status quality metrics listed in a dashboard does only change with the intervention of a stakeholder e g a project manager in this paper we want to address the shortcoming of static views with so called software quality assessment sqa profiles sqa profiles are defined as rule sets and enable a dynamic composition of ci dashboards based on stakeholder activities in tools of a ci environment e g version control system we present a set of sqa profiles for project management committee pmc members bandleader integrator gatekeeper and onlooker for this we mined the commit and issue management activities of pmc members from 20 apache projects we implemented a framework to evaluate the performance of our rule based sqa profiles in comparison to a machine learning approach the results showed that project independent sqa profiles can be used to automatically extract the profiles of pmc members with a precision of 0 92 and a recall of 0 78 a scalable big data test framework 2015 ieee this paper identifies three problems when testing software that uses hadoop based big data techniques first processing big data takes a long time second big data is transferred and transformed among many services do we need to validate the data at every transition point third how should we validate the transferred and transformed data we are developing a novel big data test framework to address these problems the test framework generates a small and representative data set from an original large data set using input space partition testing using this data set for development and testing would not hinder the continuous integration and delivery when using agile processes the test framework also accesses and validates data at various transition points when data is transferred and transformed automated verification and validation methods for transmission control software copyright 2015 sae international with the increasing popularity of seamless gear changing and smooth driving experience along with the need for high fuel efficiency transmission system development has rapidly increased in complexity so too has transmission control software while quality requirements are high and time to market is short as a result extensive testing and documentation along with quick and efficient development methods are required fev responds to these challenges by developing and integrating a transmission software product line with an automated verification and validation process according to the concept of continuous integration ci hence the following paper outlines a software architecture called persist where complexity is reduced by a modular architecture approach additionally modularity enables testability and tracking of quality defects to their root cause to tap this potential the software is tested documented and built every night with a high degree of automation in order to uncover quality risks earlier in development the effect of that approach is shown by examples from a 7 gear series transmission project continuous testing and monitoring leads to a steadily increasing quality level while the project plan fulfillment can be tracked on a daily base it is shown that the combination of modular architecture and automated verification and validation of software helps improve the overall quality and process in the increasing complexity of the transmission system icdo integrated cloud based development tool for devops this research is based on three drivers firstly software development and deployment cycles are getting shorter and require automatic building and deployment processes secondly elastic clouds are available for both hosting and development of applications thirdly the increasingly popular devops introduces new organizational and business culture this paper presents a research prototype and demonstrator of an integrated development tool the tool is cloud based and thus accessible from any web enabled terminal automation is maximized so that deployment cycles can be as fast as possible since the aim is to use cloud resources as a utility in a flexible manner cloud brokering i e finding the most suitable provider is included in the system the contributions of the paper include an idea of a new kind of devops tool description on how it can be implemented on top of standard components and implications to software development processes cloud brokerage cloud federation continuous deployment devops easi clouds project software development optimizing cms build infrastructure via apache mesos the offline software of the cms experiment at the large hadron collider lhc at cern consists of 6m lines of in house code developed over a decade by nearly 1000 physicists as well as a comparable amount of general use open source code a critical ingredient to the success of the construction and early operation of the wlcg was the convergence around the year 2000 on the use of a homogeneous environment of commodity x86 64 processors and linux apache mesos is a cluster manager that provides efficient resource isolation and sharing across distributed applications or frameworks it can run hadoop jenkins spark aurora and other applications on a dynamically shared pool of nodes we present how we migrated our continuous integration system to schedule jobs on a relatively small apache mesos enabled cluster and how this resulted in better resource usage higher peak performance and lower latency thanks to the dynamic scheduling capabilities of mesos simulation framework for autogyro opv development 2015 by the american helicopter society international inc all rights reserved the present work describes the integration of a model of a manned autogyro in a simulation framework for unmanned vehicles through a sensible set of requirements and principles this framework provides several advantages for the development process namely continuous integration flexible testing and validation and simple means of operation as a result our framework enables the continuous development of new flight maneuver flight automation algorithms for our optionally piloted autogyro testbed such that the feedback time from lessons learned during simulation trials or flight tests is short microservices cloud applications testing approach microservice architecture is a cloud application design pattern that implies that the application is divided into a number of small independent services each of which is responsible for implementing of a certain feature the need for continuous integration of developed and or modified microservices in the existing system requires a comprehensive validation of individual microservices and their co operation as an ensemble with other microservices in this paper we would provide an analysis of existing methods of cloud applications testing and identify features that are specific to the microservice architecture based on this analysis we will try to propose a validation methodology of the microservice systems cloud computing fine grained soa microservices soa hitting the target practices for moving toward innovation experiment systems springer international publishing switzerland 2015 the benefits and barriers that software development companies face when moving beyond agile development practices are identified in a multiplecase study in five finnish companies the practices that companies need to adopt when moving towards innovation experiment systems are recognised the background of the study is the stairway to heaven sth model that describes the path that many software development companies take when advancing their development practices the development practices in each case are investigated and analysed in relation to the sth model at first the results of the analysis strengthened the validity of the sth model as a path taken by software development companies to advance their development practices based on the findings the sth model was extended with a set of additional practices and their adoption levels for each step of the model the extended model was validated in five case companies agile development continuous deployment feedback loops innovation experiment systems software development improvements to the user interface for lhcb s software continuous integration system the purpose of this paper is to identify a set of steps leading to an improved interface for lhcb s nightly builds dashboard the goal is to have an efficient application that meets the needs of both the project developers by providing them with a user friendly interface as well as those of the computing team supporting the system by providing them with a dashboard allowing for better monitoring of the build job themselves in line with what is already used by lhcb the web interface has been implemented with the flask python framework for future maintainability and code clarity the database chosen to host the data is the schema less couchdb 7 serving the purpose of flexibility in document form changes to improve the user experience we use javascript libraries such as jquery 11 scaling agile infrastructure to people when cern migrated its infrastructure away from homegrown fabric management tools to emerging industry standard open source solutions the immediate technical challenges and motivation were clear the move to a multi site cloud computing model meant that the tool chains that were growing around this ecosystem would be a good choice the challenge was to leverage them the use of open source tools brings challenges other than merely how to deploy them homegrown software for all the deficiencies identified at the outset of the project has the benefit of growing with the organization this paper will examine what challenges there were in adapting open source tools to the needs of the organization particularly in the areas of multi group development and security additionally the increase in scale of the plant required changes to how change management was organized and managed continuous integration techniques are used in order to manage the rate of change across multiple groups and the tools and workflow for this will be examined research preview supporting requirements feedback flows in iterative system development springer international publishing switzerland 2015 context motivation today embedded systems are increasingly interconnected and operate in a rich context of systems and internet based services iterative development is one strategy of developing such cyber physical systems it enables exploration of early prototypes of a feature in the context of its intended use and collecting telemetric data from test runs this is a rich data source that can be leveraged for learning behavioural requirements for a feature question problem however we found practitioners struggling with deriving requirements for the next iteration from such test runs in a systematic and repeatable way principal ideas results we allow test drivers to add markers when the system behaves unexpectedly by introducing a dedicated feedback tool preliminary evaluation shows that these markers lead to better feedback to the development team and indicates a positive impact on the development cycle contribution we give an example report experiences and discuss industrial implications of feedback systems and in situ requirements gathering in iterative system development feedback system in situ requirements requirements and continuous integration research on non synchronization during deployment of hoop truss deployable antenna 2015 chinese academy of space technology all right reserved the non synchronization phenomenon during the deployment of the hoop truss deployable antenna is obvious in the ground test which was caused by the damping and other factors the multi flexible dynamics system model was established by the absolute nodal coordinate formulation ancf to analyze the non synchronization phenomenon the stress and the distortion were obtained during the continuous deployment of the truss the results show that the non synchronization is caused by the distortion and friction between the elements of the antenna the non synchronization phenomenon is consistent with the stress which makes the truss endure higher load absolute nodal coordinate formulation deployment hoop truss deployable antenna multi flexible dynamics non synchronization phenomenon dtel m k firm rtdw a decentralized extract transform load approach based on m k firm constraint for real time data warehouse nowadays the data from various source systems lies in different formats to make data suitable for strategic decisions we need an extract transform and load etl software to extract data from various resources and to transform them into required data formats and then load it into the data warehouse in this paper we have designed a new approach called dtel m k firm rtdw decentralized extract transform load approach based on m k firm constraint for real time data warehouse which enables to deal with diversity and disparities in data source systems and thus to reduce the time for etl considerably and to provide real time data continuous integration loading experimental results show that our approach performs better than the conventional fcsa rtdw feedback control scheduling architecture for real time data warehouse using the new tpc ds benchmark m k firm constraint data sources partitioning etl rtdw defining continuous planning through a multiple case study springer international publishing switzerland 2015 new and innovative approaches that support continuous development and planning throughout organisations are needed continuity is required in all levels of an organisation from business strategy and planning to software development and operational deployment as well as between these levels continuous planning is one of these activities however continuous planning is not commonly adopted and applied throughout organisations and currently involves only a certain level of planning e g release planning based on the current literature continuous planning is a relatively new and not yet fully studied field of research to augment the knowledge relating to continuous planning this paper presents a multiple case study in which the various levels of planning along with their timeframes are explored the research results point out the key activities as well as the bottlenecks of continuous planning continuous deployment continuous planning levels of planning ampx a modern cross section processing system for generating nuclear data libraries nuclear data libraries are essential for nuclear analysis code systems like scale ampx has been continuously developed since the early 1970s and is used to generate these data libraries for scale recently oak ridge national laboratory ornl initiated a major modernization effort to position ampx for continued development and deployment on modern computing platforms an important step toward modernization has been to merge ampx under the scale continuous integration ci development framework as a result ampx is now developed under the scale software quality assurance plan providing increased confidence in the quality of data libraries developed and deployed by ampx this merge allowed us to develop novel methods to generate self shielding factors that combine scale s resonance self shielding capabilities with the ampx library generation capabilities ampx processes evaluated nuclear data format endf b files the endf community is working to develop a more modern format that will replace the existing endf b format due to strict space and memory requirements when ampx was first developed the original ampx processing and reading of endf data were tightly integrated on newer computer systems this is no longer necessary now there is a separation between reading and processing which allows for simultaneous support of current and new endf formats as a result ampx is positioned to adopt any new endf format when available for end users the modernization effort has resulted in improved processing capabilities for generating nuclear data libraries the objective of the paper is to document the latest ampx cross section processing capabilities and provide the current status of the ampx modernization effort covariance matrices cross section processing shielding factors towards a deployment system for cloud applications copyright 2015 by ksi research inc and knowledge systems institute graduate school a sophisticated deployment system plays an important role in automating and improving the process of software delivery especially for cloud applications since cloud ap plications usually consist of many components run on differ ent virtual machines i e ec2 instances the deployment is time consuming and error prone which may involves man ual operations and complex scripts we develop a deploy ment system aiming to accelerate cloud application deliv ery first of all we propose a component model and a con nector model involving cloud feature then we present a component management system in which component can be configured and instantiated rapidly based component in heritance and composition finally we develop a novel de ployment mechanism that can automate deployment process across multiple cloud instances experiment shows that our approach can reduce the build time and downtime so that it can speed up the delivery process of software application application deployment cloud computing continuous delivery software architecture model based advancements at lockheed martin space systems company 2015 american institute of aeronautics and astronautics inc aiaa all rigts reserved lockheed martin space systems company lmssc has been using model based tools with autogeneration of flight code for a decade we are aggressively driving insertion of model based capabilities into all engineering and production disciplines using our digital tapestry which links all stages of manufacturing – from initial concept and design to production and qualification our space vehicle integration lab svil has been the driving force for the embedded software migration to model based technologies with insertions into nearly every stage in the software life cycle from concept and requirements definition to program simulation and qualification for embedded software we have embraced the mathworks simulink model based development tools and developed a framework in which to control the transformation of both our products and our workforce to these techniques using a consistent approach in many cases the lmssc has developed our own add ons to the cots tools available to ensure compatibility in our processes and to fill in gaps in the cots tool capabilities in that time the cost to develop software using model based tools dropped 39 varieties of capabilities contributed to the reduction and were described in detail along with their contribution to the savings lmssc is pursuing a concept called digital tapestry digital tapestry leverages the strengths of model based development to document elaborate and communicate the aspects of a system for all program stakeholders in a digital fashion with detailed sysml models the digital tapestry is enabling a capability called configure to order using the configure to order capability expert engineers make decisions about mission capabilities and system components which when combined with the detailed sysml models rapidly and automatically elaborates the impacts to other subsystems and system components once the system impacts for a given change are reported the new systems engineering products can be rapidly communicated by a set of tools developed to generate matlab and simulink code fragments from the sysml models the activity models internal block diagrams and other interface specification models from sysml can be automatically converted to implemented simulink and matlab artifacts for incorporation in the system implementation and round tripped to sysml when needed lmssc has seen the use of mathworks simulink increase dramatically in the last 7 years many adopting programs made rapid progress and developed novel and productive ways to use simulink in software development and systems analysis tasks however while collaboration helped limited standardization of the simulink development among many groups even in the same programs led to simulink models that were difficult to share and re use seeing an opportunity to improve future performance lmssc embarked on the development of the coresim initiative using the best capabilities tools and practices from across lmssc the coresim initiative developed a set of simulink development guidelines and tools that are dramatically improving the productivity of simulink development teams in lmssc in addition to development guidelines and process expectations coresim also includes a unit test framework capable of continuous integration an analysis framework to allow analysts to rapidly assess the performance of a simulink algorithm and a repository of simulink models built to coresim standards coresim use within lmssc is expanding as more programs adopt and improve the products over the next 5 years lmssc expects to save 30 of fsw algorithm development costs for programs adopting the coresim framework a new architecture for intrusion tolerant web services based on design diversity techniques web services are the realization of service oriented architecture soa security is an important challenge of web services so far several security techniques and standards based on traditional security mechanisms i e encryption and digital signature have been proposed to enhance the security of web services the aim of this work has been to propose an approach for securing web services by employing the concepts and techniques of software fault tolerance such as design diversity which is called intrusion tolerance intrusion tolerance means the continuous delivery of services in presence of security attacks which can be used as a fundamental approach for enhancing the security of web services in this paper we propose an architecture for intrusion tolerant web services itwss by using both design diversity and composite web services techniques the proposed architecture is called design diverse intrusion tolerant web service abbreviated as dditws for web service composition bpel4ws is used for modeling and verification of the proposed architecture coloured petri nets cpns and the cpn tools are used we have model checked the behavioral properties of the architecture to ensure its correctness using this tool the reliability and security evaluation of the architecture is also performed using a stochastic petri net spn model and the sharpe modeling tool the results show that the reliability and mean time to security failure mttsf in the proposed architecture are improved composite web service intrusion tolerance petri nets reliability software security test automation for nfc ics using jenkins and nunit 2015 ieee this article gives a detailed overview of the setup of a test environment which is used for near field communication nfc integrated circuits ics at ams ag the test environment is used for the verification and validation of the nfc ics as well as for pre certification test runs and is useable in two ways 1 manual execution for every developer to be able to run tests on their desk before committing code changes and to reproduce failing test cases and 2 automated execution which is necessary for the continuous integration ci approach which is followed during development and to ensure that all tests are run against the device under test dut first a description of the system which shall be tested is given afterwards the used tools and methods to execute the various test benches are discussed the hurdles which showed up during the process of setting up the environment for manual as well as automated execution are explained and the used solution is discussed jenkins nfc nunit test automation onthemove federated conference otm 2015 co located with coopis odbase and c and tc 2015 the proceedings contain 48 papers the special focus in this conference is on coopis in the cloud and social networking applications of coopis the topics include collaborative autonomic management of distributed component based applications an efficient optimization algorithm of autonomic managers in service based applications a trusted mapreduce system based on tamper resistance hardware similarity and trust to form groups in online social networks supporting peer help in collaborative learning environments real time relevance matching of news and tweets context aware process injection a multi view learning approach to the discovery of deviant process instances a genetic algorithm for automatic business process test case selection discovering bpmn models with sub processes and multi instance markers information quality in dynamic networked business process management utilizing the hive mind multilevel mapping of ecosystem descriptions determining the quality of product data integration inference control in data integration systems integrated process oriented requirements management supporting structural consistency checking in adaptive case management a probabilistic unified framework for event abstraction and process detection from log data property hypergraphs as an attributed predicate rdf rewinding and repeating scientific choreographies enabling devops collaboration and continuous delivery using diverse application environments a semantic graph model matchmaking public procurement linked open data preference queries with ceteris paribus semantics for linked data crowdsourcing for web service discovery and web services discovery based on semantic tag continuous injection of water and antioxidants possible roles on oil quality during frying 2015 elsevier ltd frying is an important process in cooking a wide spectrum of food products in homes restaurants and the food industry due to the unique sensory characteristics of fried foods combined with a relatively lowcost operation due to health and nutritional concerns however novel approaches are being sought to reduce oil uptake and minimize deleterious oil degradation this study investigated the potential of utilizing water injection and as a vehicle for continuous delivery of one or a combination of antioxidants at different ph values selected antioxidants were tested during simulated frying at 180 °c by monitoring commercial canola oil quality indices water injection increased free fatty acids ffa during frying while simultaneously enhancing oil stability as indicated by conjugated diene value radical scavenging activity and p anisidine value it was demonstrated as an effective carrier for continuous incorporation of antioxidant which maintained a low concentration of ffa during deep fat frying selection of buffer and or neutralizing chemical and antioxidant is of high importance catechin had both antioxidant and prooxidant activities highlighting the need for system optimization the protective role of water injection could be significant for foodservice and restaurants where the frying oil is maintained idle at high temperatures for relatively long periods catechin foodservice free fatty acid p anisidine value radical scavenging activity a cd3xcd123 bispecific dart for redirecting host t cells to myelogenous leukemia preclinical activity and safety in nonhuman primates 2015 american association for the advancement of science all rights reserved current therapies for acute myeloid leukemia aml are largely ineffective and aml patients may benefit from targeted immunotherapy approaches mgd006 is a bispecific cd3xcd123 dual affinity re targeting dart molecule that binds t lymphocytes and cells expressing cd123 an antigen up regulated in several hematological malignancies including aml mgd006 mediates blast killing in aml samples together with concomitant activation and expansion of residual t cells mgd006 is designed to be rapidly cleared and therefore requires continuous delivery in a mouse model of continuous administration mgd006 eliminated engrafted kg 1a cells an aml m0 line in human pbmc peripheral blood mononuclear cell reconstituted nsg β2m sup sup mice at doses as low as 0 5 μg kg per day for ∼7 days mgd006 binds to human and cynomolgus monkey antigens with similar affinities and redirects t cells from either species to kill cd123 expressing target cells mgd006 was well tolerated in monkeys continuously infused with 0 1 μg kg per day escalated weekly to up to 1 μg kg per day during a 4 week period depletion of circulating cd123 positive cells was observed as early as 72 hours after treatment initiation and persisted throughout the infusion period cytokine release observed after the first infusion was reduced after subsequent administrations even when the dose was escalated t cells from animals with prolonged in vivo exposure exhibited unperturbed target cell lysis ex vivo indicating no exhaustion a transient decrease in red cell mass was observed with no neutropenia or thrombocytopenia these studies support clinical testing of mgd006 in hematological malignancies including aml techniques for improving regression testing in continuous integration development environments copyright 2014 acm in continuous integration development environments software engineers frequently integrate new or changed code with the mainline codebase this can reduce the amount of code rework that is needed as systems evolve and speed up development time while continuous integration processes traditionally require that extensive testing be performed following the actual submission of code to the codebase it is also important to ensure that enough testing is performed prior to code submission to avoid breaking builds and delaying the fast feedback that makes continuous integration desirable in this work we present algorithms that make continuous integration processes more cost effective in an initial presubmit phase of testing developers specify modules to be tested and we use regression test selection techniques to select a subset of the test suites for those modules that render that phase more costeffective in a subsequent post submit phase of testing where dependent modules as well as changed modules are tested we use test case prioritization techniques to ensure that failures are reported more quickly in both cases the techniques we utilize are novel involving algorithms that are relatively inexpensive and do not rely on code coverage information two requirements for conducting testing cost effectively in this context to evaluate our approach we conducted an empirical study on a large data set from google that we make publicly available the results of our study show that our selection and prioritization techniques can each lead to costeffectiveness improvements in the continuous integration process continuous integration regression test selection regression testing test case prioritization climbing the stairway to heaven evolving from agile development to continuous deployment of software 2014 springer international publishing switzerland all rights reserved software intensive systems companies need to evolve their practices continuously in response to competitive pressures based on a conceptual model presented as the stairway to heaven we present the transition process when moving from traditional development towards continuous deployment of software our research confirms that the transition towards agile development requires a careful introduction of agile practices into the organization a shift to small development teams and a focus on features rather than components the transition towards continuous integration requires an automated test suite a main branch to which code is continually delivered and a modularized architecture the move towards continuous deployment requires internal and external stakeholders to be fully involved and a proactive customer with whom to explore the concept finally the transition towards r d as an innovation system requires careful ecosystem management in order to align internal business strategies with the dynamics of a competitive business ecosystem characteristic for all transitions is the critical alignment of internal and external processes in order to fully maximize the benefits as provided by the business ecosystem of which a company is part continuous integration flows 2014 springer international publishing switzerland all rights reserved while the agile practice of continuous integration has gained increasing traction in industry since its popularization in the 1990s there is considerable diversity in terms of actual implementation the term has been used to describe what may in practice be described as rather different practices with subsequently varying outcomes this diversity typically camouflaged by common terminology not only prevents effective comparison and therefore learning from industry cases but also hinders practitioners in making informed choices as to how continuous integration is best implemented in their particular context to facilitate analysis and experience exchange we present a descriptive model of automated software integration flows then helping software professionals with their ability to proactively and consciously build integration system suitable to their needs we propose an iterative method for integration flow design the civit model in a nutshell visualizing testing activities to support continuous integration 2014 springer international publishing switzerland all rights reserved nowadays innovations in many products ranging from customer electronics to high end industry electric electronic components are driven by software thus new or extended features to software and mechatronic products can be realized and deployed to the market much faster while the use of software enables an enormous flexibility mastering the ever growing complexity of the resulting products to meet the quality goals required for the market is getting more and more challenging continuous development combined with continuous testing is a successful method that actively incorporates the customer to get feedback for the feature to be deployed early and thus product owners developers and testers can collaborate more effectively to meet the market s needs from literature setting up such an agile development process is clear the individual situation in terms of organization processes and development and test tooling however is depending on the company many of the aforementioned aspects have grown over the years and cannot be easily changed in this article we present the civit model which allows companies to get an explicit understanding and overview of their current testing and integration activities with civit s intuitive representation of the current status companies are able to identify bottlenecks and derive actions points to evolve their processes methods and development and test tooling towards a more agile and continuous deployment oriented organization thus they will be able to develop integrate evaluate and deploy new features faster to the end user hence strengthening their own market position continuous software engineering 2014 springer international publishing switzerland all rights reserved this book provides essential insights on the adoption of modern software engineering practices at large companies producing software intensive systems where hundreds or even thousands of engineers collaborate to deliver on new systems and new versions of already deployed ones it is based on the findings collected and lessons learned at the software center sc a unique collaboration between research and industry with chalmers university of technology gothenburg university and malmö university as academic partners and ericsson ab volvo volvo car corporation saab electronic defense systems grundfos axis communications jeppesen boeing and sony mobile as industrial partners the 17 chapters present the stairway to heaven model which represents the typical evolution path companies move through as they develop and mature their software engineering capabilities the chapters describe theoretical frameworks conceptual models and most importantly the industrial experiences gained by the partner companies in applying novel software engineering techniques the book s structure consists of six parts part i describes the model in detail and presents an overview of lessons learned in the collaboration between industry and academia part ii deals with the first step of the stairway to heaven in which r d adopts agile work practices part iii of the book combines the next two phases i e continuous integration ci and continuous delivery cd as they are closely intertwined part iv is concerned with the highest level referred to as r d as an innovation system while part v addresses a topic that is separate from the stairway to heaven and yet critically important in large organizations organizational performance metrics that capture data and visualizations of the status of software assets defects and teams lastly part vi presents the perspectives of two of the sc partner companies the book is intended for practitioners and professionals in the software intensive systems industry providing concrete models frameworks and case studies that show the specific challenges that the partner companies encountered their approaches to overcoming them and the results researchers will gain valuable insights on the problems faced by large software companies and on how to effectively tackle them in the context of successful cooperation projects scaling agile mechatronics an industrial case study 2014 springer international publishing switzerland all rights reserved the automotive industry is currently in a state of rapid change the traditional mechanical industry has forced by electronic revolution and global threats of climate change transformed into a computerized electromechanical industry a hybrid or electric car of 2013 can have in the order of 100 electronic control units running gigabytes of code working together in a complex network within the car as well as being connected to networks in the world outside this exponential increase of software has posed new challenges for the r d organizations in many cases the commonly used method of requirement engineering towards external suppliers in a waterfall process has shown to be unmanageable part of the solution has been to introduce more in house software development and the new standardized platform for embedded software autosar during the past few years volvo cars has focused on techniques and processes for continuous integration of embedded software for active safety body functions and motor and hybrid technology the feedback times for ecu system test have decreased from months to in the best cases hours domain specific languages dsl for both software and physical models have been used to great extent when developing in house embedded software at volvo cars the main reasons are the close connection with mechatronic systems motors powertrain servos etc the advantage of having domain experts not necessarily software experts developing control software and the facilitated reuse of algorithms model driven engineering also provides a method for agile development and early learning in projects where hardware and mechanics usually are available only late model based testing of the software is performed both as pure simulation mil and in hardware in the loop hil rigs before it is deployed in real cars this testing is currently being automated for several rigs as part of the continuous integration strategy the progress is however not without challenges details of the work split with tier 1 suppliers using the young autosar standard and the efficiency of autosar code are still open problems another challenge is to manage the complex model framework required for virtual verification when applied on system level and numerous dsls have to be executed together towards continuous integration for cyber physical systems on the example of self driving miniature cars 2014 springer international publishing switzerland all rights reserved today s consumer life is already pervasively supported by visible and unnoticeable technology we are consuming information flows contributing within social webs and integrating our virtual communities into an interconnected lifestyle this interconnected and assisted way of living is realized by various products ranging from consumer electronics products like smartphones and wearable computing up to safety critical systems like intelligent cars which aim for unnoticeably protecting the user and its surroundings in critical situations and at the end of this decade the technology of a self driving car is reported to be available for consumers enabling various opportunities for new businesses from consumer level technology like smartphones smart tvs or laptops users are used to feature extensions and evolution over time by having automated application and operating system updates thus further system features are continuously rolled out on a large user base enabling new use cases nowadays the digitally connected lifestyle integrates components like wearable computing and smart mobility where an oem could hardly anticipate the nearly limitless variety of complex combinations the trend of a continuously evolving user experience in terms of new features and functionalities puts further challenges requirements and constraints on a system provider to maintain the expected high quality of the product and in the future of the interconnected and integrated product network this article presents the design of a simulation based testing and integration approach for cyber physical systems by using self driving miniature cars as the running example dashboards for continuous monitoring of quality for software product under development 2014 elsevier inc all rights reserved modern software development often uses agile and lean principles which focus on the customer s requirement for continuous delivery of new functionality these principles are applied to both small software products developed by one team and to large ones developed by programs consisting of over ten teams as the agile and lean principles often prescribe the empowerment of the teams a number of new challenges in monitoring and controlling software quality appear one such challenge is to provide a comprehensive and succinct visualization of the progress of software development and the progress of quality assurance in this chapter we explain how to develop and use dashboard for monitoring of software development progress and discuss the quality of software architectures under development we present a case study of three companies ericsson volvo car corporation and saab electronic defense systems that have implemented such dashboards the presented dashboards support the companies in monitoring the release readiness of their products the quality of the software or the progress of development we present a number of elements that make dashboards successful and show how the specifics of each company are reflected in the design of the dashboards we conclude the chapter with recommendations for companies willing to adopt similar dashboards and list additional readings for researchers and practitioners interested in exploring the subject further dashboards indicators iso 15939 performance measurement software metrics reviser efficiently updating ide ifds based data flow analyses in response to incremental program changes 2014 acm most application code evolves incrementally and especially so when being maintained after the applications have been deployed yet most data flow analyses do not take advantage of this fact instead they require clients to recompute the entire analysis even if little code has changeda time consuming undertaking especially with large libraries or when running static analyses often e g on a continuous integration server in this work we present reviser a novel approach for automatically and efficiently updating inter procedural dataflow analysis results in response to incremental program changes reviser follows a clear and propagate philosophy aiming at clearing and recomputing analysis information only where required thereby greatly reducing the required computational effort the reviser algorithm is formulated as an extension to the ide framework for inter procedural finite distributed environment problems and automatically updates arbitrary ide based analyses we have implemented reviser as an open source extension to the heros ifds ide solver and the soot program analysis framework an evaluation of reviser on various client analyses and target programs shows performance gains of up to 80 in comparison to a full recomputation the experiments also show reviser to compute the same results as a full recomputation on all instances tested ide ifds incremental analysis inter procedural static analysis ow sensitive analysis open source and standards the role of open source in the dialogue between research and standardization 2014 ieee open source communities have emerged as increasingly popular structure for the rapid introduction of software standards the telecom industry is increasingly based on deployment and operation of software technologies for infrastructure services not just enterprise support functions as open source communities are increasingly enabling new innovation ecosystems for communications services they are expected to take on a key role in the process of developing telecom industry standards this fundamental transition is being driven by a technology evolution that will raise organizational legal and architectural challenges to the telecommunications industry while the challenges have to be carefully considered the business agility and operational efficiency potential of open source adoption is expected to mitigate the difficulty of adopting this new model for designing developing testing and deploying network solutions business agility collaboration continuous integration devops industry alliance innovation open source community standards antagonism of brain insulin like growth factor 1 receptors blocks estradiol effects on memory and levels of hippocampal synaptic proteins in ovariectomized rats rationale treatment with estradiol the primary estrogen produced by the ovaries enhances hippocampus dependent spatial memory and increases levels of hippocampal synaptic proteins in ovariectomized rats increasing evidence indicates that the ability of estradiol to impact the brain and behavior is dependent upon its interaction with insulin like growth factor 1 igf 1 objective the goal of the current experiment was to test the hypothesis that the ability of estradiol to impact hippocampus dependent memory and levels of hippocampal synaptic proteins is dependent on its interaction with igf 1 methods adult rats were ovariectomized and implanted with estradiol or control capsules and trained on a radial maze spatial memory task after training rats were implanted with intracerebroventricular cannulae attached to osmotic minipumps flow rate 0 15 μl h half of each hormone treatment group received continuous delivery of jb1 300 μg ml an igf 1 receptor antagonist and half received delivery of acsf vehicle rats were tested on trials in the radial arm maze during which delays were imposed between the fourth and fifth arm choices hippocampal levels of synaptic proteins were measured by western blotting results estradiol treatment resulted in significantly enhanced memory jb1 blocked that enhancement estradiol treatment resulted in significantly increased hippocampal levels of postsynaptic density protein 95 psd 95 spinophilin and synaptophysin jb1 blocked the estradiol induced increase of psd 95 and spinophilin and attenuated the increase of synaptophysin conclusions results support a role for igf 1 receptor activity in estradiol induced enhancement of spatial memory that may be dependent on changes in synapse structure in the hippocampus brought upon by estradiol igf 1 interactions 2013 springer verlag berlin heidelberg estradiol estrogen hippocampus igf 1 learning memory psd 95 spinophilin synaptic proteins synaptophysin sequence depth not pcr replication improves ecological inference from next generation dna sequencing recent advances in molecular approaches and dna sequencing have greatly progressed the field of ecology and allowed for the study of complex communities in unprecedented detail next generation sequencing ngs can reveal powerful insights into the diversity composition and dynamics of cryptic organisms but results may be sensitive to a number of technical factors including molecular practices used to generate amplicons sequencing technology and data processing despite the popularity of some techniques over others explicit tests of the relative benefits they convey in molecular ecology studies remain scarce here we tested the effects of pcr replication sequencing depth and sequencing platform on ecological inference drawn from environmental samples of soil fungi we sequenced replicates of three soil samples taken from pine biomes in north america represented by pools of either one two four eight or sixteen pcr replicates with both 454 pyrosequencing and illumina miseq increasing the number of pooled pcr replicates had no detectable effect on measures of a and β diversity pseudo b diversity which we define as dissimilarity between re sequenced replicates of the same sample decreased markedly with increasing sampling depth the total richness recovered with illumina was significantly higher than with 454 but measures of α and β diversity between a larger set of fungal samples sequenced on both platforms were highly correlated our results suggest that molecular ecology studies will benefit more from investing in robust sequencing technologies than from replicating pcrs this study also demonstrates the potential for continuous integration of older datasets with newer technology 2014 smith peay a novel gene therapy strategy using secreted multifunctional anti hiv proteins to confer protection to gene modified and unmodified target cells current human immunodeficiency virus type i hiv gene therapy strategies focus on rendering hiv target cells non permissive to viral replication however gene modified cells fail to accumulate in patients and the virus continues to replicate in the unmodified target cell population we have designed lentiviral vectors encoding secreted anti hiv proteins to protect both gene modified and unmodified cells from infection soluble cd4 scd4 a secreted single chain variable fragment sscfv 17b and a secreted fusion inhibitor sfi t45 were used to target receptor binding co receptor binding and membrane fusion respectively additionally we designed bi and tri functional fusion proteins to exploit the multistep nature of hiv entry of the seven antiviral proteins tested scd4 scd4 scfv 17b scd4 fit45 and scd4 scfv 17b fi t45 efficiently inhibited hiv entry the neutralization potency of the bi functional fusion proteins scd4 scfv 17b and scd4 fi t45 was superior to that of scd4 and the food and drug administration approved fusion inhibitor t 20 in co culture experiments scd4 scd4 scfv 17b and scd4 fi t45 secreted from gene modified producer cells conferred substantial protection to unmodified peripheral blood mononuclear cells in conclusion continuous delivery of secreted anti hiv proteins via gene therapy may be a promising strategy to overcome the limitations of the current treatment 2014 macmillan publishers limited bi functional fusion proteins fusion inhibitor hiv entry secreted antiviral proteins towards many dimensional real time quantum theory for heavy particle dynamics i semiclassics in the lagrange picture of classical phase flow we study a quantum theory in terms of action decomposed function adf a class of quantum wave function towards many dimensional applications to quantum dynamics of heavy particles as in chemical reactions the equation of motion for the complex valued amplitude of adf represents a coupling between the internal diffusive motion of a wave packet and dynamics of its group velocity in a hierarchical manner ascending from classical to purely quantum mechanics via semiclassical dynamics we attempt to solve this equation of motion dividing it into two stages a semiclassical level and beyond in this paper as the first stage we develop a semiclassical approximation in the lagrange picture of classical phase flow in the euler picture as in the standard wkb picture continuous integration of the stability matrix along the paths is required by adopting the lagrange picture on the other hand we represent the semiclassical amplitude in terms of what we call deviation determinant which can be evaluated readily in many dimensional systems numerical tests show that adf reproduces quantum wave packets at each space time point along classical paths very well however the adf in this stage is not free of the semiclassical singularity in other words the wave functions diverge at turning points or caustics depending on the initial conditions chosen this divergence is known to take place at points where classical paths smoothly distributed in phase space have focuses in configuration space or momentum space and reflects an intrinsic relationship between quantum and classical mechanics therefore it is by studying the mechanism of removing the singularity that the essential feature of quantum mechanics will be clarified this aspect will be discussed in a companion paper k takatsuka and s takahashi phys rev a 89 012109 2014 1050 294710 1103 physreva 89 012109 as the second stage of many body quantum theory 2014 american physical society continuous integration and its tools continuous integration has been around for a while now but the habits it suggests are far from common practice automated builds a thorough test suite and committing to the mainline branch every day sound simple at first but they require a responsible team to implement and constant care what starts with improved tooling can be a catalyst for long lasting change in your company s shipping culture continuous integration is more than a set of practices it s a mindset that has one thing in mind increasing customer value the web extra at http youtu be tdl chfrjzo is an audio podcast of the tools of the trade column discusses how continuous integration is more than a set of practices it s a mindset that has one thing in mind increasing customer value 2014 ieee continuous delivery continuous integration testing continuous software engineering and beyond trends and challenges throughout its short history software development has been characterized by harmful disconnects between important activities e g planning development and implementation the problem is further exacerbated by the episodic and infrequent performance of activities such as planning testing integration and releases several emerging phenomena reect attempts to address these problems for example the enterprise agile concept has emerged as a recognition that the benefits of agile software development will be suboptimal if not complemented by an agile approach in related organizational function such as finance and hr continuous integration is a practice which has emerged to eliminate discontinuities between development and deployment in a similar vein the recent emphasis on devops recognizes that the integration between software development and its operational deployment needs to be a continuous one we argue a similar continuity is required between business strategy and development bizdev being the term we coin for this these disconnects are even more problematic given the need for reliability and resilience in the complex and data intensive systems being developed today drawing on the lean concept of ow we identify a number of continuous activities which are important for software development in today s context these activities include continuous planning continuous integration continuous deployment continuous delivery continuous verification continuous testing continuous compliance continuous security continuous use continuous trust continuous run time monitoring continuous improvement both process and product all underpinned by continuous innovation we use the umbrella term continuous ∗ continuous star to identify this family of continuous activities bizdev continuous software engineering continuous star devops rugby an agile process model based on continuous delivery in this paper we introduce rugby an agile process model that includes workows for the continuous delivery of software it allows part timers to work in a project based organization with multiple projects for the rapid delivery of prototypes and products we show how continuous delivery improves the development process in two ways first rugby improves the interaction between developers and customers with a continuous feedback mechanism second rugby improves the coordination and communication with stakeholders and across multiple teams in project based organizations with event based releases we have evaluated rugby in two large university software engineering capstone courses with up to 100 participants working in 10 simultaneous projects with industry partners in 2012 and 2013 we describe the metrics used in the evaluation first results indicate that rugby increases the frequency and quality of the interaction between developers and customers leading to improved results in the delivered products agile methods communication models continuous delivery continuous integration executable prototypes feedback release management software evolution user involvement version control system toward design decisions to enable deployability empirical study of three projects reaching for the continuous delivery holy grail 2014 ieee there is growing interest in continuous delivery practices to enable rapid and reliable deployment while practices are important we suggest architectural design decisions are equally important for projects to achieve goals such continuous integration ci build automated testing and reduced deployment cycle time architectural design decisions that conflict with deploy ability goals can impede the team s ability to achieve the desired state of deployment and may result in substantial technical debt to explore this assertion we interviewed three project teams striving to practicing continuous delivery in this paper we summarize examples of the deploy ability goals for each project as well as the architectural decisions that they have made to enable deploy ability we present the deploy ability goals design decisions and deploy ability tactics collected and summarize the design tactics derived from the interviews in the form of an initial draft version hierarchical deploy ability tactic tree architecture tactic continuous delivery continuous integration deployability test automation toward design decisions to enable deployability empirical study of three projects reaching for the continuous delivery holy grail 2014 ieee there is growing interest in continuous delivery practices to enable rapid and reliable deployment while practices are important we suggest architectural design decisions are equally important for projects to achieve goals such continuous integration ci build automated testing and reduced deployment cycle time architectural design decisions that conflict with deploy ability goals can impede the team s ability to achieve the desired state of deployment and may result in substantial technical debt to explore this assertion we interviewed three project teams striving to practicing continuous delivery in this paper we summarize examples of the deploy ability goals for each project as well as the architectural decisions that they have made to enable deploy ability we present the deploy ability goals design decisions and deploy ability tactics collected and summarize the design tactics derived from the interviews in the form of an initial draft version hierarchical deploy ability tactic tree architecture tactic continuous delivery continuous integration deployability test automation introduction of continuous delivery in multi customer project courses continuous delivery is a set of practices and principles to release software faster and more frequently while it helps to bridge the gap between developers and operations for software in production it can also improve the communica tion between developers and customers in the development phase i e before software is in production it shortens the feedback cycle and developers ideally use it right from the beginning of a software development project in this paper we describe the implementation of a cus tomized continuous delivery workow and its benefits in a multi customer project course in summer 2013 our work ow focuses on the ability to deliver software with only a few clicks to the customer in order to obtain feedback as early as possible this helps developers to validate their under standing about requirements which is especially helpful in agile projects where requirements might change often we describe how we integrated this workow and the role of the release manager into our project based organization and how we introduced it using different teaching methods within three months 90 students worked in 10 different projects with real customers from industry and delivered 490 releases after the project course we evaluated our approach in an online questionnaire and in personal inter views our findings and observations show that participat ing students understood and applied the concepts and are convinced about the benefits of continuous delivery copyright 2014 acm continuous delivery continuous integration devops executable prototypes feedback release management user involvement version control system continuous delivery in the cloud an economic evaluation using system dynamics 2014 nova science publishers inc the continuous delivery paradigm has been recently introduced to help facilitate the speedy deployment of working software into production and shifting software delivery exercise from a major event culture to an effortless activity recent advancements have made it possible to practice continuous delivery in the cloud however many practitioners are still skeptical about the economic profitability of practicing continuous delivery in the cloud this work investigates the economic profitability of carrying out continuous delivery in the cloud and compares it with on premise continuous delivery adoption using system dynamics results show that the continuous delivery in the cloud environment over a six year period has a net present value npv over twice that of the on premise continuous delivery agile software development cloud services continuous delivery dynamic continuous delivery towards agile and beyond an empirical account on the challenges involved when advancing software development practices during the last decade the vast majority of software companies have adopted agile development practices now companies are looking to move beyond agile and further advance their practices in this paper we report on the experiences of a company in the embedded systems domain that is adopting agile practices with the intention to move beyond agile and towards continuous deployment of software based on case study research involving group interviews and a web based survey we identify challenges in relation to 1 the adoption of agile practices 2 testing practices 3 continuous deployment and 4 customer validation springer international publishing switzerland 2014 agile practices beyond agile challenges continuous deployment nmotion a continuous integration system for nasa software this paper describes the architecture and implementation of a continuous integration system for nasa independent verification and validation iv v program s independent test capability itc team developed spacecraft simulation environments this system is composed of a repository automated build server automated unit tests integrated issue tracking with one click deployment scenarios supporting multiple targets and platforms this presentation will describe a use case whereas test scripts were integrated into the james webb space telescope integrated simulation and test jist simulator and executed in a completely automated fashion to perform v v of the flight software fsw additional use cases and plans are discussed for incorporating the gsfc core flight system cfs software into the itc continuous integration system automation continuous integration simulation testing v amp v challenges when adopting continuous integration a case study springer international publishing switzerland 2014 the complexity of software development has increased over the last few years customers today demand higher quality and more stable software with shorter delivery time software companies strive to improve their processes in order to meet theses challenges agile practices have been widely praised for the focus they put on customer collaboration and shorter feedback loops companies that have well established agile practices have been trying to improve their processes further by adopting continuous integration the concept where teams integrate their code several times a day however adopting continuous integration is not a trivial task this paper presents a case study in which we based on interviews at a major swedish telecommunication services and equipment provider assess the challenges of continuous integration the study found 23 adoption challenges that organisations may face when adopting the continuous integration process challenges continuous integration software modeling continuous integration practice differences in industry software development continuous integration is a software practice where developers integrate frequently at least daily while this is an ostensibly simple concept it does leave ample room for interpretation what is it the developers integrate with what happens when they do and what happens before they do these are all open questions with regards to the details of how one implements the practice of continuous integration and it is conceivable that not all such implementations in the industry are alike in this paper we show through a literature review that there are differences in how the practice of continuous integration is interpreted and implemented from case to case based on these findings we propose a descriptive model for documenting and thereby better understanding implementations of the continuous integration practice and their differences the application of the model to an industry software development project is then described in an illustrative case study 2013 elsevier inc agile software development continuous integration automated software integration flows in industry a multiple case study there is a steadily increasing interest in the agile practice of continuous integration consequently there is great diver sity in how it is interpreted and implemented and a need to study document and analyze how automated software inte gration ows are implemented in the industry today in this paper we study five separate cases using a descriptive model developed to address the variation points in continuous integration practice discovered in literature each case is dis cussed and evaluated individually whereupon six guidelines for the design and implementation of automated software integration are presented furthermore the descriptive model used to document the cases is evaluated and evolved copyright 2014 acm agile software development automation continuous integration methodologies software integration implementation of continuous integration and automated testing in software development of smart grid scheduling support system 2014 ieee when smart grid scheduling support system d5000 system was developed the development team ran across tough issue that the system is difficult for integration and becomes unstable after integration due to the complexity to resolve the problem the author made research and introduced continuous integration and automated testing approach on d5000 system development this paper provides the concept and advantages of continuous integration and analyzes the necessity for continuous integration it also describes automated testing for quality improvement with code static analytics automated unit testing and automated function testing this paper gives a case study to deploy continuous integration and automated testing on d5000 system development which resolves quality and integration issues effectively and efficiently automated testing continuous integration smart grid continuous integration in a social coding world empirical evidence from github 2014 ieee continuous integration is a software engineering practice of frequently merging all developer working copies with a shared main branch e g several times a day with the advent of github a platform well known for its social coding features that aid collaboration and sharing and currently the largest code host in the open source world collaborative software development has never been more prominent in github development one can distinguish between two types of developer contributions to a project direct ones coming from a typically small group of developers with write access to the main project repository and indirect ones coming from developers who fork the main repository update their copies locally and submit pull requests for review and merger in this paper we explore how github developers use continuous integration as well as whether the contribution type direct versus indirect and different project characteristics e g main programming language or project age are associated with the success of the automatic builds automatic build collaborative software development continuous integration github continuous integration and deployment software to automate nuclear data verification and validation we developed and implemented a highly automated nuclear data quality assurance system advance automated data verification and assurance for nuclear calculations enhancement which is based on the continuous integration and deployment concept that originated from the software industry advance uses readily available open source software components to deliver its powerful functionalities this paper presents in detail the system s data verification functionalities which are being used to ensure the quality of every new evaluation submitted to the nndc also discussed are the current development efforts to incorporate data validation capabilities into the system 2014 visualizing testing activities to support continuous integration a multiple case study while efficient testing arrangements are the key for software companies that are striving for continuous integration most companies struggle with arranging these highly complex and interconnected testing activities there is often a lack of an adequate overview of companies end to end testing activities which tend to lead to problems such as double work slow feedback loops too many issues found during post development disconnected organizations and unpredictable release schedules we report from a multiplecase study in which we explore current testing arrangements at five different software development sites the outcome of the study is a visualization technique of the testing activities involved from unit and component level to product and release level that support the identification of improvement areas this model for visualizing the end to end testing activities for a system has been used to visualize these five cases and has been validated empirically springer international publishing switzerland 2014 and visualization continuous integration software testing streamlining development of a multimillion line computational chemistry code computational science and engineering projects have goals and development strategies that can be quite different from those in the traditional scientific software engineering domain however methodologies developed by and for software engineers can be both applicable and helpful to the scientific software development process this article presents a case study of applying the software engineering strategy of continuous integration to the assisted model building with energy refinement amber molecular dynamics code the authors identify and discuss the key points of this strategy their applicability to scientific codes and how they were implemented in the amber project despite difficulties in adapting existing continuous integration servers to fit the project s requirements the implementation has streamlined the development process and continues to assist developers in identifying errors introduced on a commit by commit basis rather than immediately before release 1999 2011 ieee continuous integration scientific computing software engineering process software program validation continuous test generation enhancing continuous integration with automated test generation 2014 acm in object oriented software development automated unit test generation tools typically target one class at a time a class however is usually part of a software project consisting of more than one class and these are subject to changes over time this context of a class offers significant potential to improve test generation for individual classes in this paper we introduce continuous test generation ctg which includes automated unit test generation during continuous integration i e infrastructure that regularly builds and tests software projects ctg offers several benefits first it answers the question of how much time to spend on each class in a project second it helps to decide in which order to test them finally it answers the question of which classes should be subjected to test generation in the first place we have implemented ctg using the evosuite unit test generation tool and performed experiments using eight of the most popular open source projects available on github ten randomly selected projects from the sf100 corpus and five industrial projects our experiments demonstrate improvements of up to 58 for branch coverage and up to 69 for thrown undeclared exceptions while reducing the time spent on test generation by up to 83 automated test generation continuous integration continuous testing unit testing towards a super collaborative software engineering mooc recently there has been rapid growth in the number of online courses and venues through which students can learn introductory computer programming as software engineering education becomes more prevalent online online education will need to address how to give students the skills and experience at programming collaboratively on realistic projects in this paper we analyse factors affecting how a supercollaborative on campus software studio course could be adapted as a project led supercollaborative mooc copyright 2014 acm continuous integration massively open online course software engineering studio course towards automated execution and evaluation of simulated prototype hri experiments autonomous robots are highly relevant targets for interaction studies but can exhibit behavioral variability that confounds experimental validity currently testing on real systems is the only means to prevent this but remains very labour intensive and often happens too late to improve this situation we are working towards early testing by means of partial simulation with automated assessment and based upon continuous software integration to prevent regressions we will introduce the concept and describe a proof of concept that demonstrates fast feedback and coherent experiment results across repeated trials continuous integration human robot interaction simulation system evaluation testing blueprint for software engineering in technology enhanced learning projects many projects in technology enhanced learning tel aim to develop novel approaches models and systems by field testing new ideas with software prototypes a major challenge is that project consortia need to establish a distributed typically understaffed developer community that has to align its development efforts with needs from application partners and input from research partners tackling this challenge this paper provides a blueprint for software engineering process and infrastructure which was distilled from successful practices in recent tel projects we present a composition of freely available instruments that support open distributed software engineering practices using continuous integration processes the blueprint considers the full development cycle including requirements engineering software architecture issue tracking build management and social aspects of developer community building in tel projects some lessons learned are provided particularly related to open source commitment innovation as a social process and the essential role of time we aim to make software development in tel fit for horizon 2020 with processes and instruments that can be readily adopted for planning and executing future projects 2014 springer international publishing switzerland structuring simulink models for verification and reuse copyright 2014 acm model based development mbd tool suites such as simulink and stateflow offer powerful tools for design development and analysis of models these models can be used for several purposes for code generation for prototyping as descriptions of an environment plant that will be controlled by software as oracles for a testing process and many other aspects of software development in addition a goal of model based development is to develop reusable models that can be easily managed in a version controlled continuous integration process although significant guidance exists for proper structuring of source code for these purposes considerably less guidance exists for mbd approaches in this paper we discuss structuring issues in constructing models to support use and reuse of models for design and verification in critical software development projects we illustrate our approach using a generic patient controlled analgesia infusion pump gpca a medical cyber physical system model based development simulink design ver ifier verification an android based approach for automatic unit test unit test is the first stage in the v model and the foundation of software test by means of integrating android junit a testing tool into a continuous integration platform named jenkins this paper proposes a fully automated unit test approach the approach can generate a visual test report for android projects this achievement can significantly increase the efficiency of multiple unit tests and thus enable engineers to deliver their software products to the end users as soon as possible android automated jenkins junit unit test supporting continuous integration by mashing up software quality information continuous integration ci has become an established best practice of modern software development its philosophy of regularly integrating the changes of individual developers with the mainline code base saves the entire development team from descending into integration hell a term coined in the field of extreme programming in practice ci is supported by automated tools to cope with this repeated integration of source code through automated builds testing and deployments currently available products for example jenkins ci sonarqube or github allow for the implementation of a seamless ci process one of the main problems however is that relevant information about the quality and health of a software system is both scattered across those tools and across multiple views we address this challenging problem by raising awareness of quality aspects and tailor this information to particular stakeholders such as developers or testers for that we present a quality awareness framework and platform called sqa mashup it makes use of the service based mashup paradigm and integrates information from the entire ci toolchain in a single service to evaluate its usefulness we conducted a user study it showed that sqa mashup s single point of access allows to answer questions regarding the state of a system more quickly and accurately than standalone ci tools 2014 ieee automated testing of industrial automation software practical receipts and lessons learned 2014 acm the share of software in industrial automation systems is steadily increasing thus software quality issues become a critical concern for many automation projects which require effective software quality assurance measures in this paper we describe an architecture for automated testing of software applications part of industrial automation systems we focus on testing programmable logic controller plc software for machineries which can be achieved by using test automation frameworks derived from software development the paper provides a collection of practical receipts describing how different approaches from software engineering best practices can be applied in the context of industrial automation systems a combination of these receipts has been used for automating software tests in an industrial automation project in this project more than 200 tests have been developed to assure the quality of critical plc components the tests are an integral part of the project s automated build and continuous integration system every software change triggers an automated build and test process to ensure that no defects or unintended side effects have been introduced the feedback from the project confirmed the applicability and usefulness of the outlined testing approach in context of industry automation the paper concludes with providing recommendations and sharing additional lessons we have learned from automated testing of plc software iec 61131 3 plc software software testing test automation ubuild automated testing and performance evaluation of embedded linux systems this paper describes ubuild a novel tool designed to support the automated execution of repeatable and controlled tests of embedded linux systems this is useful for continuous integration purposes and to evaluate the impact of various design and implementation options on the system s performance ubuild allows the designer to build the embedded system image from scratch by compiling all the needed software from the source code and by even building the needed cross compilation toolchain if required it provides deterministic control on the configuration options used to build the cross compilation toolchain the linux kernel the system libraries and all the programs in this way the effects of each option can be tested and evaluated in isolation 2014 springer international publishing switzerland continuous testing embedded systems performance evaluation 1st international workshop on rapid continuous software engineering rcose 2014 proceedings the proceedings contain 8 papers the topics discussed include continuous software engineering and beyond trends and challenges rapid requirements checks with requirements smells two case studies rapidly locating and understanding errors using runtime monitoring of architecture carrying code building blocks for continuous experimentation supported approach for agile methods adaptation an adoption study rugby an agile process model based on continuous delivery scrum for cyber physical systems a process proposal and personalized continuous software engineering continuous data integration guarantees isca sede 2014 in this paper i investigate the problem of providing acid guarantees for continuous data integration i proposed an integration technique called ccetl continuous consistent extract translate and load ccetl consumes uml class diagrams to identify transactional membership of the data elements that makeup the integration ccetl transforms the hierarchical relationships using a version of topological sort the sort maintains a navigation path from the original uml classes the ccetl approach guarantees acid properties up to the level of snapshot isolation between systems maintaining a continuous integration consistency data integration distributed database model semantics modeling devopslang bridging the gap between development and operations devops is an emerging paradigm to eliminate the split and barrier between developers and operations personnel that traditionally exists in many enterprises today the main promise of devops is to enable continuous delivery of software in order to enable fast and frequent releases this enables quick responses to changing requirements of customers and thus may be a critical competitive advantage in this work we propose a language called devopslang in conjunction with a methodology to implement devops as an efficient means for collaboration and automation purposes efficient collaboration and automation are the key enablers to implement continuous delivery and thus to react to changing customer requirements quickly 2014 international federation for information processing application evolution cloud computing deployment automation devops devops specification devopsfile an integrated infrastructure in support of software development this paper describes the design and the current state of implementation of an infrastructure made available to software developers within the italian national institute for nuclear physics infn to support and facilitate their daily activity the infrastructure integrates several tools each providing a well identified function project management version control system continuous integration dynamic provisioning of virtual machines efficiency improvement knowledge base when applicable access to the services is based on the infn wide authentication and authorization infrastructure the system is being installed and progressively made available to infn users belonging to tens of sites and laboratories and will represent a solid foundation for the software development efforts of the many experiments and projects that see the involvement of the institute the infrastructure will be beneficial especially for small and medium size collaborations which often cannot afford the resources in particular in terms of know how needed to set up such services published under licence by iop publishing ltd on visual assessment of software quality 2003 2009 e informatyka pl wszelkie prawa zastrzezone development and maintenance of understandable and modifiable software is very challenging good system design and implementation requires strict discipline the architecture of a project can sometimes be exceptionally difficult to grasp by developers a project s documentation gets outdated in a matter of days these problems can be addressed using software analysis and visualization tools incorporating such tools into the process of continuous integration provides a constant up to date view of the project as a whole and helps keeping track of what is going on in the project in this article we describe an innovative method of software analysis and visualization using graph based approach the benefits of this approach are shown through experimental evaluation in visual assessment of software quality using a proof of concept implementation the magnify tool cooperation between information system development and operations a literature review 2014 authors software development can profit from improvements in the deployment and maintenance phases devops improves these phases through a collection of principles and practices centered around close collaboration between development and operations personnel both sides have paid little attention to issues faced by each other yet knowledge sharing is invaluable development personnel can for example make software more robust by implementing scalability and performance features desired by operations personnel automation cloud computing continuous delivery culture development devops measurement operations service oriented architecture services sharing evolution of software only simulation at nasa iv amp v software only simulations are an emerging but quickly developing field of study throughout nasa the nasa independent verification validation iv v independent test capability itc team has been rapidly building a collection of simulators for a wide range of nasa missions itc specializes in full end to end simulations that enable developers v v personnel and operators to test as you fly in four years the team has delivered a wide variety of spacecraft simulations ranging from low complexity science missions such as the global precipitation management gpm satellite and the deep space climate observatory dscovr to the extremely complex missions such as the james webb space telescope jwst and space launch system sls this paper describes the evolution of itc s technologies and processes that have been utilized to design implement and deploy end to end simulation environments for various nasa missions a comparison of mission simulators is discussed with focus on technology and lessons learned in complexity hardware modeling and continuous integration the paper also describes the methods for executing the mission s unmodified flight software binaries not cross compiled for verification and validation activities practical experience with test driven development during commissioning of the multi star ao system argos commissioning time for an instrument at an observatory is precious especially the night time whenever astronomers come up with a software feature request or point out a software defect the software engineers have the task to find a solution and implement it as fast as possible in this project phase the software engineers work under time pressure and stress to deliver a functional instrument control software ics the shortness of development time during commissioning is a constraint for software engineering teams and applies to the argos project as well the goal of the argos advanced rayleigh guided ground layer adaptive optics system project is the upgrade of the large binocular telescope lbt with an adaptive optics ao system consisting of six rayleigh laser guide stars and wavefront sensors for developing the ics we used the technique test driven development tdd whose main rule demands that the programmer writes test code before production code thereby tdd can yield a software system that grows without defects and eases maintenance having applied tdd in a calm and relaxed environment like office and laboratory the argos team has profited from the benefits of tdd before the commissioning we were worried that the time pressure in that tough project phase would force us to drop tdd because we would spend more time writing test code than it would be worth despite this concern at the beginning we could keep tdd most of the time also in this project phase this report describes the practical application and performance of tdd including its benefits limitations and problems during the argos commissioning furthermore it covers our experience with pair programming and continuous integration at the telescope 2014 spie adaptive optics commissioning continuous integration distributed software system instrument control software lbt pair programming software engineering tdd test driven development testing testing robotized paint system using constraint programming an industrial case study 2014 ifip international federation for information processing advanced industrial robots are composed of several independent control systems particularly robots that perform process intensive tasks such as painting gluing and sealing have dedicated process control systems that are more or less loosely coupled with the motion control system validating software for such robotic systems is challenging a major reason for this is that testing the software for such systems requires access to physical systems to test many of their characteristics in this paper we present a method developed at abb robotics in collaboration with simula for automated testing of such process control systems our approach draws on previous work from continuous integration and the use of well established constraint based testing and optimization techniques we present a constraint based model for automatically generating test sequences where we both generate and execute the tests as part of a continuous integration process we also present results from a pilot installation at abb robotic where abb s integrated process control system has been tested the results show that the model is both able to discover completely new errors and to detect old reintroduced errors from this experience we report on lessons learned from implementing an automatic test generation and execution process for a distributed control system for industrial robots technical dependency challenges in large scale agile software development this qualitative study investigates challenges associated with technical dependencies and their communication such challenges frequently occur when agile practices are scaled to large scale software development the use of thematic analysis on semi structured interviews revealed five challenges planning task prioritization knowledge sharing code quality and integration more importantly these challenges interact with one another and can lead to a domino effect or vicious circle if an organization struggles with one challenge it is likely that the other challenges become problematic as well this situation can have a significant impact on process and product quality our recommendations focus on improving planning and knowledge sharing with practices such as scrum of scrums continuous integration open space technology to break the vicious circle and to reestablish effective communication across teams which will then enable large scale companies to achieve the benefits of large scale agility springer international publishing switzerland 2014 cross functional teams xft large scale agile qualitative research technical dependencies 15th international conference on agile software development xp 2014 the proceedings contain 27 papers the special focus in this conference is on agile software development the topics include ux design in agile agile principles in the embedded system development agile software development in practice technical dependency challenges in large scale agile software development contracting in agile software projects why we need a granularity concept for user stories self organized learning in software factory using agile methods to implement a laboratory for software product quality evaluation software metrics in agile software visualizing testing activities to support continuous integration comparing a hybrid testing process with scripted and exploratory testing examining the structure of lean and agile values among software developers agile methodologies in web programming the theory and practice of randori coding dojos locating expertise in agile software development projects social contracts simple rules and self organization realizing agile software enterprise transformations by team performance development a test driven approach for model based development of powertrain functions a global agile architecture design approach an experience report from teams at cisco and an empirical account on the challenges involved when advancing software development practices quality assurance for open source software configuration management commonly used open source configuration management systems such as puppet chef and cfengine allow for system configurations to be expressed as scripts a number of quality issues that may arise when executing these scripts are identified an automated quality assurance service is proposed that identifies the presence of these issues by automatically executing scripts across a range of environments test results are automatically published to a format capable of being consumed by script catalogues and social coding sites this would serve as an independent signal of script trustworthiness and quality to script consumers and would allow developers to be made quickly aware of quality issues as a result potential consumers of scripts can be assured that a script is likely to work when applied to their particular environment script developers can be notified of compatibility issues and take steps to address them 2013 ieee assurance automated configuration automated deployment configuration management continuous integration service orchestration moped a mobile open platform for experimental design of cyber physical systems 2014 ieee due to the increasing importance of cyber physical and embedded systems in industry there is a strong demand for engineers with an updated knowledge on contemporary technology and methods in the area this is a challenge for educators in particular when it comes to creating hands on experiences of real systems due to their complexity and the fact that they are usually proprietary therefore a laboratory environment that is representative of the industrial solutions is needed with a focus on software and systems engineering issues this paper describes such an environment called the mobile open platform for experimental design moped it consists of a model car chassis equipped with a network of three control units based on standard hardware and running the automotive software standard autosar which consists of operating system middleware and application software structures it is equipped with various sensors and actuators and is open to extensions both in hardware and software it also contains elements of future systems since it allows connectivity to cloud services development of federated embedded systems and continuous deployment of new functionality in this way the platform provides a very relevant learning environment for cyber physical systems today and in the future automotive autosar cyber physical systems education federated embedded systems software engineering scapegoat an adaptive monitoring framework for component based systems modern component frameworks support continuous deployment and simultaneous execution of multiple software components on top of the same virtual machine however isolation between the various components is limited a faulty version of any one of the software components can compromise the whole system by consuming all available resources in this paper we address the problem of efficiently identifying faulty software components running simultaneously in a single virtual machine current solutions that perform permanent and extensive monitoring to detect anomalies induce high overhead on the system and can by themselves make the system unstable in this paper we present an optimistic adaptive monitoring system to determine the faulty components of an application suspected components are finely instrumented for deeper analysis by the monitoring system but only when required unsuspected components are left untouched and execute normally thus we perform localized just in time monitoring that decreases the accumulated overhead of the monitoring system we evaluate our approach against a state of the art monitoring system and show that our technique correctly detects faulty components while reducing overhead by an average of 80 2014 ieee based software architecture models runtime monitoring software adaptation the cognitive interaction toolkit – improving reproducibility of robotic systems experiments 2014 springer international publishing switzerland research on robot systems either integrating a large number of capabilities in a single architecture or displaying outstanding performance in a single domain achieved considerable progress over the last years results are typically validated through experimental evaluation or demonstrated live e g at robotics competitions while common robot hardware simulation and programming platforms yield an improved basis many of the described experiments still cannot be reproduced easily by interested researchers to confirm the reported findings we consider this a critical challenge for experimental robotics hence we address this problem with a novel process which facilitates the reproduction of robotics experiments we identify major obstacles to experiment replication and introduce an integrated approach that allows i aggregation and discovery of required research artifacts ii automated software build and deployment as well as iii experiment description repeatable execution and evaluation we explain the usage of the introduced process along an exemplary robotics experiment and discuss our approach in the context of current ecosystems for robot programming and simulation continuous integration development process experimental robotics semantic web software deployment software engineering testability test automation and test driven development for the trick simulation toolkit 2014 by the american institute of aeronautics and astronautics inc all rights reserved this paper describes the adoption of a test driven development approach and a continuous integration system in the development of the trick simulation toolkit a generic simulation development environment for creating high fidelity training and engineering simulations at the nasa johnson space center and many other nasa facilities it describes the approach and the significant benefits seen such as fast thorough and clear test feedback every time code is checked into the code repository it also describes an approach that encourages development of code that is testable and adaptable considering polymorphism in change based test suite reduction springer international publishing switzerland 2014 with the increasing popularity of continuous integration algorithms for selecting the minimal test suite to cover a given set of changes are in order this paper reports on how polymorphism can handle false negatives in a previous algorithm which uses method level changes in the base code to deduce which tests need to be rerun we compare the approach with and without polymorphism on two distinct cases —pmd and cruisecontrol— and discovered an interesting trade off incorporating polymorphism results in more relevant tests to be included in the test suite hence improves accuracy however comes at the cost of a larger test suite hence increases the time to run the minimal test suite change based test selection cheopsj polymorphism test selection unit testing agile software development 2014 the author s looking back at the origins of agile software development we provide an overview of its unique blend of supple practices and vivid cultural facets that contrast so sharply with the plan driven world of waterfall methodologies we discuss the iterative and incremental nature of agility and introduce a tool agile charting that can be used to facilitate communication within agile teams against this backdrop we introduce and compare the three methodologies i e xp scrum and dsdm used throughout this book to illustrate our application of agile risk management we conclude with a glimpse at the state of affairs of agility today and at the management perspective on agile project management thereby setting the tone for the remainder of the book agile practice agile process agile software development continuous integration iterative development mdsplus automated build and distribution system support of the mdsplus data handling system has been enhanced by the addition of an automated build system which does nightly builds of mdsplus for many computer platforms producing software packages which can now be downloaded using a web browser or via package repositories suitable for automatic updating the build system was implemented using an extensible continuous integration server product called hudson which schedules software builds on a collection of vmware based virtual machines new releases are created based on updates via the mdsplus cvs code repository and versioning are managed using cvs tags and branches currently stable beta and alpha releases of mdsplus are maintained for eleven different platforms including windows macosx redhat enterprise linux fedora ubuntu and solaris for some of these platforms mdsplus packaging has been broken into functional modules so users can pick and choose which mdsplus features they want to install an added feature to the latest linux based platforms is the use of package dependencies when installing mdsplus from the package repositories any additional required packages used by mdsplus will be installed automatically greatly simplifying the installation of mdsplus this paper will describe the mdsplus package automated build and distribution system 2013 elsevier b v data acquisition systems data formats data management mdsplus design for arinc 653 conformance architecting independent validation of a safety critical rtos 2014 ieee the arinc 653 specification not only provides a standard application programming interface for an rtos but also specifies how to validate an arinc 653 based rtos arinc 653 part 3 conformity test specification specifies test procedures for validation of arinc 653 part 1 required services specification existing arinc 653 verification suites and packs do not provide platform independency maintainability gained by an open source framework a reliable communication protocol and automated testing principles at the same time this paper introduces a brand new validation suite gvt a653 which is platform independent and ensures conformance to arinc 653 specification the suite is based on tetware trademark of opengroup and builds upon continuous integration ci principles it also brings flexibility by providing various protocols including avionics full duplex switched ethernet afdx network that provides deterministic communication required in avionics applications the verification and validation of a large scale system equipment taas as an example this study proposed testing inspecting as a service taas for computing equipment and aimed to test objects in the context of combining software and hardware we developed a dynamic feedback framework to enhance both the efficiency and the quality of the testing service the framework consists of inner and outer feedback mechanisms in which the updated industrial standards and client requirements are incorporated allowing the testing to be modified accordingly a detailed description of the testing procedures and industrial metrology are provided in the study the improvement gained from taas are demonstrated and discussed the results show that by using infrastructure as a service iaas the physical hardware improves in quality due to the fact that the quality was verified by automated optical inspection aoi taas or cloud container data center cdc taas etc moreover the os in platform as a service paas can be ensured because the resources of the physical virtual data in the iaas were tested by the os taas as for the application app taas the stability and performance of the os system can be maintained since the applications of the software on the paas have been tested 2014 ieee automated optical inspection aoi industrial metrology industrial standards system continuous integration test scit testing inspecting as a service taas economic governance of software delivery agility without objective governance cannot scale and governance without agility cannot compete agile methods are mainstream and software enterprises are adopting these practices in diverse delivery contexts and at enterprise scale ibm s broad industry experience with agile transformations and deep internal know how point to two key principles to deliver sustained improvements in software business outcomes with higher confidence measure and streamline change costs and steer with economic governance and bayesian analytics applying these two principles in context is the crux of measured improvement in continuous delivery of smarter software intensive systems this article describes more meaningful measurement and prediction foundations for economic governance the web extra at http youtu be gham8ifyevi is a video in which walker royce author ieee software editorial board member and ibm chief software economist describes how to reason about software delivery governance with lean principles 1984 2012 ieee bayesian analytics economic governance measuring agility steering leadership developing in the cloud many affordable cloud based offerings that cover software development needs like version control issue tracking remote application monitoring localization deployment payment processing and continuous integration do away with the setup maintenance and user support costs and complexity associated with running such systems in house the most important risks of cloud based tools concern control of the data stored and the services an organization uses on the other hand cloud based tools dramatically lower the capital requirements and setup costs of a software development organization they also help organizations adopt best practices in each domain simply by registering with the corresponding service using a cloud based service also means fewer worries regarding scalability while from the customers perspective delivering a service through the cloud allows an organization to have a much closer relationship with them through cloud based services the development infrastructure is becoming increasingly homogeneous allowing developers to use the same tools across diverse projects and employers transferring knowledge and skills from one job to the next and offering a deeper talent pool of experienced developers the web extra at http youtu be szfwwlr30qk is an audio podcast of author diomidis spinellis reading his tools of the trade column in which he discusses how cloud based services are making the software development infrastructure increasingly homogeneous by allowing developers to use the same tools across diverse projects and employers transferring knowledge and skills from one job to the next and offering a deeper talent pool of experienced developers 1984 2012 ieee cloud paas platform as a service tools process model engineering lifecycle holistic concept proposal and systematic literature review 2014 ieee organizations establish process models for different purposes and goals however they have in common issues concerning quality assurance and successful project management process models can be applied on different levels in process engineering depending on whether processes are to be executed defined or improved various individual methods and tools have been developed to help process engineers and members who are engineering and performing processes these methods however do not support continuous integration into a process engineering lifecycle that consists of creating defining executing and improving processes therefore we propose a holistic view of artifacts and activities in the process engineering lifecycle in an effort to create a consensus on what constitutes a process engineering lifecycle and take a look at recent research results covering the issues mentioned we have identified approaches on process descriptions on meta models and on support in executing processes in the last years continuous process engineering lifecycle process engineering detecting and identifying system changes in the cloud via discovery by example 2014 ieee discovering and identifying system changes caused by events such as software installation and updates configuration changes and security patches are important functionalities for change management security compliance and problem diagnosis in emerging cloud platforms currently most discovery tools use manually written rules which require specific knowledge of software and systems approaches based on manually written rules are often fragile and require constant maintenance in this era of continuous integration in this paper we propose a novel discovery by example approach to autonomously search for and identify system changes our approach learns characteristic features of system changes automatically without requiring any explicit rule definitions or specific knowledge of the underlying software or systems in this approach given a system change our method searches a repository that contains previous stored system changes and returns those that are similar to it we further explore the use of various forms of fingerprints to represent system changes efficiently and faithfully in a compact manner we propose and evaluate two types of fingerprints the basename fingerprint and the 1 d histogram fingerprint we show that both fingerprints exhibit different efficiency and accuracy trade offs and they can be effectively employed in different use cases we evaluate the performance of our approach with both techniques and further present an application of it in system real time streaming monitoring 28th space simulation conference extreme environments pushing the boundaries the proceedings contain 28 papers the topics discussed include overview of a closed loop gn2 system retrofit at david florida laboratory design of a spacecraft integration and test facility at the johns hopkins university applied physics laboratory optimizing 3 component force sensor installation for satellite force limited vibration testing review of v994 shaker system performance comparing factory acceptance test and final acceptance test results investigation of input and response of pyrotechnic shock tests using vibration exciter and mechanical impact test apparatus statistical tolerance bounds overview and applications to spacecraft and launch vehicle dynamic environments outgassing modeling for solar probe plus solar array degradation in a high fluence high temperature environment nmotion a continuous integration system for nasa software and discrepancies caused by common industry standard practices part 1 environment specification numerical simulation model of grounding resistance reducing device in substation 2014 ieee how to simulate the grounding resistance reducing device in existing software and how to quantize the effectiveness of for decreasing grounding impedance have been paid a lot of attention to by power companies and become the research topics for grounding engineers firstly a new calculation method using numerical integration to calculate the grounding system electrical parameters in soil with any number of layers was proposed this method takes place of the traditional calculation method which uses taylor series expansion summing to calculate the point charge electric field with continuous integration then based on a lot of field experiment and calculation a numerical simulation model for new impedance reduction methods is proposed experimental data of the grounding modules and ion sticks with different structures from different factories are collected through four seasons and compared by normalization to derive the equivalent radius and length of the conductor by the measured value of grounding impedance enlarging the equivalent length is used to establish the model for grounding module as enlarging the equivalent radius for ion stick as explosion grounding technique is also a way to let the grid extend into great depths it is equivalent to vertical grounding electrode with enlarged radius the equivalent radius is influenced by the geological condition of the soil around the grounding equipments deep ground well technique utilizes groundwater in deep well to make grounding electrodes which can be equivalent to cylindrical grounding electrode model the numerical simulation model is proved to be valid and practical by actual example deep ground well technique explosion grounding technique impedance reduction equipment impedance reduction module ion grounding electrode simulation model using cp in automatic test generation for abb robotics paint control system designing industrial robot systems for welding painting and assembly is challenging because they are required to perform with high precision speed and endurance abb robotics has specialized in building highly reliable and safe robotized paint systems based on an integrated process control system however current validation practices are primarily limited to manually designed test scenarios a tricky part of this validation concerns testing the timing aspects of the control system which is particularly challenging for paint robots that need to coordinate paint activation with the robot motion control to overcome these challenges we have developed and deployed a costeffective automated test generation technique based on constraint programming aimed at validating the timing behavior of the process control system we designed a constraint optimization model in sicstus prolog using arithmetic and logic constraints including use of global constraints this model has been integrated into a fully automated continuous integration environment allowing the model to be solved on demand prior to test execution which allows us to obtain the most optimal and diverse set of test scenarios for the present system configuration after three months of daily operational use of the constraint model in our testing process we have collected data on its performance and bug finding capabilities we report on these aspects along with our experiences and the improvements gained by the new testing process 2014 springer international publishing switzerland measuring the gain of automatic debug 2013 ieee the purpose of regression testing is to quickly catch any deterioration in quality of a product under development the more frequently tests are run the earlier new issues can be detected resulting in a larger burden for the engineers who need to manually debug all test failures many of which are failing due to the same underlying bug however there are software tools that automatically debug the test failures back to the faulty change and notifies the engineer who made this change by analyzing data from a real commercial asic project we aimed to measure whether bugs are fixed faster when using automatic debug tools compared to manual debugging all bugs in an asic development project were analyzed over a period of 3 months in order to determine the time it took the bug to be fixed and to compare the results from both automatic and manual debug by measuring the time from when the bug report was sent out by the automatic debug tool until the bug was fixed we can show that bugs are fixed 4 times faster with automatic debug enabled bug fixing time was on average 5 7 hours with automatic debug and 23 0 hours for manual debug the result was achieved by comparing bugs that were automatically debugged to those issues that could not be debugged by the tool because those issues were outside the defined scope of the device under test such issues are still reported by the automatic debug tool but marked as requiring manual debug and is consequently a good point of comparison a 4 times quicker bug fixing process is significant and can ultimately contribute to a shortening of a development project as the bug turnaround time is one of the key aspects defining the length of a project especially in the later phase just before release automatic debug continuous integration regression testing system on chip verification investigating the applicability of agility assessment surveys a case study 2014 elsevier inc all rights reserved context agile software development has become popular in the past decade without being sufficiently defined the agile principles can be instantiated differently which creates different perceptions of agility this has resulted in several frameworks being presented in the research literature to evaluate the level of agility however the evidence of their actual use in practice is limited objective the objective is to identify online surveys that assess profile agility in practice and to evaluate the surveys in an industrial setting method the agility assessment surveys were identified through searching the web then they were explored and two surveys were identified as most promising for our objective the selected surveys were evaluated in a case study with three agile teams in a software consultancy company results each team and its customer separately judged the team s agility this outcome was compared with the two survey results in focus group meetings and finally one of the surveys was agreed to provide a more holistic assessment of agility conclusions different surveys may judge agility differently which supports the viewpoint that agile is not well defined therefore practitioners must decide what agile means to them and select the assessment survey that matches their definition abbreviations ci continuous integration gsd global software development pp pair programming rq research question s1 survey 1 s2 survey 2 tdd test driven development tr trouble report t1 team 1 t2 team 2 t3 team 3 xp extreme programming discovery and preclinical development of new antibiotics antibiotics are the medical wonder of our age but an increasing frequency of resistance among key pathogens is rendering them less effective if this trend continues the consequences for cancer patients organ transplant patients and indeed the general community could be disastrous the problem is complex involving abuse and overuse of antibiotics selecting for an increasing frequency of resistant bacteria together with a lack of investment in discovery and development resulting in an almost dry drug development pipeline remedial approaches to the problem should include taking measures to reduce the selective pressures for resistance development and taking measures to incentivize renewed investment in antibiotic discovery and development bringing new antibiotics to the clinic is critical because this is currently the only realistic therapy that can ensure the level of infection control required for many medical procedures here we outline the complex process involved in taking a potential novel antibiotic from the initial discovery of a hit molecule through lead and candidate drug development up to its entry into phase i clinical trials the stringent criteria that a successful drug must meet balancing high efficacy in vivo against a broad spectrum of pathogens with minimal liabilities against human targets explain why even with sufficient investment this process is prone to a high failure rate this emphasizes the need to create a well funded antibiotic discovery and development pipeline that can sustain the continuous delivery of novel candidate drugs into clinical trials to ensure the maintenance of the advanced medical procedures we currently take for granted 2014 informa healthcare candidate drug efficacy hit molecule lead molecule liability testing medicinal chemistry systemic delivery of estradiol but not testosterone or progesterone alters very low density lipoprotein triglyceride kinetics in postmenopausal women context sexual dimorphism in plasma triglyceride tg metabolism is well established but it is unclear to what extent it is driven by differences in the sex hormone milieu results from previous studies evaluating the effects of sex steroids on plasma tg homeostasis are inconclusive because they relied on orally administered synthetic hormone preparations or evaluated only plasma lipid concentrations but not kinetics objective the purpose of this study was to evaluate the effects of systemically delivered 17β estradiol progesterone and t on very low density lipoprotein triglyceride vldl tg concentration and kinetics in postmenopausal women setting and design vldl tg concentration and kinetics were evaluated by using stable isotope labeled tracer techniques in four groups of postmenopausal women n 27 total who were studied before and after treatment with either 17β estradiol 0 1 mg d via continuous delivery skin patch progesterone 100 mg d via vaginal insert and t 12 5 mg d via skin gel or no intervention control group results vldl tg concentration and kinetics were unchanged in the control group and not altered by t and progesterone administration estradiol treatment in contrast reduced vldl tg concentration by approximately 30 due to accelerated vldl tg plasma clearance 25 1 ± 2 5 vs 17 4 ± 2 7 ml min p 01 conclusions estradiol but not progesterone or t is a major regulator of vldl tg metabolism copyright 2014 by the endocrine society monopolar radiofrequency use in deep gluteal space endoscopy sciatic nerve safety and fluid temperature purpose the purpose of this study was to evaluate the temperature at the sciatic nerve when using a monopolar radiofrequency rf probe to control bleeding in deep gluteal space endoscopy as well as assess the fluid temperature profile methods ten hips in 5 fresh frozen human cadaveric specimens from the abdomen to the toes were used for this experiment temperatures were measured at the sciatic nerve after 2 5 and 10 seconds of continuous rf probe activation over an adjacent vessel a branch of the inferior gluteal artery fluid temperatures were then measured at different distances from the probe 3 5 and 10 mm after 2 5 and 10 seconds of continuous probe activation all tests were performed with irrigation fluid flow at 60 mm hg allowing outflow results after 2 5 or 10 seconds of activation over the crossing branch of the inferior gluteal artery the mean temperature increased by less than 1 c on the surface and in the perineurium of the sciatic nerve considering the fluid temperature profile in the deep gluteal space the distance and duration of activation influenced temperature p 05 continuous delivery of rf energy for 10 seconds caused fluid temperature increases of 1 2 c 2 c and 3 1 c on average at 10 mm 5 mm and 3 mm of distance respectively conclusions this study found the tested monopolar rf device to be safe during use in vessels around the sciatic nerve after 2 5 and 10 seconds of continuous activation the maximum fluid temperature 28 c after 10 seconds of activation at 3 mm of distance is lower than the minimal reported temperature necessary to cause nerve changes 40 c to 45 c clinical relevance monopolar rf seems to be safe to the neural structures when used at more than 3 mm of distance and with less than 10 seconds of continuous activation in deep gluteal space endoscopy with fluid inflow and outflow 2014 by the arthroscopy association of north america improving reusability in software process lines software processes orchestrate manual or automatic tasks to create new software products that meet the requirements of specific projects while most of the tasks are about inventiveness modern developments also require recurrent boring and time consuming tasks e g the ide configuration or the continuous integration setup such tasks struggle to be automated due to their various execution contexts according to the requirements of specific projects in this paper we propose a methodology that benefits from an explicit modeling of a family of processes to identify the possible reuse of automated tasks in software processes we illustrate our methodology on industrial projects in a software company our methodology promoted both the identification of possible automated tasks for configuring ides and continuous integration and their reuse in various projects of the company our methodology contributes to the companies efficiency including their agility and ability to experiment new practices while remaining focused on solving business problems 2013 ieee automation software process lines software processes continuous integration for web based software infrastructures lessons learned on the webinos project testing web based software infrastructures is challenging the need to interact with different services running on different devices with different expectations for security and privacy contributes not only to the complexity of the infrastructure but also to the approaches necessary to test it moreover as large scale systems such infrastructures may be developed by distributed teams simultaneously making changes to apis and critical components that implement them in this paper we describe our experiences testing one such infrastructure the webinos software platform and the lessons learned tackling the challenges faced while ultimately these challenges were impossible to overcome this paper explores the techniques that worked most effectively and makes recommendations for developers and teams in similar situations in particular our experiences with continuous integration and automated testing processes are described and analysed 2013 springer international publishing switzerland automated testing continuous integration functional testing web based software infrastructure test case prioritization for continuous regression testing an industrial case study regression testing in continuous integration environment is bounded by tight time constraints to satisfy time constraints and achieve testing goals test cases must be efficiently ordered in execution prioritization techniques are commonly used to order test cases to reflect their importance according to one or more criteria reduced time to test or high fault detection rate are such important criteria in this paper we present a case study of a test prioritization approach rocket prioritization for continuous regression testing to improve the efficiency of continuous regression testing of industrial video conferencing software rocket orders test cases based on historical failure data test execution time and domain specific heuristics it uses a weighted function to compute test priority the weights are higher if tests uncover regression faults in recent iterations of software testing and reduce time to detection of faults the results of the study show that the test cases prioritized using rocket 1 provide faster fault detection and 2 increase regression fault detection rate revealing 30 more faults for 20 of the test suite executed comparing to manually prioritized test cases 2013 ieee continuous integration history based prioritization regression testing software testing test case prioritization hardware and software verification and testing 9th international haifa verification conference hvc 2013 proceedings the proceedings contain 24 papers the topics discussed include backbones for equality increasing confidence in liveness model checking results with proofs speeding up the safety verification of programmable logic controller code modeling firmware as service functions and its application to test generation symbolic model based testing for industrial automation software predictive taint analysis for extended testing of parallel executions continuous integration for web based software infrastructures lessons learned on the webinos project improving post silicon validation efficiency by using pre generated data development and verification of complex hybrid systems using synthesizable monitors assertion checking using dynamic inferenceb and formal specification of an erase block management layer for flash memory using openstack to improve student experience in an h e environment the cloud computing paradigm promises to deliver hardware to the end user at a low cost with an easy to use interface via the internet this paper outlines an effort at the university of huddersfield to deploy a private infrastructure as a service cloud to enhance the student learning experiance the paper covers the deployment methods and configurations for openstack along with the security provisions that were taken to deliver computer hardware the rationale behind the provisions of virtual hardware and os configurations have been defined in great detail supported by examples this paper also covers how the resource has been used within the taught courses as a virtual laboratory and in the research projects a second use case of the cloud for automated formative assesment afa by using jclouds and chef for continuous integration is presented the afa deployment is an example of a software as a service offering that has been added on to the iaas cloud this development has led to an increase in freedom for the student 2013 the science and information organization security of public continuous integration services continuous integration ci and free libre and open source software floss are both associated with agile software development contradictingly floss projects have di culties to use ci and software forges still lack support for ci two factors hamper widespread use of ci in floss development cost of the computational resources and security risks of public ci services through security analysis of public ci services this paper identies possible attack vectors to eliminate one class of attack vectors the paper describes a concept that encapsulates a part of the ci system via virtualization the concept is implemented as a proof of concept copyright 2010 acm one graph to rule them all software measurement and management the software architecture is typically defined as the fundamental organization of the system embodied in its components their relationships to one another and to the system s environment it also encompases principles governing the system s design and evolution in order to manage the architecture of a large software system the architect needs a holistic model that supports continuous integration and verification for all system artifacts in earlier papers we proposed a unified graph based approach to the problem of managing knowledge about the architecture of a software system in this paper we demonstrate that this approach facilitates convenient and efficient project measurement first we show how existing software metrics can be translated into our model in a way that is independent of the programming language second we introduce new metrics that cross the programming language boundaries and are easily implementable using our approach we conclude by demonstrating how the new model can be implemented using existing tools in particular graph databases are a convenient implementation of an architectural repository graph query languages and graph algorithms are an effective way to define metrics and specialized graph views architectural knowledge software architecture software measurement implementing continuous integration software in an established computational chemistry software package continuous integration is the software engineering principle of rapid and automated development and testing we identify several key points of continuous integration and demonstrate how they relate to the needs of computational science projects by discussing the implementation and relevance of these principles to amber a large and widely used molecular dynamics software package the use of a continuous integration server has both improved collaboration and communication between amber developers who are globally distributed as well as making failure and benchmark information that would be time consuming for individual developers to obtain by themselves available in real time continuous integration servers currently available are aimed at the software engineering community and can be difficult to adapt to the needs of computational science projects however as demonstrated in this paper the effort payoff can be rapid since uncommon errors are found and contributions from geographically separated researchers are unified into one easily accessible web based interface 2013 ieee 2013 5th international workshop on software engineering for computational science and engineering se cse 2013 proceedings the proceedings contain 14 papers the topics discussed include a case study agile development in the community laser induced incandescence modeling environment cliime binary instrumentation support for measuring performance in openmp programs software design for decoupled parallel meshing of cad models scientific software process improvement decisions a proposed research strategy water science software institute an open source engagement process techniques for testing scientific programs without an oracle design and rationale of a quality assurance process for a scientific framework implementing continuous integration software in an established computational chemistry software package towards flexible automated support to improve the quality of computational science and engineering software and implicit provenance gathering through configuration management building test suites in social coding sites by leveraging drive by commits github projects attract contributions from a community of users with varying coding and quality assurance skills developers on github feel a need for automated tests and rely on test suites for regression testing and continuous integration however project owners report to often struggle with implementing an exhaustive test suite convincing contributors to provide automated test cases remains a challenge the absence of an adequate test suite or using tests of low quality can degrade the quality of the software product we present an approach for reducing the effort required by project owners for extending their test suites we aim to utilize the phenomenon of drive by commits capable users quickly and easily solve problems in others projects even though they are not particularly involved in that project and move on by analyzing and directing the drive by commit phenomenon we hope to use crowdsourcing to improve projects quality assurance efforts valuable test cases and maintenance tasks would be completed by capable users giving core developers more resources to work on the more complicated issues 2013 ieee identifying potential risks and benefits of using cloud in distributed software development cloud based infrastructure has been increasingly adopted by the industry in distributed software development dsd environments its proponents claim that its several benefits include reduced cost increased speed and greater productivity in software development empirical evaluations however are in the nascent stage of examining both the benefits and the risks of cloud based infrastructure the objective of this paper is to identify potential benefits and risks of using cloud in a dsd project conducted by teams based in helsinki and madrid a cross case qualitative analysis is performed based on focus groups conducted at the helsinki and madrid sites participants observations are used to supplement the analysis the results of the analysis indicated that the main benefits of using cloud are rapid development continuous integration cost savings code sharing and faster ramp up the key risks determined by the project are dependencies unavailability of access to the cloud code commitment and integration technical debt and additional support costs the results revealed that if such environments are not planned and set up carefully the benefits of using cloud in dsd projects might be overshadowed by the risks associated with it 2013 springer verlag benefits and risks of using cloud case study cloud computing cloud based software development distributed software development dsd empirical software engineering global software development offshore software development verification and validation studies for the lava cfd solver verification and validation studies applied to the launch ascent and vehicle aerodynamics lava computational fluid dynamics cfd solver is presented a modern strategy for verification and validation is implemented incorporating verification tests validation benchmarks continuous integration and version control methods for automated testing in a collaborative development environment the purpose of the approach is to integrate the verification and validation process into the development of the solver and improve productivity the procedure uses the method of manufactured solutions mms for the verification of 2d euler equations and 3d navier stokes equations a method for systematic refinement of unstructured grids is also presented inviscid vortex propagation and flow over a flat plate are included in the verification study simulation results using laminar and turbulent flow past a naca 0012 airfoil and onera m6 wing are validated against experimental data compare to existing numerical results test generation for robotized paint systems using constraint programming in a continuous integration environment advanced industrial robots usually consist of several independent control systems particularly robots that perform process intensive tasks like painting gluing and sealing have dedicated process control systems that are more or less loosely coupled with the motion control system testing the software for such systems is challenging because physical systems are necessary to test many of their characteristics this paper proposes a method for automated testing of such robot systems our approach draws on previous work on continuous integration combined with constraint programming techniques for test sequence generation in abb robotics process control system for robotized painting many tests are only conducted every six months during the release test with our automated test approach we expect to reduce the round trip time from code change to test completion to less than one day 2013 ieee constraint programming continuous integration robotized painting mobile authoring and sharing system for creating learning materials research indicates that the integration of computer technology in schools has increased significantly as educators see the benefits of learning with technology for widening students skills in reasoning and problem solving and with the continuous integration of social networks and mobile devices smartphone tablet into daily life the classroom environment can benefit from an enriched curriculum students desire to have these new technologies incorporated in their classroom learning experiences however creating the new mobile software specifically for student needs is not an easy task development will require selecting a device mobile operating system and a software development kit a method for content to be created displayed and reviewed and a method to share this content in this paper the educational mobile authoring and sharing system aims to alleviate burdens from educators when making mobile content for their classroom teachers are not required to have prior experience in programming once the ne w content is completed the mobile software can distribute this content to students immediately without delay by an app store review process using this system students work can be evaluated in real time and results are encrypted and sent to instructors by email 2013 ieee the system design of granular bulk materials continuous conveyor weighing measurement for continuous transportation of granular bulk materials measurement and weighing is important link in industrial and agricultural production warehousing and product flows improve said heavy of precision and speed is the key of dynamic measurement and traditional for the continuous delivery of materials weighing system can not meet the requirements of quickly weighed and measured accurately while sometimes variable and nonlinear and random factors in actual process in the of interference in order to overcome these factors on continuous conveying material measurement system of effect this article based on rbf neural network model for foundation made a dynamic clustering algorithm optimization strategy by means algorithm calculated the center of the base function further extended rbf neural network constants from hidden layer to output layer then using least squares method calculates the weight matrix to determine the final network structure and main parameters this article on measurement of dynamic process has simulation test research and bp neural network for effect comparison simulation results indicates that based on rbf of neural network on continuous conveying bulk material measurement control system is more effective and weighing accuracy and speed is improved in order to achieve the continuous delivery of materials weighing process optimization control and at the same time provides an effective way to solve the existing problems of this type of system 2013 trans tech publications switzerland dynamic clustering dynamic weighing extended matrix rbf neural network revising the spectral method as applied to modeling mantle dynamics the observed geoid dynamic topography and surface plate velocities are controlled by various factors such as density and viscosity variations in the earth s mantle and strength of the lithosphere previous studies have shown that the geoid signal cannot be resolved in details within the framework of a simplified model of the mantle flow considering only radial viscosity variations thus a modeling technique handling both radial and lateral variations of viscosity and other parameters should be used the spectral method provides a high accuracy semianalytical solution of the navier stokes and poisson equations when viscosity is only depth radially dependent in this study we present the numerical approach built up on the substantially revised method originally proposed by zhang and christensen 1993 for solving the navier stokes equation in the spectral domain with lateral variations of viscosity lvv this approach incorporates a number of numerical algorithms providing efficient calculations of the instantaneous stokes flow in the sphere and taking into account the effects of lvv self gravitation and compressibility in contrast to the traditionally used propagator method our approach suggests a continuous integration over depth without introducing internal interfaces various numerical tests have been employed to test accuracy and efficiency of the proposed technique benchmarking of the code shows its ability to solve the mantle convection problems implying strong lvv with high resolution 2013 american geophysical union all rights reserved lateral variations of viscosity mantle dynamics numerical modeling spectral method a comparison of two iterations of a software studio course based on continuous integration in previous work we introduced a software studio course in which seventy students used continuous integration practices to collaborate on a common legacy code base this enabled students to experience the issues of realistically sized software projects and learn and apply appropriate techniques to overcome them in a course without significant extra staffing although the course was broadly successful in its goals it received a mixed response from students and our paper noted several issues to overcome this paper considers experimental changes to the course in light of our previous findings and additional data from the official student surveys two iterations of the course and their respective results are compared whereas our previous paper addressed the feasibility of such a course this paper considers how the student experience can be improved the paper also considers how such a course can be adapted for more heterogeneous cohorts such as the introduction of an unknown number of design and database students or the introduction of online students copyright 2013 acm continuous integration experience report software engineering studio course integrating behavior driven development and programming by contract this paper developed a contracted behavior driven development cbdd method that extends and combines the ideas behind test behavior driven development tdd bdd and programming by contract pbc to improve the overall stability and quality of a system a tool is developed to derive unit tests automatically by analyzing human written specifications for preconditions and post conditions when coupled with data definitions these results will be used to generate code to be run by a unit testing framework before deployment either as part of a continuous integration environment or by individual developers the tool will also generate wireframe classes implementing pre and post conditions within the code and using runtime contract analysis to generate information when an exception occurs thereby helping to automate verification of bug fixes 2013 springer verlag berlin heidelberg software contracted behavior driven development software development tools adapting extreme programming approach in developing electronic document online system edoc extreme programming xp is one of new discipline of software development methodology on values of simplicity communication feedback and also courage xp is an explorative and agile development method that seeks to satisfy the customer through early and continuous delivery of valuable software xp software development process starts with planning and all iterations consist of four basic phases in its life cycle designing coding testing and listening this paper tends to report the experience in adapting xp in developing electronic document online system for the use of centre for diploma studies universiti tun hussein onn malaysia edoc the project under study is a system that is use to store office documents such as letter in an online database the objective of this paper is to discuss the xp practices that had been choosed and also the lesson learnt by practising xp in developing edoc 2013 trans tech publications switzerland agile methodology extreme programming software engineering electrical power system assessment method based on bayesian networks the impact of the design of automotive electrical distribution systems eds is becoming more and more significant with the continuous integration of new safety relevant functions and the substitution of mechanical systems having reached a high degree of robustness the introduction of hybrid and electric vehicles amplify this trend and lead to the design of even more complex electrical networks with multiple voltage levels and new challenges to assess electrical power systems with respect to their ability to supply the involved electrical consumers in various driving and consuming situations at a high level of reliability and voltage stability simulation studies bench testing and driving tests are conducted however a sustained strategy to define relevant consuming and driving situations in order to test the eds under consistent loading conditions is missing the total installed electrical power being much larger than the available generator power a dedicated strategy to define which electrical loads are likely to be switched simultaneously is needed as this is dependent upon driving situations and environment conditions and possesses a probabilistic character an adapted bayesian network based strategy is proposed in this paper to generate consistent loading situations in order to assess the design of the power network in this paper the stimuli generation based on a bayesian network is introduced and discussed the test sequences generated have been implemented on a test bench for electrical power systems to prove their efficiency the analysis conducted on the generated stimuli and the results produced on the test bench are presented and discussed copyright 2013 sae international processes in securing open architecture software systems our goal is to identify and understand issues that arise in the development and evolution processes for securing open architecture oa software systems oa software systems are those developed with a mix of closed source and open source software components that are configured via an explicit system architectural specification such a specification may serve as a reference model or product line model for a family of concurrently sustained oa system versions variants we employ a case study focusing on an oa software system whose security must be continually sustained throughout its ongoing development and evolution we limit our focus to software processes surrounding the architectural design continuous integration release deployment and evolution found in the oa system case study we also focus on the role automated tools software development support mechanisms and development practices play in facilitating or constraining these processes through the case study our purpose is to identify issues that impinge on modeling specification and integration of these processes and how automated tools mediate these processes as emerging research problems areas for the software process research community finally our study is informed by related research found in the prescriptive versus descriptive practice of these processes and tool usage in studies of conventional and open source soft ware development projects copyright 2013 acm configuration continuous software development open architecture process integration process modeling building lean thinking in a telecom software development organization strengths and challenges the potential shown by lean in different domains has aroused interest in the software industry however it remains unclear how lean can be effectively applied in a domain such as software development that is fundamentally different from manufacturing this paper explores how lean principles are implemented in software development companies and the challenges that arise when applying lean software development for that a case study was conducted at ericsson r d finland which successfully adopted scrum in 2009 and subsequently started a comprehensible transition to lean in 2010 focus groups were conducted with company representatives to help devise a questionnaire supporting the creation of a lean mindset in the company team amplifier afterwards the questionnaire was used in 16 teams based in finland hungary and china to evaluate the status of their transformation by using lean thinking ericsson r d finland has made important improvements to the quality of its products customer satisfaction and transparency within the organization the study makes two main contributions to research first the main factors that have enabled ericsson r d s achievements are analysed elements such as network of product owners continuous integration work in progress limits and communities of practice have been identified as being of fundamental importance second three categories of challenges in using lean software development were identified achieving flow transparency and creating a learning culture copyright 2013 acm agile software development le agile lean software development method adoption process improvement process introduction full exploitation of process variation space for continuous delivery of optimal delay test quality the increasing magnitude of process variations individualizes effectively each chip necessitating distinct quantities of test resources for each in order to optimize overall delay test quality without exceeding set test budgets this paper proposes an analytical framework that delivers the optimal test time assignment per chip in order to minimize the delay defect escape rate adjustment of the chip specific test time in the continuous process variation space is attained through an adaptive test flow that utilizes process data measurements from the device under test the results evince that a substantial improvement in the delay test quality can be obtained at no increase whatsoever to test time consumed by conventional test flows 2013 ieee experienced benefits of continuous integration in industry software product development a case study in this paper we present a multi case study of industrial experiences of continuous integration among software professionals working in large scale development projects in literature multiple benefits of continuous integration are suggested but case studies validating these benefits are lacking this study investigates the extent to which continuous integration effects increased developer productivity increased project predictability improved communication and enabling agile testing suggested in literature are experienced in industry development projects the study involves four independent products at different levels of continuous integration maturity within ericsson ab in each of these products developers testers project managers and line managers have been interviewed their experiences of continuous integration are quantitatively assessed and discussed in comparison to the continuous integration benefits proposed in related work agile software development continuous integration software methodologies towards r amp d as innovation experiment systems a framework for moving beyond agile software development in focusing on flexibility efficiency and speed agile development practices have lead to a paradigm shift in how software is developed however while agile practices have indeed proven to be successful these are not the final step of software development there is a beyond agile in which software development companies can capitalize even more on customer contributions and where customer feedback is the main driver for innovation in this paper we present a multiple case study where we explore five software development companies moving from agile towards continuous deployment of software and a future in which r d works as an innovation experiment system based on a qualitative interview study we present benefits and barriers when moving towards r d as an innovation experiment system also we present a framework in which we identify key initiatives that companies deploy in order to evolve their software development practices agile software development continuous deployment innovation experiment system iasted multiconferences proceedings of the iasted international conference on software engineering se 2013 the proceedings contain 18 papers the topics discussed include the user information based mobile application recommender system towards r d as innovation experiment systems a framework for moving beyond agile software development software requirement acquisition method based on probabilistic action model algorithm investigating the candidate pair generation of the vf2 algorithm evaluating and improving design patterns applicability with metrics experienced benefits of continuous integration in industry software product development a case study interview guidelines for analyzing software architectural practices in agile projects integration of functional and interface requirements of an web based software a vdm based formal approach using ahp to compare and evaluate software security testing techniques and influence of code completion methods on the usability of apis successful extreme programming fidelity to the methodology or good teamworking context developing a theory of agile technology in combination with empirical work must include assessing its performance effects and whether all or some of its key ingredients account for any performance advantage over traditional methods given the focus on teamwork is the agile technology what really matters or do general team factors such as cohesion primarily account for a team s success perhaps the more specific software engineering team factors for example the agile development method s collective ownership and code management are decisive objective to assess the contribution of agile methodology agile specific team methods and general team factors in the performance of software teams method we studied 40 small scale software development teams which used extreme programming xp we measured 1 the teams adherence to xp methods 2 their use of xp specific team practices and 3 standard team attributes as well as the quality of the project s outcomes we used williams et al s 2004a 33 shodan measures of xp methods and regression analysis results all three types of variables are associated with the project s performance teamworking is important but it is the xp specific team factor continuous integration coding standards and collective code ownership that is significant only customer planning release planning planning game customer access short releases and stand up meeting is positively related to performance a negative relationship between foundations automated unit tests customer acceptance tests test first design pair programming and refactoring is found and is moderated by craftsmanship sustainable pace simple design and metaphor system of names of the general team factors only cooperation is related to performance cooperation mediates the relationship between the xp specific team factor and performance conclusion client and team foci of the xp method are its critical active ingredients 2012 elsevier b v all rights reserved agile methods cooperation extreme programming performance software development teamwork a silicone elastomer vaginal ring for hiv prevention containing two microbicides with different mechanisms of action vaginal rings are currently being developed for the long term at least 30 days continuous delivery of microbicides against human immunodeficiency virus hiv research to date has mostly focused on devices containing a single antiretroviral compound exemplified by the 25 mg dapivirine ring currently being evaluated in a phase iii clinical study however there is a strong clinical rationale for combining antiretrovirals with different mechanisms of action in a bid to increase breadth of protection and limit the emergence of resistant strains here we report the development of a combination antiretroviral silicone elastomer matrix type vaginal ring for simultaneous controlled release of dapivirine a non nucleoside reverse transcriptase inhibitor and maraviroc a ccr5 targeted hiv 1 entry inhibitor vaginal rings loaded with 25 mg dapivirine and various quantities of maraviroc 50 400 mg were manufactured and in vitro release assessed the 25 mg dapivirine and 100 mg maraviroc formulation was selected for further study a 24 month pharmaceutical stability evaluation was conducted indicating good product stability in terms of in vitro release content assay mechanical properties and related substances this combination ring product has now progressed to phase i clinical testing 2012 elsevier b v all rights reserved controlled release dapivirine hiv microbicide maraviroc vaginal ring improved framework for the maintenance of the jet intershot analysis chain at the jet experiment data from routine diagnostics is analysed automatically by a suite of codes within minutes after operation the maintenance of these interdependent codes and the provision of a consistent state of the physics database over many experimental campaigns against a backdrop of continuous hardware and software updates requires well defined maintenance and validation procedures in this paper the development of a new generation of maintenance tools using distributed version control and a work flow following the principle of continuous integration 1 is described 2012 elsevier b v all rights reserved continuous integration data traceability distributed version control intershot analysis of plasma diagnostics increasing quality and managing complexity in neuroinformatics software development with continuous integration high quality neuroscience research requires accurate reliable and well maintained neuroinformatics applications as software projects become larger offering more functionality and developing a denser web of interdependence between their component parts we need more sophisticated methods to manage their complexity if complexity is allowed to get out of hand either the quality of the software or the speed of development suffer and in many cases both to address this issue here we develop a scalable low cost and open source solution for continuous integration ci a technique which ensures the quality of changes to the code base during the development procedure rather than relying on a pre release integration phase we demonstrate that a ci based workflow due to rapid feedback about code integration problems and tracking of code health measures enabled substantial increases in productivity for a major neuroinformatics project and additional benefits for three further projects beyond the scope of the current study we identify multiple areas in which ci can be employed to further increase the quality of neuroinformatics projects by improving development practices and incorporating appropriate development tools finally we discuss what measures can be taken to lower the barrier for developers of neuroinformatics applications to adopt this useful technique 2012 zaytsev and morrison system dynamics modeling of agile continuous delivery process the popularization of agile development as well as the recent prevalence of virtualization and cloud computing has revolutionized the software delivery process making it faster and affordable for businesses to release their software continuously hence the need for a reliable and predictable delivery process for software applications the aim of this paper is to develop a system dynamics sd model to achieve a repetitive risk free and effortless continuous delivery process to reduce the perils of delayed delivery delivery cost overrun and poor quality delivered software 2013 ieee agile software development continuous delivery delivery pipeline system dynamics the future of continuous integration in gnome in free and open source software foss projects based on linux systems the users usually install the software from distributions the distributions act as intermediaries between software developers and users distributors collect the source code of the different projects and package them ready to be installed by the users packages seems to work well for managing and distributing stable major and minor releases it presents however various release management challenges for developers of projects with multiples dependencies not always available in the stable version of their systems in projects like gnome composed of dozens of individual components developers must build newer versions of the libraries and applications that their applications depend upon before working in their own projects this process can be cumbersome for developers who are not programmers such as user interaction designers or technical writers in this paper we describe some of the problems that the current distribution model presents to do continuous integration testing and deployment for developers in gnome and present ongoing work intended to address these problems that uses a git like approach to the building and deployment of applications 2013 ieee continuous integration free open source software gnome release engineering continuous delivery easy just change everything well maybe it is not that easy rally software transitioned from shipping code every eight weeks with time boxed scrum sprints to a model of continuous delivery with kanban the team encountered complex challenges with their build systems automated test suites customer enablement and internal communication but there was light at the end of the tunnel greater control and flexibility over feature releases incremental delivery of value lower risks fewer defects easier on boarding of new developers less off hours work and a considerable up tick in confidence this experience report describes the journey to continuous delivery with the aim that others can learn from our mistakes and get their teams deploying more frequently we will describe and contrast this transition from the business product management and engineering perspectives 2013 ieee handling server side software versioning the smart technology approach deploying new versions of server side software is similar to deploying new versions of desktop software however it is considered more complex and time consuming therefore if new versions are released frequently and they need to be deployed to many servers doing the work manually may lead to several problems errors due to incorrect deployments misconfigurations and considerable amount of time spent on routine tasks this paper is a study of methods used for desktop software versioning in order to apply them to server side software needs the main focus was set on server side software that is based on php and oracle technology however solutions where sought that could be used for other serverside technologies as well e g asp net java and ruby as a result a solution was created and applied in a real world scenario that helps handling server side software versioning by automating builds of new versions deployment and validation processes 2013 the authors and ios press all rights reserved build and deployment automation comparing databases configuration management continuous integration software integrity check proceedings agile 2013 the proceedings contain 21 papers the topics discussed include agile software development with distributed teams agility distribution and trust proposing regulatory driven automated test suites agile testing a systematic mapping across three conferences understanding agile testing in the xp agile universe agile and xp conferences analyzing effectiveness of workshops for learning agile development principles system dynamics modeling of agile continuous delivery process transforming a public sector company from stone age to agile adapting agile methodology to overcome social differences in project members beyond requirements dictator how agile helped a business analyst discover her real value black swan farming using cost of delay discover nurture and speed up delivery of value need 4 speed leverage new metrics to boost your velocity without compromising on quality continuous delivery easy just change everything well maybe it is not that easy and refactoring as a lifeline lessons learned from refactoring continuous integration and automation for devops the task of managing large installations of computer systems presents a number of unique challenges related to heterogeneity consistency information flow and documentation the emerging field of devops borrows practices from software engineering to tackle complexity in this paper we provide an insight in how automation can to improve scalability and testability while simultaneously reducing the operators work 2013 springer science business media administration automation devops elearning heterogeneous systems system management openflipper a highly modular framework for processing and visualization of complex geometric models openflipper is an open source framework for processing and visualization of complex geometric models suitable for software development in both research and commercial applications in this paper we describe in detail the software architecture which is designed in order to provide a high degree of modularity and adaptability for various purposes although openflipper originates in the field of geometry processing many emerging applications in this domain increasingly rely on immersion technologies consequently the presented software is unlike most existing vr software frameworks mainly intended to be used for the content creation and processing of virtual environments while directly providing a variety of immersion techniques by keeping openflipper s core as simple as possible and implementing functional components as plugins the framework s structure allows for easy extensions replacements and bundling we particularly focus on the description of the integrated rendering pipeline that addresses the requirements of flexible modern high end graphics applications furthermore we describe how cross platform unit and smoke testing as well as continuous integration is implemented in order to guarantee that new code revisions remain portable and regression is minimized openflipper is licensed under the lesser gnu public license and available up to this state for linux windows and mac osx 2013 ieee the semantic web takes wing programming ontologies with tawny owl the tawny owl library provides a fully programmatic environment for ontology building it enables the use of a rich set of tools for ontology development by recasting development as a form of programming it is built in clojure a modern lisp dialect and is backed by the owl api used simply it has a similar syntax to owl manchester syntax but it provides arbitrary extensibility and abstraction it builds on existing facilities for clojure which provides a rich and modern programming tool chain for versioning distributed development build testing and continuous integration in this paper we describe the library this environment and the its potential implications for the ontology development process devops patterns to scale web applications using cloud services scaling a web applications can be easy for simple crud software running when you use platform as a service clouds paas but if you need to deploy a complex software with many components and a lot users you will need have a mix of cloud services in paas saas and iaas layers you will also need knowledge in architecture patterns to make all these software components communicate accordingly in this article we share our experience of using cloud services to scale a web application we show usage examples of load balancing session sharing e mail delivery asynchronous processing logs processing monitoring continuous deployment realtime user monitoring rum these are a mixture of development and system operations devops that improved our application availability scalability and performance copyright 2013 by the association for computing machinery inc acm apis aws cloud cloud computing devops elo7 email iaas load balancing paas rest s3 saas scalability scalable tomcat web services ultimate architecture enforcement custom checks enforced at code commit time creating a software architecture is a critical task in the development of software systems however the architecture discussed and carefully created is often not entirely followed in the implementation unless the architecture is communicated effectively to all developers divergence between the intended architecture created by the architect and the actual architecture found in the source code tends to gradually increase static analysis tools which are often used to check coding conventions and best practices can help however the common use of static analysis tools for architecture enforcement has two limitations one is the fact that design rules specific to a software architecture are not known and hence not enforced by the tool the other limitation is more of a practical issue static analysis tools are often integrated to the ide or to a continuous integration environment they report violations but the developers may choose to ignore them this paper reports a successful experience where we addressed these two limitations for a large codebase comprising over 50 java applications using a free open source tool called checkstyle and its java api we implemented custom checks for design constraints specified by the architecture of our software systems in addition we created a script that executes automatically on the subversion software configuration management server prior to any code commit operation this script runs the custom checks and denies the commit operation in case a violation is found when that happens the developer gets a clear error message explaining the problem the architecture team is also notified and can proactively contact the developer to address any lack of understanding of the architecture this experience report provides technical details of our architecture enforcement approach and recommendations to employ this or similar solutions more effectively copyright 2013 by the association for computing machinery inc acm architecture conformance architecture enforcement checkstyle java software architecture static analysis important considerations for agile software development methods governance after introducing agile approach in 2001 several agile methods were founded over the last decade agile values such as customer collaboration embracing changes iteration and frequent delivery continuous integration etc motivate all software stakeholders to use these methods in their projects moving to agile methods needs a huge change in organization and involved people this change is a fundamental and critical mutation the main issue is that agile transition and governance action plan needs to consider different aspects of change related issues conduction a grounded theory study with participation of 37 agile experts from 13 countries showed that software companies should consider three main factors before inception of transformation action plan adoption styles method selection and awareness of challenges and constraints these fundamental considerations encompass many critical items for agile movement and adoption process however these items may lead to different results in different companies but they should be studied in deep before any transition action plan 2005 2013 jatit lls all rights reserved agile adoption agile governance agile methods agile software development agile transformation agile transition context data distribution with quality guarantees for android based mobile systems in the last years context awareness namely the provisioning of the current execution context to the application level has received an increasing attention up to becoming a core capability in next generation mobile scenarios context awareness intrinsically forces a continuous delivery of context data to resource constrained mobile devices such as mobile phones and personal digital assistants to allow application adaptation and that can become too severe a constraint even for modern platforms android ios etc this paper focuses on the realization of a context data distribution support for android based mobile phones with guaranteed quality levels on the context data delivery time android notwithstanding its great potential puts harsh constraints on the implementation of specific context distribution primitives thus preventing the realization of a wide set of significant deployment scenarios at this stage we face that a large campaign of tests are necessary we have collected main experimental results in a real testbed to highlight noteworthy details on the runtime performance obtainable with a real android deployment copyright 2012 john wiley sons ltd context awareness has received an increasing attention up to becoming a core capability in next generation mobile scenarios context awareness intrinsically forces a continuous delivery of context data to resource constrained mobile devices to allow application adaptation and that can become too severe a constraint even for modern mobile platforms android ios etc this article focuses on the realization of a context data distribution support for android based mobile phones with guaranteed quality levels on the context data delivery time 2012 john wiley sons ltd android os context awareness context data distribution infrastructure mobile systems automated verification of cardiovascular models with continuous integration tools models in general but especially in medicine need extensive testing and verification to ensure that they do not contain errors and produce correct results traditionally this happens after completing the development in this work an approach to automated and continuous testing verification and documentation based on a continuous integration tool is presented this practice has several advantages in comparison to the traditional way of verification as the model is verified after every single change that is made to it one benefit is the earlier and more precise tracing of errors another advantage is the aggregation of code generation software building testing verifying and documentation in one tool to ensure maximum automation and to reduce expenditure of time furthermore due to the integration of central versioning systems it makes working in development teams easier in this work the development processes of two cardiovascular models are incorporated into a continuous integration system cardiovascular model continuous integration development tools model verification design of development as a service in the cloud saas software as a service is software that provides the necessary service only when actually required on the other hand paas platform as a service is a platform where integrated software development is executed using the networked environment saas for developers is supported by version control systems or forums however these services do not support deployment therefore users need to use other services like paas we propose a development environment service in which development and deployment are integrated in this paper we propose a development and deployment service in the educational cloud the integrated system is referred to as development as a service devaas which describes the system this system currently supports a bug tracking system continuous integration system version control system several well known programming languages an editor and deployment environments in the cloud 2012 ieee cloud devaas development paas saas communicating continuous integration servers for increasing effectiveness of automated testing automated testing and continuous integration are established concepts in today x2019 s software engineering landscape but they work in a kind of isolated environment as they do not fully take into consideration the complexity of dependencies between code artifacts in different projects in this paper we demonstrate the continuous change impact analysis process ccip that breaks up the isolation by actively taking into account project dependencies the implemented ccip approach extends the traditional continuous integration ci process by enforcing communication between ci servers whenever new artifact updates are available we show that the exchange of ci process results contribute to improving effectiveness of automated testing 2012 acm dependency management software libraries software project dependency software testing test coverage distributed agile software development for the ska the ska software will most probably be developed by many groups distributed across the globe and coming from different backgrounds like industries and research institutions the ska software subsystems will have to cover a very wide range of different areas but still they have to react and work together like a single system to achieve the scientific goals and satisfy the challenging data flow requirements designing and developing such a system in a distributed fashion requires proper tools and the setup of an environment to allow for efficient detection and tracking of interface and integration issues in particular in a timely way agile development can provide much faster feedback mechanisms and also much tighter collaboration between the customer scientist and the developer continuous integration and continuous deployment on the other hand can provide much faster feedback of integration issues from the system level to the subsystem developers this paper describes the results obtained from trialing a potential ska development environment based on existing science software development processes like alma the expected distribution of the groups potentially involved in the ska development and experience gained in the development of large scale commercial software projects 2012 spie agile ska software development square kilometre array one graph to rule them all software measurment and management for a software intensive system software architecture is typically defined as the fundamental organization of the system embodied in its components their relationships to one another and to the system s environment and the principles governing the system s design and evolution in this paper we propose a unified approach to the problem of managing knowledge about the architecture of a software system we postulate that only a holistic model that supports continuous integration and verification for all system artifacts is the one worth taking and we formally define it then we demonstrate that our approach facilitates convenient project measurement first we show how existing software metrics can be translated into the model in a way that is independent of the programming language next we introduce new metrics that cross the programming language boundaries and are easily implemented using our approach eventually we show that other concerns for architectural knowledge can be also dealt with using our approach we conclude by demonstrating how the new model can be implemented using existing tools in particular graph databases are a convenient implementation of an architectural repository and graph query languages and graph algorithms are an effective way to define metrics and specialized graph views architectural knowledge software architecture software measurement agile practices in regulated railway software development complex software is becoming an important component of modern safety critical systems to assure the correct function of such software the development processes are heavily regulated by international standards often making the process very rigid unable to accommodate changes causing late integration and increasing the cost of development agile methods have been introduced to address these issues in several software domains but their use in safety critical applications remains to be investigated this paper provides an initial analysis of agile practices in the context of software development for the european railway sector regulated by the en 50128 standard the study complements previous studies on the use of agile methods in other regulated domains a systematic mapping between en 50128 requirements and agile practices showed that all practices support some objectives of the standard important supporting features recognized were focus on simple design test automation coding standards continuous integration and validation however several problematic areas were also identified including vague requirement analysis and change management most agile practices must be adapted to suit regulated software development and this analysis outlines a subset of the required changes 2012 ieee agile practices en 50128 railway safety critical systems software development processes software engineering alma software regression tests the evolution under an operational environment the alma software is a large collection of modules which implements the functionality needed for the observatory day to day operations including among others array antenna control correlator telescope calibration and data archiving many software patches must periodically be applied to fix problems detected during operations or to introduce enhancements after a release has been deployed and used under regular operational conditions under this scenery it has been imperative to establish besides a strict configuration control system a weekly regression test to ensure that modifications applied do not impact system stability and functionality a test suite has been developed for this purpose which reflects the operations performed by the commissioning and operations groups and that aims to detect problems associated to the changes introduced at different versions of alma software releases this paper presents the evolution of the regression test suite which started at the alma test facility and that has been adapted to be executed in the current operational conditions topics about the selection of the tests to be executed the validation of the obtained data and the automation of the test suite are also presented 2012 spie alma continuous integration process data validation software engineering testing framework instrument control software development process for the multi star ao system argos the argos project advanced rayleigh guided ground layer adaptive optics system will upgrade the large binocular telescope lbt with an ao system consisting of six rayleigh laser guide stars this adaptive optics system integrates several control loops and many different components like lasers calibration swing arms and slope computers that are dispersed throughout the telescope the purpose of the instrument control software ics is running this ao system and providing convenient client interfaces to the instruments and the control loops the challenges for the argos ics are the development of a distributed and safety critical software system with no defects in a short time the creation of huge and complex software programs with a maintainable code base the delivery of software components with the desired functionality and the support of geographically distributed project partners to tackle these difficult tasks the argos software engineers reuse existing software like the novel middleware from linc nirvana an instrument for the lbt provide many tests at different functional levels like unit tests and regression tests agree about code and architecture style and deliver software incrementally while closely collaborating with the project partners many argos ics components are already successfully in use in the laboratories for testing argos control loops 2012 spie adaptive optics continuous delivery distributed system instrument control software lbt pair programming rapid prototyping software development process test driven development tdd alma operation support software and infrastructure the atacama large millimeter submillimeter array alma will be a unique research instrument composed of at least 66 reconfigurable high precision antennas located at the chajnantor plain in the chilean andes at an elevation of 5000 m each antenna contains instruments capable of receiving radio signals from 31 3 ghz up to 950 ghz these signals are correlated inside a correlator and the spectral data are finally saved into the archive system together with the observation metadata this paper describes the progress in the development of the alma operation support software which aims to increase the efficiency of the testing distribution deployment and operation of the core observing software this infrastructure has become critical as the main array software evolves during the construction phase in order to support and maintain the core observing software it is essential to have a mechanism to align and distribute the same version of software packages across all systems this is achieved rigorously with weekly based regression tests and strict configuration control a build farm to provide continuous integration and testing in simulation has been established as well given the large amount of antennas it is imperative to have also a monitoring system to allow trend analysis of each component in order to trigger preventive maintenance activities a challenge for which we are preparing this year consists in testing the whole alma software performing complete end to end operation from proposal submission to data distribution to the alma regional centers the experience gained during deployment testing and operation support will be presented 2012 spie infrastructure monitoring operation operation support software regression tests runmycode an innovative platform for social production and evaluation of scientific research in this paper we describe runmycode rmc a cloud based platform as a service tool paas which involves researchers collaborating across different time and geographical zones to produce research results along with their related papers it helps researchers to incrementally co create deploy evaluate optimize and reproduce their research it allows also a continuous integration and enhancement of different types of research artefacts including visual models software and scripts rmc extends traditional collaborative platforms by means of a scientific research oriented social layer which allows intensive user contribution interactions and community building along with a content management system that lets researchers share evaluate and develop valuable knowledge rmc features include load balancing elastic context aware and data driven resource allocation and parallel and parameterizable task execution 2012 ieee cloud computing evaluation scientific community social coding social network social ranking teaching tornado from communication models to releases in this paper we describe tornado which we teach in our software engineering project courses tornado is a new process model that combines the unified process with scrum elements the tornado model focuses on scenario based design starting with visionary scenarios funneling down to demo scenarios tornado offers models for a broad range of activities in addition to formal models used for analysis and design tornado encourages the developer to use informal models as communication medium for the interaction with the customer and end user these communication models can be used as the basis of early releases to increase the feedback from customer to developer we argue that the combination of informal modeling and release management can be introduced early in software engineering project courses we describe a case study in which we demonstrate the use of communication models and release management in a multi customer course with 80 students in three months the students produced 163 releases for 11 customers 2012 acm agile techniques continuous integration executable prototypes extreme programming informal modeling project courses prototyping release management scenario based design scrum software engineering education unified process adaptive orthonormal basis functions for high dimensional metamodeling with existing sample points high dimensional model representation hdmr is a tool for generating an approximation of an input output model for a multivariate function it can be used to model a black box function for metamodel based optimization recently the authors team has developed a radial basis function based hdmr rbf hdmr model that can efficiently model a high dimensional black box function and moreover to uncover inner variable structures of the black box function this approach however requests a complete new although optimized set of sample points as dictated by the methodology while in engineering design practice one often has many existing sample data how to utilize the existing data to efficiently construct a hdmr model is the focus of this paper we first identify the random sampling hdmr rs hdmr which uses orthonormal basis functions as hdmr component functions and existing sample points can be used to calculate the coefficients of the basis functions one of the important issues related to the rs hdmr is that in theory the basis functions are obtained based on the continuous integrations related to the orthonormality conditions in practice however the integrations are approximated by monte carlo summation and thus the basis functions may not satisfy the orthonormality conditions in this paper we propose new and adaptive orthonormal basis functions with respect to a given set of sample points for rs hdmr approximation rs hdmr models are built for different test functions using the standard and new adaptive basis functions for different number of sample points the relative errors for both models are calculated and compared the results show that the models that are built using the new basis functions are more accurate 2012 by asme basis function monte carlo simulation orthonormality rs hdmr a traffic injection framework to support the evaluation of effects of reconfigurations on energy consumption in multi rat networks with the ongoing growth of mobile internet usage numbers and the continuous deployment of additional mobile network nodes the power consumption in mobile networks is increasing at dramatic rates leading to a huge amount of carbon emissions therefore improving energy efficiency in mobile radio networks is of increasing relevance the reduction of energy consumption in mobile radio networks can be achieved by adaptively reconfiguring mobile network components e g switching on off base stations based on user behavior and predicted capacity demands for a certain cell or area in the project communicate green we evaluate and develop concepts and algorithms that can be utilized to apply a context aware power management in heterogeneous multi rat networks one of the biggest problems we encountered during the evaluation of the proposed mechanisms in our testbed is the fact that testbeds do not resemble realistic conditions due to a lack of actual users and therefore traffic in this paper we propose a traffic injection framework to simulate various load scenarios in testbeds the architecture is implemented in the multi rat testbed in berlin germany which consists of multiple distinct radio access technologies the traffic injection framework features a distributed approach where client applications are installed on the nodes of the testbed and remotely managed by a control server 2012 ieee context awareness mobile networks power management testbed traffic generation wireless networks using test clouds to enable continuous integration testing of distributed real time and embedded system applications it is critical to evaluate the quality of service qos properties of enterprise distributed real time and embedded dre system early in their lifecycle instead of waiting until system integration to minimize the impact of rework needed to remedy qos defects unfortunately enterprise dre system developers and testers often lack the necessary resources to support such testing efforts this chapter discusses how test clouds i e cloud computing environments employed for testing can provide the necessary testing resources when combined with system execution modeling sem tools test clouds can provide the necessary toolsets to perform qos testing earlier in the lifecycle a case study of design and implementing resource management infrastructure from the domain of shipboard computing environments is used to show how sem tools and test clouds can help identify defects in system qos specifications and enforcement mechanisms before they become prohibitively expensive to fix 2013 igi global a technique for agile and automatic interaction testing for product lines product line developers must ensure that existing and new features work in all products adding to or changing a product line might break some of its features in this paper we present a technique for automatic and agile interaction testing for product lines the technique enables developers to know if features work together with other features in a product line and it blends well into a process of continuous integration the technique is evaluated with two industrial applications testing a product line of safety devices and the eclipse ides the first case shows how existing test suites are applied to the products of a 2 wise covering array to identify two interaction faults the second case shows how over 400 000 test executions are performed on the products of a 2 wise covering array using over 40 000 existing automatic tests to identify potential interactions faults 2012 ifip international federation for information processing agile automatic combinatorial interaction testing continuous integration product lines testing climbing the stairway to heaven a mulitiple case study exploring barriers in the transition from agile development towards continuous deployment of software agile software development is well known for its focus on close customer collaboration and customer feedback in emphasizing flexibility efficiency and speed agile practices have lead to a paradigm shift in how software is developed however while agile practices have succeeded in involving the customer in the development cycle there is an urgent need to learn from customer usage of software also after delivering and deployment of the software product the concept of continuous deployment i e the ability to deliver software functionality frequently to customers and subsequently the ability to continuously learn from real time customer usage of software has become attractive to companies realizing the potential in having even shorter feedback loops however the transition towards continuous deployment involves a number of barriers this paper presents a multiple case study in which we explore barriers associated with the transition towards continuous deployment based on interviews at four different software development companies we present key barriers in this transition as well as actions that need to be taken to address these 2012 ieee agile software development continuous deployment continuous integration customer collaboration improving software quality by improving architecture management for a software intensive system software quality measures how well the software is designed and how well the software conforms to that design whereas architecture of a software system is typically defined as the fundamental organization of the system embodied in its components their relationships to each other and the environment and the principles governing the system s design and evolution obviously as long as there were no software systems governing their architecture was no problem at all when there were only small systems governing their architecture became a mild problem and now we have gigantic software systems and governing their architecture has become an equally gigantic problem to paraphrase edsger dijkstra in this paper we propose a unified approach to the problem of governing or managing the knowledge about architecture of software systems and demonstrate by example its impact on a certain software project first we postulate that only the holistic approach that supports continuous integration and verification for all system architectural artifacts is one worth taking next we demonstrate by example how a concrete large software project being developed in an agile approach is being perceived using the model in question copyright 2012 acm architecture graph metric model software simulation modeling and programming for autonomous robots third international conference simpar 2012 proceedings the proceedings contain 33 papers the topics discussed include a geometric perspective of anthropomorphic embodied actions cybernics fusion of human machine and information robot suit for the future if abstraction is the answer what is the question reasoning for everyday manipulation tasks towards partners profiling in human robot interaction contexts motivation based autonomous behavior control of robotic computer an evaluation method for smart variable space in living space visual trace simulation of concurrent finite state machines for validation and model checking of complex behavior a differential algebraic multistate friction model simulation of flexible objects in robotics continuous integration for iterative validation of simulated robot models software abstractions for simulation and control of a continuum robot and a comparison of sampling strategies for parameter estimation of a robot simulator continuous integration for iterative validation of simulated robot models simulated environments often provide the first and are usually the most frequent test environment for robotic systems primarily due to their cost and safety advantages unfortunately changing aspects of both the simulation and the real robot as well as actuator control algorithms are often not taken into account when relying on simulation results in this paper we present a continuous integration approach to verify simulated robot models in an integrated and frequent manner comprising a simulated and a real robot for comparison the central aspect of our concept is to iteratively assess the fidelity of simulated robot models in an exemplary case study we distilled a first set of requirements and metrics which can be used by developers to verify their algorithms and to automatically detect further system changes 2012 springer verlag berlin heidelberg key factors determining the efficacy of gene therapy for continuous dopa delivery in the parkinsonian brain l dopa is currently the standard treatment for alleviating the motor symptoms in parkinson s disease the therapeutic efficacy however diminishes as the disease progresses it has been suggested that the beneficial effect of l dopa could be reestablished by changing the mode of administration indeed continuous delivery of l dopa has been shown to be an effective way to circumvent many of the side effects seen with traditional oral administration which results in an intermittent supply of the dopamine precursor to the brain however all currently tested continuous dopaminergic stimulation approaches rely on peripheral administration this is not ideal since it gives rise to off target effects and is difficult to maintain long term thus there is an unmet need for an effective continuous administration method with an acceptable side effect profile viral mediated gene therapy is a promising alternative paradigm that can meet this demand encouraging preclinical studies in animal models of parkinson s disease showed therapeutic efficacy after expression of the genes encoding the enzymes required for biosynthesis of dopamine although the first phase i clinical trials using these approaches have been conducted clear positive data in placebo controlled efficacy studies is still lacking we are now at a critical junction and need to carefully review the preclinical data from the clinical translation perspective and identify the key factors that will determine the potential for success in gene therapy for parkinson s disease 2011 adeno associated viral vector aromatic l amino acid decarboxylase l dopa parkinson s disease tyrosine hydroxylase applying industrial strength testing techniques to critical care medical equipment hardware and software development of embedded systems interdependently gear into each other even more so if the device under development is intended for use in critical care facilities such as intensive care units especially in this case safety measures and risk mitigation techniques are implemented using both hardware and software components thus applying hardware and software testing approaches in combination is inevitable as well the increasing utilization of test domain specific languages test dsls code generators and keyword driven interpreters tends to raise the level of abstraction in test development this approach aims to enhance productivity by generating executable tests from a non programming language created for describing test cases a second goal is to increase coverage by generating tests for as many as possible combinations of input values black box test or for all reasonable paths of a program flow white box test in combination with hardware supported signal generation and fault injection this can be a very powerful strategy for testing safety critical embedded devices this article introduces an example of this strategy the usage of a keyword driven testing technique in cooperation with additional test hardware in the context of an embedded medical device development all the while emphasizing the benefit of combining different approaches it discusses the utilization of commercial off the shelf cots testing hardware as well as the application of an in house developed test box it also highlights the integration of commercial software for requirements engineering test management and continuous integration with a self developed testing framework powered by its own keyword based test dsl 2012 springer verlag domainspecific language embedded system keyword driven medical device safety critical testing hardware extracting efsms of web applications for formal requirements specification web applications have begun to be used in wide variety of areas including social networks shopping online banking control systems and other critical systems complexity of applications have raised as well as requirements for security and traceability due to short delivery times and changing requirements quality assurance of web applications is usually an informal process formal methods have been proven to be safe approach to the specification verification and testing of systems the aim of the proposed research is to make formal methods applicable to the web applications development a technique that could extract extended finite state model by combination of static and dynamic analysis is developed this method supports both applications with transitions between web pages and single page applications with ajax requests and dynamic dom modifications two different algorithms are proposed that simplify the state model by merging similar states to achieve a human readable models even for the complex real world web applications the obtained model could be used to define formal requirements for the application and to make model checking part of the continuous integration process for web development 2012 springer verlag fsm model checking model based testing web applications facilitating research cooperation through linking and sharing of heterogenous research artefacts cross platform linking of semantically enriched research artefacts researchers and other knowledge workers frequently produce and use diverse research artefacts such as papers data sets experiment specifications software etc in this they are often faced with unclear relationships e g which version of a software was in use for a particular paper creating unnecessary work and potentially errors semantic web technologies can provide metadata as well as explicit specific links between the artefacts however data acquisition and perceived utility are potential stumbling blocks for adoption therefore we propose a system which is focused on integrating and augmenting existing data thus protecting the existing investment and examine it using an interactionoriented perspective on users without semantic web experience specifically we first study requirements of the target group and then present an exploratory study of managing research artefacts related to software centric projects the results confirm that diverse data sources are in common use that re using existing repositories is perceived as efficient e g more convenient shorter cycle time and that the experimented aggregates are perceived as functionally relevant furthermore the integration of quality assurance mechanisms such as continuous integration is perceived as beneficial despite some added effort copyright 2012 acm aggregates linking research artefacts semantic web mixed data parallel scheduling for distributed continuous integration in this paper we consider the problem of scheduling a special kind of mixed data parallel applications arising in the context of continuous integration continuous integration ci is a software engineering technique which consists in re building and testing interdependent software components as soon as developers modify them the ci tool is able to provide quick feedback to the developers which allows them to fix the bug soon after it has been introduced the ci process can be described as a dag where nodes represent package build tasks and edges represent dependencies among these packages build tasks themselves can in turn be run in parallel thus ci can be viewed as a mixed data parallel application a crucial point for a successful ci process is its ability to provide quick feedback thus make span minimization is the main goal our contribution is twofold first we provide and analyze a large dataset corresponding to a build dag second we compare the performance of several scheduling heuristics on this dataset 2012 ieee continuous integration dag scheduling mixed parallelism communicating continuous integration servers for increasing effectiveness of automated testing automated testing and continuous integration are established concepts in today s software engineering landscape but they work in a kind of isolated environment as they do not fully take into consideration the complexity of dependencies between code artifacts in different projects in this paper we demonstrate the continuous change impact analysis process ccip that breaks up the isolation by actively taking into account project dependencies the implemented ccip approach extends the traditional continuous integration ci process by enforcing communication between ci servers whenever new artifact updates are available we show that the exchange of ci process results contribute to improving effectiveness of automated testing copyright 2012 acm dependency management project dependency software software libraries software testing test coverage joint 10th working conference on software architecture wicsa 2012 and 6th european conference on software architecture ecsa 2012 proceedings companion volume the proceedings contain 35 papers the topics discussed include elements for a cloud based development environment online collaboration revision control and continuous integration an architecture framework for application managed scaling of cloud hosted relational databases framework for monitoring and testing web application scalability on the cloud a first account on stigmergic information systems and their impact on platform development towards bridging the communication gap between consumers and providers in the cloud vulcan architecture model based software development workbench aligning architecture knowledge management with scrum a case study on co evolution of software artifacts using integrated views modeling variability in product lines using domain quality attribute scenarios and configurator as a service tool support for deriving software architectures at runtime elements for a cloud based development environment online collaboration revision control and continuous integration in almost any other field than software development the world wide web or simply the web has revolutionized collaboration by providing a platform for cooperative applications and services in contrast so far revision control also referred to as version management has been the key technology for enabling collaborative software development however even today it is feasible to build a cooperative development environment that runs in a cloud following the spirit of google docs where developers can cooperate in real time in this paper we take a step beyond from the design of the technical artifact that allows collaborative coding and consider the consequences of such an approach to software development in a more general sense in particular from the perspective of version management the goal is to integrate collaborative development with traditional facilities commonly associated with software engineering this in turn will lead to a development approach where new opportunities complement tools that are known to work and provide a lot of added value in software development copyright 2012 acm collaboration revision control version management development of standardized insulin treatment protocols for spontaneous rodent models of type 1 diabetes standardized protocols for maintaining near normal glycemic levels in diabetic rodent models for testing therapeutic agents to treat disease are unavailable we developed protocols for 2 common models of spontaneous type 1 diabetes the biobreeding diabetes prone bbdp rat and nonobese diabetic nod mouse insulin formulation dose level timing of dose administration and delivery method were examined and adjusted so that glycemic levels remained within a normal range and fluctuation throughout feeding and resting cycles was minimized protamine zinc formulations provided the longest activity regardless of the source of insulin glycemic control with few fluctuations was achieved in diabetic bbdp rats through twice daily administration of protamine zinc insulin and results were similar regardless of whether bbdp rats were acutely or chronically diabetic at initiation of treatment in contrast glycemic control could not be attained in nod mice through administration of insulin twice daily however glycemic control was achieved in mice through daily administration of 0 25 u insulin through osmotic pumps whereas twice daily injections of protamine zinc insulin provided glycemic control with only minor fluctuations in bbdp rats mice required continuous delivery of insulin to prevent wide glycemic excursions use of these standard protocols likely will aid in the testing of agents to prevent or reverse diabetes copyright 2012 by the american association for laboratory animal science simulation of biological neural microcircuits on multi core systems our research focuses on the identification and quantification of the impact that multi core parallelization strategies have on the stability of the result of spiking neural networks simulations we investigated openmp based implementations of the spike response model and spike time dependent plasticity for studying behaviors of biological neurons and synapses the underlying neural microcircuits have small world topologies the simulation strategy is a synchronous one the software development methodology we follow makes use of systematic unit testing and continuous integration giving us a way to verify various perturbations of simulation results we carried out investigations on systems having different multi core processors the processing speed spikes second of our simulator scales well with the number of cores but the parallel efficiency is moderate when all cores of the system are used in the simulation 0 57 for 12 cores e g the primary outcomes of this work are twofold one the one hand the proposed parallel simulation strategies show a dynamic behavior unaltered by the use of multi core specific technologies on the other hand we analyze issues met in our approach to multi core simulations 2012 crown copyright lean software development a tutorial experts described the origin of software development and how it related to well known agile development practices and was expected to evolve in the future some similarities between japanese management and pc style software development were becoming apparent by the mid 1990s the popularization of the term lean and its association with agile for software product development emerged from later efforts of experts was described in the book lean software development these experts also emphasized on eliminating waste and bureaucracy in product development encouraged learning through short cycles and frequent builds and promoted late changes and fast iterations with feedback pulling changes into a product lean software development needed to be based on a better understanding of a job that customers wanted to be done how this job needed to be mediated by software agile continuous delivery design thinking kanban lean lean startup scrum toyota xp making the leap to a software platform strategy issues and challenges context while there are many success stories of achieving high reuse and improved quality using software platforms there is a need to investigate the issues and challenges organizations face when transitioning to a software platform strategy objective this case study provides a comprehensive taxonomy of the challenges faced when a medium scale organization decided to adopt software platforms the study also reveals how new trends in software engineering i e agile methods distributed development and flat management structures interplayed with the chosen platform strategy method we used an ethnographic approach to collect data by spending time at a medium scale company in scandinavia we conducted 16 in depth interviews with representatives of eight different teams three of which were working on three separate platforms the collected data was analyzed using grounded theory results the findings identify four classes of challenges namely business challenges organizational challenges technical challenges and people challenges the article explains how these findings can be used to help researchers and practitioners identify practical solutions and required tool support conclusion the organization s decision to adopt a software platform strategy introduced a number of challenges these challenges need to be understood and addressed in order to reap the benefits of reuse researchers need to further investigate issues such as supportive organizational structures for platform development the role of agile methods in software platforms tool support for testing and continuous integration in the platform context and reuse recommendation systems 2012 elsevier b v all rights reserved ethnographic study grounded theory platform challenges software platform software reuse implementing continuous integration towards rapid application development if one is working in isolation continuous integration may not be good for him or her however not many of us have the lavishness of working alone in software development most software development are done in a team leveraging on diverse functional groups delivering different modules or subsystem in an enterprise where development of software involves a collection of developers working on modules integration management is absolutely a necessity we need to find ways to work efficiently and effectively to make the long and heavy integration process to a simpler and joyful task the value of an integrated streamlined build process is something that any software engineers would immediately recognize all this needs lead us to the philosophy of continuous integration it is the intent of this paper to illustrate a journey and learning process in setting up a continuous integration for a software group 2012 ieee build process continuous integration software configuration management drug eluting microfibrous patches for the local delivery of rolipram in spinal cord repair spinal cord injury sci remains a major challenge for regenerative medicine following sci axon growth inhibitors and other inflammatory responses prevent functional recovery previous studies have demonstrated that rolipram an anti inflammatory and cyclic adenosine monophosphate preserving small molecule improves spinal cord regeneration when delivered systemically however more recent studies showed that rolipram has some adverse effects in spinal cord repair here we developed a drug delivery platform for the local delivery of rolipram into the spinal cord the potential of drug eluting microfibrous patches for continuous delivery of high and low dose rolipram concentrations was characterized in vitro following c5 hemisections athymic rats were treated with patches loaded with low and high doses of rolipram in general animals treated with low dose rolipram experienced greater functional and anatomical recovery relative to all other groups outcomes from the high dose rolipram treatment were similar to those with no treatment in addition high dose treated animals experienced reduced survival rates suggesting that systemic toxicity was reached with the ability to control the release of drug dosage locally within the spinal cord drug eluting microfibrous patches demonstrate the importance of appropriate local release kinetics of rolipram proving their usefulness as a therapeutic platform for the study and repair of sci 2012 elsevier b v alginate drug delivery hydrogel scaffold spinal cord controlled release systemic delivery a new concept in cancer chemoprevention many chemopreventive agents have encountered bioavailability issues in pre clinical clinical studies despite high oral doses we report here a new concept utilizing polycaprolactone implants embedded with test compounds to obtain controlled systemic delivery circumventing oral bioavailability issues and reducing the total administered dose compounds were released from the implants in vitro dose dependently and for long durations months which correlated with in vivo release polymeric implants of curcumin significantly inhibited tissue dna adducts following the treatment of rats with benzo a pyrene with the total administered dose being substantially lower than typical oral doses a comparison of bioavailability of curcumin given by implants showed significantly higher levels of curcumin in the plasma liver and brain 30 days after treatment compared with the dietary route withaferin a implants resulted in a nearly 60 inhibition of lung cancer a549 cell xenografts but no inhibition occurred when the same total dose was administered intraperitoneally more than 15 phytochemicals have been tested successfully by this formulation together our data indicate that this novel implant delivery system circumvents oral bioavailability issues provides continuous delivery for long durations and lowers the total administered dose eliciting both chemopreventive chemotherapeutic activities this would also all ow the assessment of activity of minor constituents and synthetic metabolites which otherwise remain uninvestigated in vivo the author 2012 published by oxford university press all rights reserved non cross linked porcine based collagen i iii membranes do not require high vascularization rates for their integration within the implantation bed a paradigm shift there are conflicting reports concerning the tissue reaction of small animals to porcine based non cross linked collagen i iii membranes matrices for use in guided tissue bone regeneration the fast degradation of these membranes matrices combined with transmembrane vascularization within 4 weeks has been observed in rats compared with the slow vascularization and continuous integration observed in mice the aim of the present study was to analyze the tissue reaction to a porcine based non cross linked collagen i iii membrane in mice using a subcutaneous implantation model the membrane was implanted subcutaneously in mice for up to 60 days the extent of scaffold vascularization tissue integration and scaffold thickness were assessed using general and specialized histological methods together with a unique histomorphometrical analysis technique a dense bombyx mori derived silk fibroin membrane was used as a positive control whilst a polytetrafluoroethylene ptfe membrane served as a negative control within the observation period the collagen membrane induced a mononuclear cellular tissue response including anti inflammatory macrophages and the absence of multinucleated giant cells within its implantation bed transmembrane scaffold vascularization was not observed whereas a mild scaffold vascularization was generated through microvessels located at both scaffold surfaces however the silk fibroin induced a mononuclear and multinucleated cell based tissue response in which pro inflammatory macrophages and multinucleated giant cells were associated with an increasing transmembrane scaffold vascularization and a breakdown of the membrane within the experimental period the ptfe membrane remained as a stable barrier throughout the study and visible cellular degradation was not observed however multinucleated giant cells were located on both interfaces the present study demonstrated that the tested non cross linked collagen membrane remained as a stable barrier membrane throughout the study period the membrane integrated into the subcutaneous connective tissue and exhibited only a mild peripheral vascularization without experiencing breakdown the silk fibroin in contrast induced granulation tissue formation which resulted in its high vascularization and the breakdown of the material over time the presence of multinucleated giant cells at both interfaces of the pfte membrane is a sign of its slow cellular biodegradation and might lead to adhesions between the membrane and its surrounding tissue this hypothesis could explain the observed clinical complications associated with the retrieval of these materials after guided tissue regeneration 2012 acta materialia inc published by elsevier ltd all rights reserved angiogenesis biogide mucograft non cross linked collagen membrane vascularization using continuous integration of code and content to teach software engineering with limited resources previous courses addressing the gap between student and professional programming practice have either isolated small groups development in such a way that larger scale difficulties that motivate many professional practices do not arise or have required significant additional staffing that would be expensive to provide in a large cohort core undergraduate software engineering course we describe the first iteration of a course that enabled 73 students to work together to improve a large common legacy code base using professional practices and tools staffed only by two lecturers and two undergraduate students employed as part time tutors the course relies on continuous integration and automated metrics that coalesce frequently updated information in a manner that is visible to students and can be monitored by a small number of staff the course is supported by a just in time teaching programme of thirty two technical topics we describe the constraints that determined the design of the course and quantitative and qualitative data from the first iteration of the course 2012 ieee continuous integration experience report resource constraints software engineering studio course ambient awareness of build status in collocated software teams we describe the evaluation of a build awareness system that assists agile software development teams to understand current build status and who is responsible for any build breakages the system uses ambient awareness technologies providing a separate easily perceived communication channel distinct from standard team workflow multiple system configurations and behaviours were evaluated an evaluation of the system showed that while there was no significant change in the proportion of build breakages the overall number of builds increased substantially and the duration of broken builds decreased team members also reported an increased sense of awareness of and responsibility for broken builds and some noted the system dramatically changed their perception of the build process making them more cognisant of broken builds 2012 ieee ambient awareness build processes continuous integration software teams status information the 3c approach for agile quality assurance continuous integration is an agile practice for the continuous integration of new source code into the code base including the automated compile build and running of tests from traditional quality assurance we know software metrics as a very good approach to measure software quality combining both there is a promising approach to control and ensure the internal software quality this paper introduces the 3c approach which is an extension to the agile practice continuous integration it adds continuous measurement and continuous improvement as subsequent activities to ci and establishes metric based quality gates for an agile quality assurance it was developed and proven in an agile maintenance and evolution project for the automotive industry at t systems international a large german ict company within the project the approach was used for a legacy java based web application including the use of open source tools from the java eco system but the approach is not limited to these technical boundaries as similar tools are available also for other technical platforms 2012 ieee agile continuous integration empirical case study quality assurance software metrics a holistic approach to developing a progress tracking system for distributed agile teams managing development progress in agile projects is a co ordination problem several technical factors e g source code versioning unit testing acceptance testing continuous integration and releasing affect the progress status hence they need to be managed in distributed agile projects manual based methods such as communication via video conferencing tools are insufficient distributed agile teams find it difficult with infrequent communications to understand how the work of one team member at one site influences the work progress of another team member at a different site moreover current technologybased methods such as project management tools are static and rely completely on team members to consider any impact of the technical factors on development progress the lack of mechanisms to effectively manage progress change resulting from the technical factors may lead to the project being delayed or to low quality code in this paper we propose a holistic approach to developing a progress tracking system that supports identifying and co ordinating the impact of the various technical factors on the development progress the result will provide distributed agile teams with an improved awareness of the actual progress 2012 ieee agile development co ordination geographically distributed project progress tracking the entity system architecture and its application in an undergraduate game development studio this paper reports on the software architecture methodology and student learning experience from the development of morgan s raid an educational game designed to teach fourth graders about indiana s civil war history the game was designed and developed during the 2010 2011 academic year and involved a multidisciplinary undergraduate team the team used industrial best practices of game development including continuous integration distributed version control test driven development and scrum this methodology was supported by an entity system architecture a software architectural design pattern that addresses many shortcomings of inheritance based game engine architectures detailed definitions of the entity system architecture and methodology are presented along with the experience report this combination of methodology and architecture directly contributed to the success of the project both as a software development exercise and as a learning experience copyright 2012 acm education entity system architecture game development studio learning cluster as a service for self deployable cloud applications cloud computing environments have certain techniques to make applications auto scale according to the load the environment is also used for disaster recovery for instance environments may allow applications to move to other cloud environments in the event of disasters some applications like continuous integration tests need to have execution environments that depend on their test running context currently these application types are treated individually for development however they can be developed as self deployable applications and the cluster as a service foundation supports them this paper shows the possibility of this idea and discusses a prototype foundation 2012 ieee cloud computing cluster as a service infrastructure as a service selfdeployable application sexual activity increases the number of newborn cells in the accessory olfactory bulb of male rats in rodents sexual behavior depends on the adequate detection of sexually relevant stimuli the olfactory bulb ob is a region of the adult mammalian brain undergoing constant cell renewal by continuous integration of new granular and periglomerular neurons in the accessory aob and main mob olfactory bulbs the proliferation migration survival maturation and integration of these new cells to the ob depend on the stimulus that the subjects received we have previously shown that 15 days after females control paced the sexual interaction an increase in the number of cells is observed in the aob no changes are observed in the number of cells when females are not allowed to control the sexual interaction in the present study we investigated if in male rats sexual behavior increases the number of new cells in the ob male rats were divided in five groups 1 males that did not receive any sexual stimulation 2 males that were exposed to female odors 3 males that mated for 1 h and could not pace their sexual interaction 4 males that paced their sexual interaction and ejaculated one time and 5 males that paced their sexual interaction and ejaculated three times all males received three injections of the dna synthesis marker bromodeoxyuridine at 1h intervals starting 1 h before the beginning of the behavioral test fifteen days later males were sacrificed and the brains were processed to identify new cells and to evaluate if they differentiated into neurons the number of newborn cells increased in the granular cell layer grcl also known as the internal cell layer of the aob in males that ejaculated one or three times controlling paced the rate of the sexual interaction some of these new cells were identified as neurons in contrast no significant differences were found in the mitral cell layer also known as the external cell layer and glomerular cell layer glcl of the aob in addition no significant differences were found between groups in the mob in any of the layers analyzed our results indicate that sexual behavior in male rats increases neurogenesis in the grcl of the aob when they control the rate of the sexual interaction 2012 portillo unda camacho sánchez corona arzate díaz and paredes accessory olfactory bulb and main olfactory bulb neurogenesis sexual behavior new agile testing modes software is considered to have high quality only if it has undergone a series of good test cases testdriven development is a software development practice that leads to better quality and fewer defects in code the purpose of study of this paper is to evaluate the difference between a traditional testing and agile testing the testing techniques involved in agile projects the different modes of testing the do s and don ts of testing and also about the set of activities an agile tester needs to keep in mind while working on agile projects a study has been conducted in a software firm in chennai india two different testing modes have been proposed based on the testing modes that is projected in this article the do s and don ts of a software tester and their activities has been formulated a simple yes no type questionnaire is included in this article which is given to the testers before testing answering the simple yes no questions makes a tester to be an efficient tester in his job by following the do s and don ts guidelines and the activities of a tester given in this article it is found that the efficiency of an employee involved in agile testing has been improved considerably it is concluded that testing plays a vital role for any project testing would be more efficient if a parallel testing could be done in par with the development team better quality could be achieved if the project team follows a continuous integration with a continuous delivery and a continuous feedback from the customer 2012 asian network for scientific information extreme programming quality assurance test driven development test strategy testing modes testing techniques sexual activity increases the number of newborn cells in the accessory olfactory bulb of male rats in rodents sexual behavior depends on the adequate detection of sexually relevant stimuli the olfactory bulb ob is a region of the adult mammalian brain undergoing constant cell renewal by continuous integration of new granular and periglomerular neurons in the accessory aob and main mob olfactory bulbs the proliferation migration survival maturation and integration of these new cells to the ob depend on the stimulus that the subjects received we have previously shown that 15 days after females control paced the sexual interaction an increase in the number of cells is observed in the aob no changes are observed in the number of cells when females are not allowed to control the sexual interaction in the present study we investigated if in male rats sexual behavior increases the number of new cells in the ob male rats were divided in five groups 1 males that did not receive any sexual stimulation 2 males that were exposed to female odors 3 males that mated for 1 h and could not pace their sexual interaction 4 males that paced their sexual interaction and ejaculated 1 time and 5 males that paced their sexual interaction and ejaculated 3 times all males received three injections of the dna synthesis marker bromodeoxyuridine at 1h intervals starting 1h before the beginning of the behavioral test fifteen days later males were sacrificed and the brains were processed to identify new cells and to e valuate if they differentiated into neurons the number of newborn cells increased in the granular cell layer also known as the internal cell layer of the aob in males that ejaculated one or three times controlling paced the rate of the sexual interaction some of these new cells were identified as neurons in contrast no significant differences were found in the mitral cell layer also known as the external cell layer and glomerular cell layer of the aob in addition no significant differences were found between groups in the mob in any of the layers analyzed our results indicate that sexual behavior in male rats increases neurogenesis in the granular cell layer of the aob when they control the rate of the sexual interaction 2012 portillo unda camacho sanchez corona arzate díaz and paredes accessory olfactory bulb and main olfactory bulb neurogenesis sexual behavior how well does test case prioritization integrate with statistical fault localization context effective test case prioritization shortens the time to detect failures and yet the use of fewer test cases may compromise the effectiveness of subsequent fault localization objective the paper aims at finding wh ether several previously identified effectiveness factors of test case prioritization techniques namely strategy coverage granularity and time cost have observable consequences on the effectiveness of statistical fault localization techniques method this paper uses a controlled experiment to examine these factors the experiment includes 16 test case prioritization techniques and four statistical fault localization techniques using the siemens suite of programs as well as grep gzip sed and flex as subjects the experiment studies the effects of the percentage of code examined to locate faults from these benchmark subjects after a given number of failures have been observed results we find that if testers have a budgetary concern on the number of test cases for regression testing the use of test case prioritization can save up to 40 of test case executions for commit builds without significantly affecting the effectiveness of fault localization a statistical fault localization technique using a smaller fraction of a prioritized test suite is found to compromise its effectiveness seriously despite the presence of some variations the inclusion of more failed test cases will generally improve the fault localization effectiveness during the integration process interestingly during the variation periods adding more failed test cases actually deteriorates the fault localization effectiveness in terms of strategies random is found to be the most effective followed by the art and additional strategies while the total strategy is the least effective we do not observe sufficient empirical evidence to conclude that using different coverage granularity levels have different overall effects conclusion the paper empirically identifies that strategy and time cost of test case prioritization techniques are key factors affecting the effectiveness of statistical fault localization while coverage granularity is not a significant factor it also identifies a mid range deterioration in fault localization effectiveness when adding more test cases to facilitate debugging 2012 elsevier b v all rights reserved adaptive random testing continuous integration coverage software process integration statistical fault localization test case prioritization design of a single aav vector for coexpression of th and gch1 to establish continuous dopa synthesis in a rat model of parkinson s disease preclinical efficacy of continuous delivery of 3 4 dihydroxyphenylalanine dopa with adeno associated viral aav vectors has recently been documented in animal models of parkinson s disease pd so far all studies have utilized a mix of two monocistronic vectors expressing either of the two genes tyrosine hydroxylase th and gtp cyclohydrolase 1 gch1 needed for dopa production here we present a novel vector design that enables efficient dopa production from a single aav vector in rats with complete unilateral dopamine da lesions functional efficacy was assessed with drug induced and spontaneous motor behavioral tests where vector treated animals showed near complete and stable recovery within 1 month recovery of motor function was associated with restoration of extracellular da levels as assessed by online microdialysis histological analysis showed robust transge ne expression not only in the striatum but also in overlying cortical areas in globus pallidus we noted loss of neun staining which might be due to different sensitivity in neuronal populations to transgene expression taken together we present a single aav vector design that result in efficient dopa production and wide spread transduction this is a favorable starting point for continued translation toward a therapeutic application although future studies need to carefully review target region vector spread and dilution with this approach the american society of gene cell therapy knowledge continuous integration process k cip social semantic web creates read write spaces where users and smart agents collaborate to produce knowledge readable by humans and machines an important issue concerns the ontology evolution and evaluation in man machine collaboration how to perform a change on ontologies in a social semantic space that currently uses these ontologies through requests in this paper we propose to implement a continuous knowledge integration process named k cip we take advantage of man machine collaboration to transform feedback of people into tests this paper presents how k cip can be deployed to allow fruitful man machine collaboration in the context of the wikitaaable system copyright is held by the international world wide web conference committee iw3c2 continuous integration process knowledge management ontology semantic wiki using acceptance tests to validate accessibility requirements in ria accessibility stands as a quality requirement for web applications however current accessibility automatic evaluation tools are not capable of evaluating dom dynamic generated content that characterizes ajax applications and rias rich internet applications in this context this paper describes an approach for testing accessibility requirements in ria by using acceptance tests the authors had implemented a set of assistive technology user scenarios in the acceptance tests in order to guarantee keyboard accessibility in web applications as the scenarios were implemented as acceptance tests scenarios they provide accessibility analysis over all layers of the software from server side to client side implementations javascript and dynamically generated dom elements in ria the test scenarios are automatically executed and by doing so fit the continuous integration process of constant delivery of new functionalities in web projects 2012 acm acceptance testing continuous integration web accessibility influences on agile practice tailoring in enterprise software development agile development projects have become a reality in large enterprises using offshore development models a case study involving seven international companies with offices in bangalore india and london uk was conducted including interviews with 19 practitioners the contribution of this paper is to illustrate the reasons for tailoring agile practices within the context of large enterprises the findings show that scrum roles and practices did not conflict with enterprise policies or processes and were thought to improve product quality and productivity however agile practices from the xp tradition were not so widely adopted test driven development did not integrate well within enterprises where independent quality assurance teams were constituted as separate departments continuous integration was found to be challenging where enterprise software products required time consuming regression testing and elaborate code release processes while adoption of coding standards and collective code ownership are necessary to facilitate interaction between disparate stakeholder groups 2012 ieee distributed agile development enterprise software extreme programming xp scrum tailoring emergence of agile methods perceptions from software practitioners in malaysia agile methods are an established process for developing software nowadays there is however less evidence on their usage among software practitioners in malaysia while the methods have become mainstream in other regions that is not the case in this country this paper empirically investigates the perceptions of agile methods usage from seven organisations involving 14 software practitioners in malaysia our participants are using scrum and have a maximum of five years experience we categorised our findings in terms of awareness introduction and challenges they are facing together with the suggested and practiced solution from them interestingly a change in mind set when practicing agile was identified to be helpful in reducing the challenges lastly we present the practices in agile they perceived to deliver the most benefits we found that the use of agile is still emerging in the country and awareness is still lacking especially within the government sector although several challenges have been encountered when introducing agile in their organisations the benefits of agile are reported to be in agile practices such as the involvement from all parties from the beginning daily stand up meeting iterative and incremental applying burn down chart sprint and continuous integration we aim to provide awareness and knowledge about agile methods to the practitioners in the country and the nearby region this paper can serve as a reference to the early adopters who intend to use agile methods in the future 2012 ieee agile methods awareness benefits challenges introduction perceptions practices software process system development for outdoor soilless production of leek allium porrum current open field vegetable production systems in the netherlands do not meet market and society demands among the most important demands are 1 the eu water framework directive requesting strong reductions in emissions of nutrients and pesticides to surface and groundwater 2 continuous delivery of uniform high quality products by retailers these demands could not be fulfilled by adapting current production systems new production systems have to be developed and tested for a number of field grown crops this paper emphasizes the development of soilless growing systems for leek important current product requirements for leek are a white part of the stem of at least 14 cm a minimal thickness of 2 5 cm and a straight firm stem important system requirements for outdoor cultivation are handling of rainfall wind and frost opportunities for soilless systems for leek are a clean product without soil contamination and a strong reduction in labor need for preparing it for the market finally the developed system needs to be economically viable acceptable by growers and give sufficient reduction in emissions from 2008 aspects of plant raising traditional method with stone wool and coir plugs were investigated together with demands to get a firm straight stem covering of the plants to achieve a long white part was investigated comparing several smart solutions the use of substrate nft and deep flow were also compared results show a fast growing crop where 4 crops per year with at least 80 plants per m 2 and a total yield up to 300 t ha are possible it can achieve the minimal market weight in a much shorter period 40 60 days in summer compared to growing in the soil however the production of long white stems requires special measures with a good system design there are hardly any problems with rain and frost dft hydroponics nft open field vegetables substrate effects of a novel estrogen free progesterone receptor modulator contraceptive vaginal ring on inhibition of ovulation bleeding patterns and endometrium in normal women background progesterone receptor modulators prms delivered by contraceptive vaginal rings provide an opportunity for development of an estrogen free contraceptive that does not require daily oral intake of steroids the objective of this proof of concept study was to determine whether continuous delivery of 600 800 mcg of ulipristal acetate upa from a contraceptive vaginal ring could achieve 80 to 90 inhibition of ovulation study design this was a prospective controlled open labeled multicenter international trial to examine the effectiveness and safety of this prototype vaginal ring thirty nine healthy women 21 40 years old and not at risk of pregnancy were enrolled at three clinic sites volunteers participated in a control cycle a 12 week treatment period and a post treatment cycle pharmacodynamic effects on follicular function and inhibition of ovulation effects on endometrium bleeding patterns and serum upa levels were evaluated results mean upa levels during treatment were nearly constant approximately 5 1 ng ml throughout the study ovulation was documented in 32 of 111 4 week treatment cycles a correlation was observed between serum upa and degree of inhibition of ovarian activity there was no evidence of hyperplasia of endometrium but prm associated endometrial changes were frequently observed 41 conclusion in this study the minimum effective contraceptive dose was not established further studies are required testing higher doses of upa to attain ovulation suppression in a higher percentage of subjects 2012 elsevier inc contraceptive vaginal ring follicular development ovulation inhibition prm associated endometrial changes progesterone receptor modulators ulipristal acetate nitroglycerin prevents coagulopathies and foetal death associated with abnormal maternal inflammation in rats inflammation associated foetal loss is often linked to maternal coagu lopathies here we characterised the role of maternal inflammation in the development of various systemic maternal coagulopathies and foetal death during mid to late gestation in rats since nitric oxide no functions as an inhibitor of platelet aggregation and anti oxidant we also tested whether the no mimetic nitroglycerin glyceryl trinitrate gtn prevents inflammation associated coagulopathies and foetal death to induce chronic inflammation pregnant wistar rats were injected with low doses of lipopolysaccharide lps 10 40 μg kg on ges tational days gd 13 5 16 5 to determine whether the effects of inflammation are mediated by tumour necrosis factor α tnf α the tnf α inhibitor etanercept was injected on gd 13 5 and 15 5 controls consisted of rats injected with saline gtn was administered to lps treated rats via daily application of a transdermal patch on gd 12 5 16 5 using thromboelastography teg various coagulation parameters were assessed on gd 17 5 foetal viability was determined morphologically reference coagulation parameters were established based on teg results obtained from control animals lps treated rats exhibited distinct systemic coagulopathies hypercoagulability hypo coagulability hyperfibrinolysis and disseminated intravascular coagulation dic stages i and iii a specific foetal death coagulation pheno type was observed implicating teg as a potential tool to identify inflammation induced haemostatic alterations associated with pregnancy loss treatment with etanercept reduced the incidence of coagulo pathy by 47 while continuous delivery of gtn prevented foetal death and the inflammation induced coagulopathies these findings provide a rationale for investigating the use of gtn in the prevention of maternal coagulopathies and inflammation mediated foetal death schattauer 2012 disseminated intravascular coagulation dic etanercept nitric oxide no pregnancy thromboelastography a microfluidic platform for controlled biochemical stimulation of twin neuronal networks spatially and temporally resolved delivery of soluble factors is a key feature for pharmacological applications in this framework microfluidics coupled to multisite electrophysiology offers great advantages in neuropharmacology and toxicology in this work a microfluidic device for biochemical stimulation of neuronal networks was developed a micro chamber for cell culturing previously developed and tested for long term neuronal growth by our group was provided with a thin wall which partially divided the cell culture region in two sub compartments the device was reversibly coupled to a flat micro electrode array and used to culture primary neurons in the same microenvironment we demonstrated that the two fluidic ally connected compartments were able to originate two parallel neuronal networks with similar electrophysiological activity but functionally independent furthermore the device allowed to connect the outlet port to a syringe pump and to transform the static culture chamber in a perfused one at 14 days invitro sub networks were independently stimulated with a test molecule tetrodotoxin a neurotoxin known to block action potentials by means of continuous delivery electrical activity recordings proved the ability of the device configuration to selectively stimulate each neuronal network individually the proposed microfluidic approach represents an innovative methodology to perform biological pharmacological and electrophysiological experiments on neuronal networks indeed it allows for controlled delivery of substances to cells and it overcomes the limitations due to standard drug stimulation techniques finally the twin network configuration reduces biological variability which has important outcomes on pharmacological and drug screening 2012 american institute of physics making software integration really continuous the earlier merge conflicts are detected the easier it is to resolve them a recommended practice is for developers to frequently integrate so that they detect conflicts earlier however manual integrations are cumbersome and disrupt programming flow so developers commonly defer them besides manual integrations do not help to detect conflicts with uncommitted code of co workers consequently conflicts grow over time thus making resolution harder at late stages we present a solution that continuously integrates in the background uncommitted and committed changes to support automatic detection of conflicts emerging during programming to do so we designed a novel merge algorithm that is o n complex and implemented it inside an ide thus promoting a metaphor of continuous merging similar to continuous compilation evidence from controlled experiments shows that our solution helps developers to become aware of and resolve conflicts earlier than when they use a mainstream version control system 2012 springer verlag berlin heidelberg conflict detection continuous integration continuous merging software merging version control reliability and efficacy of a new co inf 2 inf laser hollow fiber a prospective study of 39 patients we present the first series of patients treated by transoral laser surgery tls using the new acupulse 40wg co 2 laser with the fiberlase flexible waveguide co 2 lwg lumenis santa clara ca with the objective to test its reliability and efficacy patients older than 18 years with oral pharyngo laryngeal or tracheal benign or premalignant lesions were enrolled after signing an informed consent this prospective study was conducted between october 2010 and may 2011 in two tertiary care university hospitals thirty nine patients were enrolled in the study the mean age was 47 9 years range 18 86 years there were 21 women and 18 men thirteen patients had hypertrophy of lymphoid tissue palatine and or lingual nine patients had granulomas four patients had an exudative glottic lesion three patients had severe dysplasia glottic and supraglottic three patients had leukoplakia two patients had glottal cysts two patients had laryngeal papilloma two patients had bilateral paralysis of the vocal folds and one patient suffered from spasmodic dysphonia eighty two percent of the procedures were performed under general anesthesia with laryngo tracheal intubation the co 2 fiber passed through a handpiece was used with a microscope in the majority of the procedures the laser delivery mode parameter used was superpulse or continuous wave power levels were 3 15 watts w continuous delivery each procedure utilized one co 2 fiber which performed adequately throughout the procedure no complications were noted with the use of this technology a bipolar cautery was needed to control bleeding in eight procedures all these procedures were tonsillectomies the co 2 lwg is a safe and reliable tool for tls it is durable enough to last through the entire surgical procedure without the need for replacement its use must be tailored depending on the type and location of the lesion the co 2 lasers tissue effects as well as the surgeon s experience 2011 springer verlag adults co laser wave guide 2 larynx pharynx transoral laryngeal microsurgery a course for developing personal software engineering competencies the strength of a software development team is the sum of the capabilities of each individual team member there exist at the personal level core software engineering competencies that need to be cultivated to allow an individual to fulfill their potential as an effective team contributor students in a course introducing team based software engineering typically possess adequate introductory programming skills but often lack other competencies required to execute a software project successfully students have rarely been introduced to concepts beyond programming such as estimation and planning continuous integration detailed design debugging and unit testing part of being a software engineer is the knowledge of multiple programming languages and tools without such knowledge it is impossible to make intelligent engineering decisions contemporary education philosophy stresses active student initiative and personal responsibility learning in our case with a rapidly evolving technology landscape students must come to realize that as in the workplace many skills are not so much taught as learned this paper captures our experiences with a second year software engineering course designed to address these challenges in addition to discussing the topics covered in the course we also present active and cooperative learning practices utilized in class activities 2012 american society for engineering education improving the quality of emi releases by leveraging the emi testing infrastructure what is an emi release what is its life cycle how is its quality assured through a continuous integration and large scale acceptance testing these are the main questions that this article will answer by presenting the emi release management process with emphasis on the role played by the testing infrastructure in improving the quality of the middleware provided by the project the european middleware initiative emi is a close collaboration of four major european technology providers arc glite unicore and dcache its main objective is to deliver a consolidated set of components for deployment in egi as part of the unified middleware distribution umd prace and other dcis the harmonized set of emi components thus enables the interoperability and integration between grids emi aims at creating an effective environment that satisfies the requirements of the scientific communities relying on it the emi distribution is organized in periodic major releases whose development and maintenance follow a 5 phase yearly cycle i requirements collection and analysis ii development and test planning iii software development testing and certification iv release certification and validation and v release and maintenance in this article we present in detail the implementation of operational and infrastructural resources supporting the certification and validation phase of the release the main goal of this phase is to harmonize into a single release the strongly inter dependent products coming from various development teams through parallel certification paths to achieve this goal the continuous integration and large scale acceptance testing performed on the emi testing infrastructure plays a key role the purpose of this infrastructure is to provide a system where both the production and the release candidate product versions are deployed on this system inter component testing by different product team testers can concurrently take place the testing infrastructure is also continuously monitored through nagios and exposed both to automatic testing and to usage by volunteer end users furthermore the infrastructure size is increased with resources made available by volunteer end users that are interested in implementing production like deployments or specific test scenarios improving the quality of emi releases by leveraging the emi testing infrastructure what is an emi release what is its life cycle how is its quality assured through a continuous integration and large scale acceptance testing these are the main questions that this article will answer by presenting the emi release management process with emphasis on the role played by the testing infrastructure in improving the quality of the middleware provided by the project the european middleware initiative emi is a close collaboration of four major european technology providers arc glite unicore and dcache its main objective is to deliver a consolidated set of components for deployment in egi as part of the unified middleware distribution umd prace and other dcis the harmonized set of emi components thus enables the interoperability and integration between grids emi aims at creating an effective environment that satisfies the requirements of the scientific communities relying on it the emi distribution is organized in periodic major releases whose development and maintenance follow a 5 phase yearly cycle i requirements collection and analysis ii development and test planning iii software development testing and certification iv release certification and validation and v release and maintenance in this article we present in detail the implementation of operational and infrastructural resources supporting the certification and validation phase of the release the main goal of this phase is to harmonize into a single release the strongly inter dependent products coming from various development teams through parallel certification paths to achieve this goal the continuous integration and large scale acceptance testing performed on the emi testing infrastructure plays a key role the purpose of this infrastructure is to provide a system where both the production and the release candidate product versions are deployed on this system inter component testing by different product team testers can concurrently take place the testing infrastructure is also continuously monitored through nagios and exposed both to automatic testing and to usage by volunteer end users furthermore the infrastructure size is increased with resources made available by volunteer end users that are interested in implementing production like deployments or specific test scenarios the fairroot framework the fairroot framework is an object oriented simulation reconstruction and data analysis framework based on root it includes core services for detector simulation and offline analysis the framework delivers base classes which enable the users to easily construct their experimental setup in a fast and convenient way by using the virtual monte carlo concept it is possible to perform the simulations using either geant3 or geant4 without changing the user code or the geometry description using and extending the task mechanism of root it is possible to implement complex analysis tasks in a convenient way moreover using the faircuda interface of the framework it is possible to run some of these tasks also on gpu data io as well as parameter handling and data base connections are also handled by the framework since some of the experiments will not have an experimental setup with a conventional trigger system the framework can handle also free flowing input streams of detector data for this mode of operation the framework provides classes to create the needed time sorted input streams of detector data out of the event based simulation data there are also tools to do radiation studies and to visualize the simulated data a cmake cdash based building and monitoring system is also part of the fairroot services which helps to build and test the framework on many different platforms in an automatic way including also continuous integration monitoring of agricultural landscape in norway an overall societal aim is to ensure a sustainable use and management of agricultural landscapes this requires continuous delivery of reliable and up to date information to decision makers to be able to deliver this information a monitoring program for agricultural landscapes was initiated in norway 13 years ago the program documents and reports on land use land cover changes from data captured through interpretation of true colour aerial photos using stereo instruments the monitoring programme is based on a sample of 1000 squares of 1 x 1 km and the entire sample of squares is photographed over a five year period each square is then mapped repeatedly every fifth year to record changes aerial photo interpretation is based on a custom classification system which is built up hierarchically with three levels the first level comprises seven land type classes agricultural land bare ground semi natural open vegetation unforested wetland vegetation forest urban areas and water these land classes are further divided into 24 land types at level two and approximately 100 land types at level 3 in addition to land type units we map both line elements like stone fences and point elements like buildings and solitary threes by use of indicators that describe status and change focusing on themes of particular policy interest we can report on whether policy aims are being fulfilled or not four indicator themes have been in focus hitherto landscape spatial structure biological diversity cultural heritage and accessibility our data is stored in databases and most of the data quality check structure process and analyses are now being made in open source software like postgis and postsql to assess the accuracy of the photo interpretation ground truthing is carried out on 10 of the squares the results of this operation document the benefits of having access to photos of the same area from two different years the program is designed first and foremost to provide reliable statistics at a national level but the aim is also to report at regional levels for example for counties or for agricultural landscape regions the national coverage and application of standardized methods enable frequent updating this method is cost effective and enables us to quantify changes in landscape qualities as well as adapting the programme to take account of e g new findings on relevant indicators aerial agriculture change detection monitoring resources stereoscopic integrating early v amp v support to a gse tool integration platform the ever growing market pressure and complex products demand high quality work and effectiveness from software practitioners this relates also for the methods and tools they use for the development of software intensive systems validation and verification v v are the cornerstones of the overall quality of a system by performing efficient v v activities to detect defects during the early phases of development the developers are able to save time and effort required for fixing them tool support is available for all types of v v activities especially testing model checking syntactic verification and inspection in distributed development the role of tools is even more relevant than in single site development and tool integration is often imperative for ensuring the effectiveness of work in this paper we discuss how a tool integration framework was extended to support early v v activities via continuous integrations we find that integrating early v v supporting tools is feasible and useful and makes a tool integration framework even more beneficial 2011 ieee continuous integration global software integration tool integration v amp v tools verfication and validation co ordination support for managing progress of distributed agile projects progress in agile development is determined by the amount of working software produced source code versioning unit testing continuous integration and acceptance testing at are technical factors that affect the maturity of the software artefects produced therefore development progress is subject to change due to impact of these technical factors e g modifying source code artefects may affect completed user stories in co located agile projects face to face interaction is used to share information about changes that may affect development progress however in distributed projects team members find it harder to maintain an awareness of these changes which affects their understanding of the development progress this causes them to rely on less accurate progress information and contributes in producing low quality code and unnecessary rework and delays in this paper we propose a holistic approach that supports management of the development progress in geographically distributed agile projects by identifying and co ordinating the impact of the technical factors on progress which will provide distributed agile teams with improved awareness of the actual progress of the software 2011 ieee agile development co ordination geographically distributed project progress tracking including xslt stylesheets testing in continuous integration process resubmission developing knowledge systems with continuous integration with the industrial success of knowledge based systems new requirements with respect to knowledge engineering processes arise besides advanced knowledge acquisition tools novel techniques for the quality assurance need to be established in order to maintain a safe development process in software engineering the application of continuous integration as a collection of practices has proved to be suitable for this task in this paper we transfer the general ideas of continuous integration from software engineering to knowledge engineering and we demonstrate the implementation of a continuous integration tool into a state of the art knowledge engineering workbench a case study reports on the successful application and its beneficial use common agile practices in software processes objective to investigate studies about software processes looking for practices which can be used to obtain agility in software processes method a systematic review including seven search engines was executed in feb 2010 to apply the defined criteria to select papers and extract information regarding working practices bringing agility to software processes results from 6696 retrieved papers 441 were selected to support the identification of 236 occurrences of 51 distinct practices associated with the concept of agility their descriptions were deeply analyzed and consolidated after discarding those which appeared in the technical literature in a small amount of papers 17 agile practices were identified conclusion although further studies are necessary to evaluate the efficacy of these 17 agile practices 12 of them have been more commonly approached in the software projects and could be primarily considered test driven development continuous integration pair programming planning game onsite customer collective code ownership small releases metaphor refactoring sustainable pace simple design and coding standards 2011 ieee agile methods agile practices agile software processes evidence based software engineering systematic review towards behavior driven operations bdops modern enterprise software systems entail many challenges such as availability scalability complexity and providing business agility ensuring the systems to be up and running for 24 × 7 has become a mandate for operations agile development has been adopted to keep pace with the demands of business and it test driven development tdd and behavior driven development bdd are practices which enable agile development so far the agile approach has been limited to development for ensuring business to be truly agile we need to take forward the agile approach to operations in this paper we discuss the behavior driven approach for operations specifically on the core sub systems like infrastructure provisioning deployment and monitoring we share our explorations and experiments with behavior driven monitoring bdm and how the same can be adopted for infrastructure provisioning and deployment we used cucumber nagios to detect behavior of an enterprise application we close this paper with a note on the benefits to busmess and it showing its relevance to devops continuous delivery and cloud computing 2011 iet behavior driven development behavior driven infrastructure behavior driven monitoring behavior driven operations cucumber nagios current trends in ndds with special reference to nsaids transdermal drug delivery system has been in existence for a long time transdermal therapeutic systems have been designed to provide controlled continuous delivery of drugs via the skin to the systemic circulation non steroidal anti inflammatory drugs used as analgesic anti inflammatory and antipyretic in the treatment of rheumatoid arthritis and osteoarthritis but the clinical use is often limited because of adverse effect such as irritation and ulceration of the gastrointestinal tract these drugs have a relatively short half life in plasma and have the potential to be delivered topically it also has low molecular weight nsaids are an excellent drug for transdermal delivery furthermore topical administration via the dermal route can bypass disadvantages of the oral route therefore transdermal drug delivery has been considered to be an ideal route for administration of nsaids ketoprofen nsaids rheumatoid arthritis transdermal functional testing of complex event processing applications complex event processing cep is a powerful technology for implementing real time applications it offers efficient processing of occurring information pieces so called events based on an architecture that provides strong decoupling of its components event producer event processors and event consumer 4 for this cep is seen as an important contribution in future information systems cep applications already have been successfully applied in different fields of industry like finance logistics and manufacturing vendors of cep products promise agile real time systems as you are able to change business logic on the fly but this promise brings risks without comprehensive quality assurance changing small pieces of these complex systems can lead to unpredictable errors modern agile methodologies rely on strong quality assurance techniques like test driven development continuous integration or automated user acceptance tests 27 these methodologies will also be important in the development of cep applications this paper gives an overview about what requirements cep products will have to meet to support up to date software quality assurance 2011 ieee complex event processing functional testing quality assurance computing streamfunction and velocity potential in a limited domain of arbitrary shape part ii numerical methods and test experiments built on the integral formulas in part i numerical methods are developed for computing velocity potential and streamfunction in a limited domain when there is no inner boundary around a data hole inside the domain the total solution is the sum of the internally and externally induced parts for the internally induced part three numerical schemes grid staggering local nesting and piecewise continuous integration are designed to deal with the singularity of the green s function encountered in numerical calculations for the externally induced part by setting the velocity potential or streamfunction component to zero the other component of the solution can be computed in two ways 1 solve for the density function from its boundary integral equation and then construct the solution from the boundary integral of the density function 2 use the cauchy integral to construct the solution directly the boundary integral can be discretized on a uniform grid along the boundary by using local nesting or piecewise continuous integration the scheme is refined to enhance the discretization accuracy of the boundary integral around each corner point or along the entire boundary when the domain is not free of data holes the total solution contains a data hole induced part and the cauchy integral method is extended to construct the externally induced solution with irregular external and internal boundaries an automated algorithm is designed to facilitate the integrations along the irregular external and internal boundaries numerical experiments are performed to evaluate the accuracy and efficiency of each scheme relative to others 2011 chinese national committee for international association of meteorology and atmospheric sciences institute of atmospheric physics science press and springer verlag berlin heidelberg domain of arbitrary shape numerical method streamfunction velocity potential developing knowledge systems with continuous integration with the industrial success of knowledge based systems new requirements with respect to knowledge engineering processes arise besides advanced knowledge acquisition tools novel techniques for the quality assurance need to be established in order to maintain a safe development process in software engineering the application of continuous integration as a collection of practices has proved to be suitable for this task in this paper we transfer the general ideas of continuous integration from software engineering to knowledge engineering and we demonstrate the implementation of a continuous integration tool into a state of the art knowledge engineering workbench continuous integration knowledge engineering testing stories from my experiences learning scrum an experienced read old software developer recounts the ups and downs of learning scrum as part of a geographically distributed team working on a pre release product 2011 ieee agile burndown chart collaboration continuous integration estimation scrum software development time tracking on practical adequate test suites for integrated test case prioritization and fault localization an effective integration between testing and debugging should address how well testing and fault localization can work together productively in this paper we report an empirical study on the effectiveness of using adequate test suites for fault localization we also investigate the integration of test case prioritization and statistical fault localization with a postmortem analysis approach our results on 16 test case prioritization techniques and four statistical fault localization techniques show that although much advancement has been made in the last decade test adequacy criteria are still insufficient in supporting effective fault localization we also find that the use of branch adequate test suites is more likely than statement adequate test suites in the effective support of statistical fault localization 2011 ieee continuous integration debugging testing implementation technology study of distributed automated software testing in order to study the distributed and automated testing of large scale software a distributed automated software testing platform dastp was presented which was based on the analysis of an existing software testing framework and the idea of distributed continuous software quality assurance the prototype system was implemented the platform integrated a series of tools required in the software testing and could use free resources in internet to complete the continuous integration and testing of large scale software a task scheduling algorithm based on part time constraint and an improved aco algorithm for set partitioning problem were proposed these two algorithms can partition testing task into subtasks and then schedule these subtasks automatically by running mysql testing in the prototype system the feasibility of the platform architecture and the effectiveness of the algorithms were verified aco algorithm automated software testing distributed continuous quality assurance distributed task scheduling software testing platform effect of neurotrophic factors on peripheral nerve repair aims this review aimed to analyze and discuss studies about the main growth factors tested in vitro and in vivo on peripheral nerves regeneration source of data articles were selected from the databases lilacs medline and scielo using the following key words nerve regeneration nerve growth factor brain derived neurotrophic factor neurotrophin 3 neurotrophin 4 5 ciliary neurotrophic factor glial cell line derived neurotrophic factor vascular endothelial growth factor insulin like growth factor peripheral nervous system injury sensory neurons motor neurons autologous graft and rats summary of findings several trophic factors also known as growth factors are used and tested in vitro and in vivo regeneration of peripheral nerves these proteins act directly on the proliferation and differentiation of different cell types being able to promote tissue repair and functional recovery usually the ideal model for the application of these substances is a continuous delivery system via biodegradable conduits conclusions it is possible to conclude that the combination of two or more growth factors probably exercise a synergistic effect on nerve regeneration especially when associated with absorble biomaterials with controlled release although the knowledge obtained about these proteins indicate an improvement in nerve regeneration further experimental studies are needed before transpose them into clinical application biomaterials growth factors nerve regeneration identifying opaque behavioural changes developers modify their systems by changing source code updating test suites and altering their system s execution context when they make these modifications they have an understanding of the behavioural changes they expect to happen when the system is executed when the system does not conform to their expectations developers try to ensure their modification did not introduce some unexpected or undesirable behavioural change we present an approach that integrates with existing continuous integration systems to help developers identify situations whereby their changes may have introduced unexpected behavioural consequences in this research demonstration we show how our approach can help developers identify and investigate unanticipated behavioural changes 2011 authors dynamic analysis impact analysis research demonstration static analysis unexpected behavioural change test data to reduce the complexity of unit test automation in order to automatically test large and complex software systems a well defined test process and a huge amount of test data are needed when it comes to test automation the quality of test data always plays a significant role by reducing the cost and the time required for the test activities in this work we follow the principle of quality of the test cases comes before quantity to show how the right selection of test data helps to address the goal of qualitative testing while optimizing the test effort needed 2011 authors continuous integration test automation test data unit testing topical prophylaxis for hiv prevention in women becoming a reality strategies to protect against sexual transmission of hiv include the development of products formulated for topical application which limit the toxicities associated with systemic oral pre exposure prophylaxis following several clinical trial failures attention is now focused on antiretroviral arv agents highly potent arv topical formulations provide a female controlled targeted and feasible option for hiv prevention a recently completed tenofovir gel trial was the first to demonstrate significant protection against hiv acquisition topical arvs have the advantage of delivering high concentration of drug at the site of transmission of hiv with low systemic absorption sustained release formulations such as intravaginal rings will likely improve adherence and can be designed to provide controlled and continuous delivery of arv combinations further studies to test alternative dosing strategies and pharmacokinetic pharmacodynamic relationships i n the genital tract will provide valuable information as the field strives to improve upon the promising tenofovir gel trial results 2011 springer science business media llc adherence antiretroviral drugs biomarkers dapivirine genital tract herpes simplex virus human immunodeficiency virus intravaginal ring maraviroc pharmacodynamics pharmacokinetics pre exposure prophylaxis tenofovir topical microbicides continuous scrum agile management of saas products hosted software as a service products provide an opportunity to provide consumers with continuous deployment of new features as opposed to scheduled version upgrades as is the norm for products installed on premise in order to exploit this opportunity a saas provider needs to adopt an agile process that is capable of releasing new features rapidly the scrum 5 6 process is ideally suited for this purpose however when scrum has been used for agile development of an installed product parallel overlapping sprints are executed by separate teams each dealing with short medium and longer term enhancements to the product 3 with the result that version upgrades are therefore easier to manage in contrast in the case of a saas product version upgrades are no longer a constraint so we can do better in this paper we describe continuous scrum a variant of type c scrum augmented with engineering best practices in a manner ideally suited for managing saas products in our approach bug fixes minor enhancements as well as major features are released continuously on a weekly basis by a single team in contrast to meta scrum 3 we also present field data from our experience with using continuous scrum for a hosted platform as a service product for more than two years our experience reinforces other recent evidence 11 that rapid smaller releases are often preferable to infrequent larger ones continuous scrum provides a mechanism to achieve and sustain a rapid release cycle for saas products as well as we believe for custom applications developed in house agile process configuration management continuous deployment paas release management saas scrum hitting the wall what to do when high performing scrum teams overwhelm operations and infrastructure all at once scrum implementations require total commitment to change high level management support and aggressive removal of impediments several company wide implementations are known to the authors but none of them had to deal with the complexity of a large mission critical enterprise software product pegasystems develops software to manage automate and optimize a broad array of business processes in july of 2009 the company deployed over 20 scrum teams in the u s and india within two months complexity of languages frameworks and tools led to an architecture where continuous integration support for software development teams was impossible without a major upgrade in infrastructure and operations a virtual scrum team was formed to avoid hitting the wall before this impediment impacted the first scrum release of the software here we describe how a scrum team engineered a continuous integration environment for hundreds of software developers on two continents within a few weeks 2011 ieee parameter estimation of coupled water and energy balance models based on stationary constraints of surface states we use a conditional averaging approach to estimate the parameters of a land surface water and energy balance model and then use the estimated parameters to partition net radiation into latent sensible and ground heat fluxes and precipitation into evapotranspiration and drainage plus runoff through conditional averaging of the modeled fluxes with respect to soil moisture and temperature we write an objective function that approximates the temperature and moisture dependent errors of the modeled fluxes in terms of atmospheric forcing e g precipitation and radiation surface states moisture s and temperature t s and model parameters the novelty of the approach is that the error term is estimated without comparison to measured fluxes instead it is inferred from the deviation of the conditionally averaged tendency terms expectation e ds dt s and expectation e dt s dt ts from zero since each of these terms equals zero in stationary systems but diverges from zero in the presence of misspecified parameters minimization of the approximated error yields parameters for model applications this strategy was previously studied for simple water balance models using soil moisture conditional averaging here we extend the idea to include energy balance fluxes and surface temperature conditioning the method is tested at two ameriflux sites vaira ranch california and kendall grassland arizona the estimated fluxes using only observed forcing and state variables are in reasonable agreement with field measurements because this method is based on conditional averages it can be applied to situations with subsampled or missing data that is continuous integration in time is not required copyright 2011 by the american geophysical union emerging patterns of continuous integration for cross platform software development copyright 2011 acm this paper proposes a collection of continuous integration patterns for use in developing cross platform software the patterns reflect our experience in building commercial and open source cross platform software which made extensive use of continuous integration systems we focus on the patterns that represent the basic types of projects to be put on a continuous integration system to produce cross platform software an example is given in the pattern language form to illustrate the use of these patterns by putting these patterns in a pattern language insights on the relationships among these patterns become apparent by applying the patterns in this pattern language continuous integration can be made to support the development of cross platform software better continuous integration cross platform pattern pattern language prioritization of features in agile product line engineering agile software development asd and software product line engineering sple methodologies have proved significant benefits in software development although they pursue common promises faster time to market better quality and lower cost many of their foundations are completely different asd focuses on requirements at hand and proposes continuous delivery of valuable software by short time framed iterations instead sple exploits the commonality across the products of a same family by investing on an upfront design of reusable assets domain engineering which are assembled into customer specific products application engineering 2011 springer verlag berlin heidelberg drug release from plga microspheres attached to solids using supercritical co2 functionalization of a porous orthopedic implant with dexamethasone a widely used anti inflammatory drug encapsulated within a biodegradable polymer for controlled release could help reduce or eliminate the inflammation response by the local tissue in this research we investigated the possibility of using supercritical carbon dioxide co 2 for attaching dexamethasone loaded plga polylactic co glycolic acid microspheres to porous cocrmo alloy for continuous delivery of dexamethasone supercritical co 2 has been shown to be effective for attachment of plga microspheres to glass plates and porous cocrmo alloy attached microspheres showed similar dexamethasone release profiles but different magnitude of burst release microspheres attached to the porous alloy samples using supercritical co 2 at 10 bar and 40°c for 30 min showed a release profile similar to that of the nonattached microspheres the microsphere morphology and the release profiles of microspheres attached to the glass plates and to the porous alloy samples suggest that dexamethasone burst release is enhanced by plga swelling at higher co 2 pressures and better dispersion of microspheres this study shows that microspheres can be incorporated into porous solids using supercritical co 2 allowing for a wide variety of drug biodegradable polymer formulations prepared using the proven emulsion solvent evaporation method to be tested 2011 the author s continuous release dexamethasone drug delivery system microspheres orthopedic implant plga supercritical co2 status communication in agile software teams a case study developers ought to maintain awareness of the status of a software project however there are very few recorded best practices for defining what constitutes relevant status information and the appropriate modalities for communicating this information in this industry case study we conducted in depth interviews with members of an agile development team we found that their daily work practices while well defined and regular were heavily influenced by the status information they integrated from a number of sources in particular continuous integration builds had a substantial effect on the team s workflow based on our findings we provide a set of guidelines for build monitoring systems which encourage collective and individual responsibility while working within the established team environment 2010 ieee awareness build processes communication software teams status information towards an adaptive integration trigger continuous integration in software development is a practice recommended by the most important development methodologies it promises many advantages such as early detection of bugs an important element of continuous integration although largely forgotten by the scientific literature is the trigger which initiates the process of building software from development sources this paper discusses the possibility of improving this software component and opens the way for research that could be applied to other computer related fields to this end we have implemented a prototype that shows for a case study the results obtained when using existing triggers 2010 springer verlag berlin heidelberg adaptive continuous integration optimize trigger provenance of software development processes why does the build fail currently this and similar questions arise on a daily basis in software development processes sdp there is no easy way to answer these questions the required information is stored throughout different tools the version control and continuous integration systems in this example the tools mainly live in isolated worlds and no direct connection between their data exists this paper proposes a solution to such problems based on provenance technologies after outlining the complexity of a sdp the questions arising on a daily basis are categorized finally an approach to make the sdp provenance aware is proposed based on prime the open provenance model and a soa architecture using neo4j to store the data gremlin to query it and rest webservices as connection to the tools 2010 springer verlag berlin heidelberg testing in parallel a need for practical regression testing when software evolves its functionalities are evaluated using regression testing in a regression testing process a test suite is augmented reduced prioritized and run on a software build version regression testing has been used in industry for decades while in some modern software activities we find that regression testing is yet not practical to apply for example according to our realistic experiences in sohu com inc running a reduced test suite even concurrently may cost two hours or longer nevertheless in an urgent task or a continuous integration environment the version builds and regression testing requests may come more often in such a case it is not strange that a new round of test suite run needs to start before all the previous ones have terminated as a solution running test suites on different build versions in parallel may increase the efficiency of regression testing and facilitate evaluating the fitness of software evolutions on the other hand hardware and software resources limit the number of paralleled tasks in this paper we raise the problem of testing in parallel give the general problem settings and use a pipeline presentation for data visualization solving this problem is expected to make practical regression testing continuous integration pipeline scheduling regression testing test case prioritization architecture centric development in globally distributed projects in this chapter architecture centric development is proposed as a means to strengthen the cohesion of distributed teams and to tackle challenges due to geographical and temporal distances and the clash of different cultures a shared software architecture serves as blueprint for all activities in the development process and ties them together architecture centric development thus provides a plan for task allocation facilitates the cooperation of globally distributed developers and enables continuous integration reaching across distributed teams advice is also provided for software architects who work with distributed teams in an agile manner 2010 springer verlag berlin heidelberg practical realization of software integration process during the development of complex hardware software systems progress in the field of information technology and radio engineering along with customer requirements lead to development of more and more complex multicompanent radio systems our research lab is specialized in creation of heterogeneous hardware software systems which are made up of components based on various including embedded operating systems error free operation of such systems depends on a lot of factors one of which is the software quality the main conclusion to be drawn from previous experience of hardware software systems production was to make changes in software development process in particular integration practice was set up as independent process with special build engineer to maintain it during the stage by stage evolution of integration process a number of key factors have been revealed cross platform development component interdependency lack of human and time resources to meet all requirements several integration schemes have been proposed and put into operation on the basis of previous projects in depth analysis the decision on introducing into practice continuous integration and automated build processes was made the set of measures intended to integration process development which was presented in this paper allows us to reduce time cost on software releases from several days to several hours furthermore suggested scheme of integration process is considered to be universal and therefore could be applied in any project regardless of operating systems which are used in development process at the same time it should be noted that for the purpose of most successful implantation of continuous integration it is essential that all process stages should be well documented and understood by all developers before project starts 2010 ieee continuous integration hardware software system software development process a scalable autotest platform for embedded system the complex and diversity of embedded system make the developing and testing embedded system a hard work providing a high availability high scale universal testing platform can improve the development of embedded software the autotest platform focuses on layered scalability and continuous integration loose coupling between the layers makes the system easy to extend at the local layer continuous improvements reduce the initial investment optimize the platform implementation and expand capacity and capabilities according to requirements users commit test requests front end and background centralized testing cluster executes this task and returns results to users 2010 ieee autotest black box testing embedded system tdd agilists and the art of integrated assessment tool development software engineering for integrated assessment needs to address the fact that systems and models developed are increasingly used in participatory settings applying old principles of inside out design and development using waterfall model based processes is no longer sufficient new software engineering insights based on interaction design and iterative agile processes for the development help in building systems from a more outside in perspective based on two case studies from large european projects on integrated assessment this switch from applying old to applying new principles will be described and its effects discussed elements were taken from interaction design personas story boards mock ups focus groups and focus tasks these were mixed with an iterative and incremental development process using agile elements such as daily stand ups user stories planning games test driven development and continuous integration the resulting process was used for the development of the two systems siat and seamless if for integrated assessment giving them a more user oriented focus than what would have resulted from following the old principles agile software development integrated assessment interaction design influence of low frequency powertrain vibrations on driveability assessments cost and time efficient vehicle development is increasingly depending on the usage of adequate software tools to enhance effectiveness the aim is a continuous integration of simulation tools and test environments within the vehicle development process in order to save time and costs this paper introduces a procedure to reveal the cause of low frequency powertrain vibrations and the influences on the dynamic behavior of a vehicle on a roller test bench the affected longitudinal acceleration signal is an arbitrative criterion for the driveability assessment with avl driveâ a well known driveability analysis and development tool for the objective assessment concerning nvh and driveability aspects of full vehicles 1 these experimental studies are embedded into an approach which describes the functional assembly of three applied test environments road roller test bench and simulation with according tools in order to facilitate an integrated driveability development process the low powertrain vibrations are identified by performance of positive load change maneuvers tip in in the vehicle on a roller test bench a parallel recording of acceleration signals by the driveability analysis sensor system and of defined surfaces by a 3d laser scanning vibrometer lsv allows an identification of possible vibration causes by analysis of the appearing frequency spectrum furthermore is elucidated how the influences of different tires at the vehicle affect its longitudinal dynamic behavior hence the rim surface is scanned with a 3d laser scanning vibrometer focusing its longitudinal acceleration stimulation in the range of 1 100hz the gained knowledge from these investigations helps to determine room for improvement of vehicle driveability and nvh behavior but as well for advancement of simulation models copyright 2010 sae international assessment of a submerged grid mooring in the gulf of maine the university of new hampshire unh developed and maintained an offshore aquaculture test site in the western gulf of maine south of the isles of shoals in approximately 50 m of water this site was designed to have a permanent moored grid to which prototype fish cages or surface buoys could be attached for testing new designs and the viability of the structure in the exposed gulf of maine in 1999 the first moorings deployed consisted of twin single bay grids each capable of each securing one fish cage these systems were maintained until 2003 to expand the biomass capacity of the site the single bay moorings were recovered and a new four bay submerged grid mooring was deployed within the same foot print of the previous twin systems this unique system operated as a working platform to test various structures including surface and submersible fish cages feeding buoys and other supporting equipment in addition the expanded capability allowed aquaculture fish studies to be conducted along with engineering and new cage feeder testing the 4 bays of the mooring system were located 15 meters below the surface these bays were supported by nine flotation elements the system was secured to the seafloor on the sides with twelve catenary mooring legs consisting of polysteel line 27 5 m of 52 mm chain and a 1 ton embedment anchor and in the center with a single vertical line to a 2 ton weight to size the mooring gear the unh software package aqua fe was employed this program can apply waves and currents to oceanic structures predicting system motions and mooring component tensions the submerged grid was designed to withstand 9 meter 8 8 second waves with a 1 m s collinear current when securing four fish cages during its seven year deployment the site regularly experienced extreme weather events most n otably a storm with a 9 m significant wave height 10 second dominate period in april 2007 the maximum currents at the site were observed during internal solitary wave events when 0 75 m s currents with 25 minute periods and 8 m duration were observed the mooring was recovered in 2010 after 7 years of continuous deployment without problems the dominate maintenance requirement of the mooring was the cleaning once a year of excessive mussel growth on the flotation elements and grid lines no problems of anchor dragging or failure of mooring components were documented during the deployment upon recovery critical mooring components were inspected and documented focusing on items with wear or other areas of interest the mooring proved to be a reliable stable working platform for a variety of prototype ocean projects highlighting the importance of a sound engineering approach taken in the design process 2010 ieee foundations for event based process analysis in heterogeneous software engineering environments for monitoring controlling and improving software development projects project and quality managers need tool support to analyze engineering processes within development environments unfortunately technical and semantic gaps between the engineering tools and related data models make it hard to observe and analyze the implemented tool based engineering processes in this paper we build on a service oriented platform for technically and semantically integrating heterogeneous engineering tools and propose an approach to monitor analyze and improve tool based engineering processes we empirically evaluate the approach using the continuous integration and test process and discuss strengths and limitations major result was that the approach enabled comparing expected and real life engineering processes with respect to process structure performance of individual process steps and risk of bottlenecks 2010 ieee software process software validation and verification architecture in the age of compositionality the nature of software engineering is changing whereas building systems was the predominant activity more recently the focus has shifted toward composing systems from open source commercial and proprietary components and to only build the functionality that truly is competitively differentiating in addition the way software is developed has changed as well especially focusing on short development cycles and frequent or even continuous deployment because of these requirements often teams are organized around features rather than components and can change all components in the system including their interfaces a third trend is the increasing adoption of software ecosystems where significant development of functionality relevant for customers occurs outside the platform organization obviously however the quality attributes that are necessary for system success remain important as well as the ability to easily incorporate new requirements in the system in a cost effective fashion because of the above the role of software architecture and in particular the software architects is more important in this new world but there is significant evolution in the implementation of the role the paper starts by characterizing the new approach to software engineering and the role of compositionality it then explores the implications for software architecture and the role of the software architect finally it defines a number of research challenges for the ecsa community to explore 2010 springer verlag berlin heidelberg compositionality software architecture on the integration of test adequacy test case prioritization and statistical fault localization testing and debugging account for at least 30 of the project effort scientific advancements in individual activities or their integration may bring significant impacts to the practice of software development fault localization is the foremost debugging sub activity any effective integration between testing and debugging should address how well testing and fault localization can be worked together productively how likely does a testing technique provide test suites for effective fault localization to what extent may such a test suite be prioritized so that the test cases having higher priority can be effectively used in a standalone manner to support fault localization in this paper we empirically study these two research questions in the context of test data adequacy test case prioritization and statistical fault localization our preliminary postmortem analysis results on 16 test case prioritization techniques and four statistical fault localizations show that branch adequate test suites on the siemens suite are unlikely to support effective fault localization on the other hand if such a test suite is effective around 60 of the test cases can be further prioritized to support effective fault localization which indicates that the potential savings in terms of effort can be significant 2010 ieee continuous integration debugging testing reactive variability management in agile software development agile organizations focus on developing software systems that satisfy their current customer base without worrying about best practices to handle variations of requirements in the system scaling agile methods up to adopt variability management practices in their traditional form is challenging in this paper we discuss the challenges and we contribute a lightweight iterative approach that enables agile organizations to manage variability on demand in a reactive manner the approach relies on agile practices like iterative development refactoring and continuous integration and testing we present a case study to show how the approach was used to handle variability arising from technical and usability issues and we provide a discussion of the advantages and limitations of the approach 2010 ieee the adaptation of test driven software processes to industrial automation engineering software components provide an increasing part of added value in automation systems and become more complex to construct and test test driven development tdd of software systems has been successfully used for agile development of business software systems test cases guide the system implementation and can be executed automatically after software changes continuous integration build strategy however tdd processes need to be adapted to control automation systems engineering where real world systems are challenging to model and to test automatically in this paper we introduce an adapted tdd process from the business software engineering domain to industrial automation engineering we identify a set of uml models that enable the systematic derivation of test cases based on an initial empirical study we evaluate the adapted tdd process based on an industrial use case to identify strength and limitation of this approach major results of the study were that uml models enabled effective test case derivation in the study context 2010 ieee an it perspective on integrated environmental modelling the siat case policy makers have a growing interest in integrated assessments of policies the integrated assessment modelling iam community is reacting to this interest by extending the application of model development from pure scientific analysis towards application in decision making or policy context by giving tools a higher capability for analysis targeted at non experts but intelligent users many parties are involved in the construction of such tools including modellers domain experts and tool users resulting in as many views on the proposed tool during tool development research continues which leads to advanced understanding of the system and may alter early specifications accumulation of changes to the initial design obscures the design usually vastly increasing the number of defects in the software the software engineering community uses concepts methods and practices to deal with ambiguous specifications changing requirements and incompletely conceived visions and to design and develop maintainable extensible quality software the aim of this paper is to introduce modellers to software engineering concepts and methods which have the potential to improve model and tool development using experiences from the development of the sustainability impact assessment tool these range from choosing a software development methodology for planning activities and coordinating people technical design principles impacting maintainability quality and reusability of the software to prototyping and user involvement it is argued that adaptive development methods seem to best fit research projects that typically have unclear upfront and changing requirements the break down of a system into elements that overlap as little as possible in features and behaviour helps to divide the work across teams and to achieve a modular and flexible system however this must be accompanied by proper automated testing methods and automated continuous integration of the elements prototypes screen sketches and mock ups are useful to align the different views build a shared vision of required functionality and to match expectations 2010 elsevier b v assessment tool integrated assessment modelling software architecture software development process towards real time integration today most developers work in parallel inside private workspaces to ensure stability during programming but this provokes isolation from what co workers are doing isolation may result in conflicts which must be detected as early as possible to avoid bugs and expensive rework currently frequent integration and awareness are used for early conflict detection frequent integration detects actual conflicts using automated builds although only when merging checked in changes awareness informs about ongoing changes in private workspaces however developers must find actual conflicts by themselves this paper proposes the novel concept of real time integration this automates the detection of actual conflicts emerging during programming involving checked in and ongoing changes and affecting two or more developers cooperative resolution of conflicts is explicitly supported by sharing fine grained changes among private workspaces software versions will also improve quality as integration builds are run before checking in 2010 acm continuous integration cooperative work early conflict detection real time integration software product measurement and analysis in a continuous integration environment this paper describes a framework for a software internal quality measurement program with automatic metrics extraction this framework was successfully implemented in an industrial software factory that was possible through the implementation of a proposed continuous integration ci environment to periodically analyze source codes and extract metrics these metrics were consolidated in a data warehouse by allowing on line analytical processing olap and key performance indicator kpi analysis with high performance and user friendly interface the measurement program followed gq i m paradigm for metrics selection to ensure that collected metrics are relevant from the software factory goals perspective finally the measurement and analysis process area of the capability maturity model integration cmmi was used for measurement and analysis planning and implementation 2010 ieee continuous integration goal driven measurement software measurement software metrics software product quality building security in using continuous integration building security into software is harder than it should be this article explores a way to align application security practices with other software development best practices in order to make building security in easier to manage and more cost effective in particular this article looks at combining continuous integration ci with security testing and secure static code analysis integrating application security into software development it s difficult to transition application security initiatives from identifying vulnerabilities after software has been produced to proactively mitigating vulnerabilities during the entire software development process learn about a simple approach for introducing application security into ongoing software development projects 2010 ieee application security continuous integration secureci software development tech challenges in a large scale agile project a five year 25 man java project effort that started with a waterfall like methodology and that adopted scrum after less than a year has been concluded we present three key technical challenges briefly analyze their consequences and discuss the solutions we adopted firstly we discuss how we modularized our architecture module delineation principles coupling and the trade offs of abstraction then we discuss testing environments their automation and relation to branches and the effect on customer involvement and feedback finally we discuss the benefits and disadvantages of representing domain knowledge declaratively for all three challenges we discuss how the project s agility was affected 2010 springer verlag berlin heidelberg agile applicaiton layer branch business logic continuous integration coupling database updates declarative knowledge dependency deploy domain expert domain knowledge footprint ide java maven modularization module bloat pojo process scrum technical challenge testing recent advances in transdermal drug delivery system transdermal drug delivery system tdds provides a means to sustain drug release as well as reduce the intensity of action and thus reduce the side effects associated with its oral therapy transdermal drugs are self contained discrete dosage form it delivers a drug through intact skin at a controlled rate into the systemic circulation delivery rate is controlled by the skin or membrane in the delivery system a sophisticated complex drug delivery system difficult to formulate it requires specialized manufacturing process equipment formulated to meet specific biopharmaceutical and functional characteristics the materials of construction configuration and combination of the drug with the proper cosolvent excipient penetration enhancer and membrane are carefully selected and matched to optimize adhesive properties and drug delivery requirements transdermal drug delivery an approach used to deliver drugs through the skin for therapeutic use as an alternative to oral intravascular subcutaneous and transmucosal routes various transdermal drug delivery technologies are described including the use of suitable formulations carriers and penetration enhancers the most commonly used transdermal system is the skin patch using various types of technologies transdermal technologies may be applied for several categories of pharmaceuticals used for the treatment of disorders of the skin or for systemic effect to treat diseases of other organs several transdermal products and applications include hormone replacement therapy management of pain angina pectoris smoking cessation and neurological disorders such as parkinson s disease formulated to deliver the drug at optimized rate into the systemic circulation should adhere to the skin for the expected duration should not cause any skin irritation and or sensitization enhancing bioavailability via bypassing first pass metabolism minimizing pharmaco kinetic peaks and troughs improving tolerability and dosing increasing patient compliance in continuous delivery transdermal drug delivery system quality of code can be planned and automatically controlled quality of code is an important and critical health indicator of any software development project however due to the complexity and ambiguousness of calculating this indicator it is rarely used in commercial contracts as programmers are much more motivated with respect to the delivery of functionality than quality of code beneath it they often produce low quality code which leads to post delivery and maintenance problems the proposed mechanism eliminates this lack of attention to quality of code the results achieved after the implementation of the mechanism are more motivated programmers higher project sponsor confidence and a predicted quality of code 2009 ieee code coverage code quality continuous integration cyclomatic complexity quality metrics software integration process of complex hardware software systems under conditions of limited resources a lot of problems occur during the development of complex hardware software systems all these problems usually caused by the system and development process specificity in this work we present a step by step evolution of software development process that took place in research lab for mathematical simulation of physical systems of physico technical research institute of university of nizhny novgorod ptri unn based on continuous integration and automating software builds we cover a set of primary issues which have arisen due to the new functionality adaptation of integration process and discuss the ways to solve its 2009 ieee continuous integration hardware software system software quality using continuous integration and automated test techniques for a robust c4isr system we have used ci continuous integration and various software testing techniques to achieve a robust c4isr command control communications computers intelligence surveillance and reconnaissance multi platform system because of rapid changes in the c4isr domain and in the software technology frequent critical design adjustments and in turn vast code modifications or additions become inevitable defect fixes might also incur code changes these unavoidable code modifications may put a big risk in the reliability of a mission critical system also in order to stay competitive in the c4isr market a company must make recurring releases without sacrificing quality we have designed and implemented an xml driven automated test framework that enabled us developing numerous high quality tests rapidly while using ci with automated software test techniques we have aimed at speeding up the delivery of high quality and robust software by decreasing integration procedure which is one of the main bottleneck points in the industry this work describes how we have used ci and software test techniques in a large scaled multi platform multi language distributed c4isr project and what the benefits of such a system are 2009 ieee c4isr systems continuous integration mission critical systems multi platform testing robustness software test techniques xml driven automated test framework an open source based approach to software development infrastructures as software systems become larger and more complex automated software engineering tools play a crucial role for effective software development management which is a key factor to lead quality software systems in this work we present trica an open source based software development infrastructure the name of trica represents its features such as traceability relationship informativeness cost effectiveness and automation essentially in trica a continuous integration tool is coupled with a software configuration management tool and an issue tracking tool we provisioned a mechanism to connect the open source tools in trica so that project members use the collaborated information to solve various issues and implementation problems efficiently and easily share forthcoming issues during the course of the project we show that trica can help to decentralize risks throughout the software development cycle and achieve successful software development 2009 ieee component continuous integration issue tracking open source scm software engineering tools software process for rapid development of hpc software using cmake we are developing and extending the cmake family of software development tools www cmake org for use in the mobile network institute and the multi scale reactive modeling institute these tools are used to build test and package c c and fortran software in a cross platform manner by using cmake a software project can be built just as easily on a windows pc as on a cray xt5 super computer in addition cmake s ctest facility can test and then populate the testing dashboard cdash www cdash org which is a web based tool used to monitor and display the health of a software system in combination with ctest cdash provides a continuous integration testing system finally cpack can be used to package and deploy software across multiple computing platforms this paper will describe these tools how they are used in the software process and provide specific application of their usage in support of multi scale reactive modeling msrm and the mobile network modeling mnm high performance computing software applications institute the tools described in this paper are open source and available to any high performance computing hpc project the paper will provide a high level overview of the cmake tools with enough specifics to enable any hpc development effort to begin working with them we will also describe how these tools and the associated software process provide the computational infrastructure required to rapidly develop next generation hpc software building a system for advancing clinico genomic trials on cancer the analysis of clinico genomic data poses complex computational problems in the project acgt a grid based software system to sup port clinicians and bio statisticians in their daily work is being developed starting with a detailed user requirements analysis and with the continuous integration of usability analysis in the development process the project strives to develop an architecture that will substantially improve the way clinico genomic trials are conducted today in this paper results of the initial requirements analysis and approaches to address these requirements are presented we also discuss the importance of appropriate metadata to tailor the system to the needs of the users test automation framework for implementing continuous integration manual testing is a laborious and time consuming process in addition it may not be effective in finding certain defects therefore we introduce an effective framework for automated testing to help solve such problems the proposed framework helps automate the distribution execution and results analysis of test cases the workflow of tests and test environments are graphically expressed as tables many software development and testing practices can be automated and greatly simplified by using this framework it can also be used to create a continuous integration ci system by incorporating the automated build tools or ci servers this paper provides best practices on automated ci solutions using the proposed framework to provide developers and or testers with a better idea of progress and code quality throughout the project lifecycle so that they can direct their time and expertise to more important challenging issues 2009 ieee enhancing testing technologies for globalization of software engineering and productivity while successful at increasing code churn rates global software development and evolution suffers from several quality assurance challenges first sub groups within developer communities often work on loosely coupled parts of the application code each developer sub group typically modifies a local copy of the code and frequently checks in changes and downloads other developers changes consequently after making a change a developer may not immediately realize that the local change has inadvertently broken other parts of the overall software code this situation is compounded as there is little direct inter developer communication almost all communication is done via web based tools such as code commit log messages bug reports change requests and comments this chapter outlines the challenges that global software development adds to the already complex quality assurance process two case studies of real software projects implemented in a disturbed manner demonstrate the importance of continuous integration testing and the positive consequences of increasing the diversity of quality assurance techniques tools finally it concludes with an outline of how software integration testing needs to be enhanced to meet the new challenges of globalization 2010 igi global dielectric heating of solid catalyst particles a well working hydrogen fuel cell needs continuous delivery of pure hydrogen instead of storing hydrogen on board an alternative is to produce it in situ via bio fuel steam reforming dielectric heating of potential catalysts for microwave enhanced steam reforming has been investigated the reproducibility and accuracy of the two most widely used temperature measurement techniques fo and ir have been tested a high performance charged particle cmos image sensor with per column a d conversion a cmos image sensor for charged particle imaging such as for transmission electron microscopy with per column 10 bit adc s has been designed fabricated and tested the 0 25 μm design s parallel data conversion allows up to 390 frames s operation the sensor s sensitivity and signal to noise ratio ≥17 5 rms are sufficiently high that the imaging of individual energetic incident electrons e g 200 kev is practical electron microscope tests have shown that images created by accumulating processing and superimposing low flux images of individual incident electrons electron counting mode yields superior contrast and resolution to equivalent continuous exposures to demonstrate this we measured the modulation transfer function of using electron counting mode which for example was found to be more than two times higher at 50 line pairs per mm versus the equivalent standard continuous integration 2009 ieee active pixel sensor charged particle imaging per column analog to digital conversion enabling agile testing through continuous integration a continuous integration system is often considered one of the key elements involved in supporting an agile software development and testing environment as a traditional software tester transitioning to an agile development environment it became clear to me that i would need to put this essential infrastructure in place and promote improved development practices in order to make the transition to agile testing possible this experience report discusses a continuous integration implementation i led last year the initial motivations for implementing continuous integration are discussed and a pre and post assessment using martin fowler s practices of continuous integration is provided along with the technical specifics of the implementation the report concludes with a retrospective of my experiences implementing and promoting continuous integration within the context of agile testing killing the gatekeeper introducing a continuous integration system this is the story of how the launchpad https launchpad net development team switched to a continuous integration system to increase several flows in their development process • flow of changes on trunk •flow of changes requiring database schema upgrade • flow of deployed changes to end users the switch to a buildbot based system meant violating a very old company taboo a trunk that doesn t pass its test suite the risk of a broken trunk was offset by allowing each developer to run the full test suite in the amazon ec2 cloud creating habitable code lessons in longevity from cruisecontrol a major challenge for software organizations is creating software that can continue to adapt and change over time a code base the team can live with this paper reviews the lessons learned from cruise control a popular tool for continuous integration cruise control is an open source success story not only because it has had over 400 000 downloads but also because it includes contributions from over 200 different people for practitioners who are tired of brittle code that must be discarded and rewritten cruise control provides valuable lessons implementing an effective test automation framework testing automation tools enable developers and or testers to easily automate the entire process of testing in software development evertheless adopting automated testing is not easy and unsuccessful due to a lack of key information and skills in order to help solve such problems we have designed a new framework to support a clear overview of the test design and or plan for automating tests in distributed environments those that are new to testing do not need to delve into complex automation tools or test scripts this framework allows a programmer or tester to graphically specify the granularity of execution control and aids communication between various stakeholders and software developers it also enables them to perform the automated continuous integration environment in this paper we describe details on experiences and usage of the proposed framework 2009 ieee automated testing continuous integration fit fitnesse staf stax test automation framework how well do test case prioritization techniquessupport statistical fault localization in continuous integration a tight integration of test case prioritization techniques and fault localization techniques may both expose failures faster and locate faults more effectively statistical fault localization techniques use the execution information collected during testing to locate faults executing a small fraction of a prioritized test suite reduces the cost of testing and yet the subsequent fault localization may suffer this paper presents the first empirical study to examine the impact of test case prioritization on the effectiveness of fault localization among many interesting empirical results we find that coverage based techniques and random ordering can be more effective than distribution based techniques in supporting statistical fault localization furthermore the integration of random ordering for test case prioritization and statistical fault localization can be effective in locating faults quickly and economically 2009 ieee continuous integration fault localization software process integration test case prioritization history of a large test automation project using selenium in 2007 i started work as a tester for a company called socialtext when i joined the company there was already a selenium based test framework in place but there were only a couple of automated test cases created we had about 400 test steps or individual assertions about the behavior of the application when i left socialtext two years later we had just surpassed 10 000 test steps in the main set of regression tests we also had browserspecific test sets in place an automated test case for visually checking the application and a continuous integration like script that ran all day and all night against the latest version of the code at about 4000 test steps regression bugs released to production dropped essentially to zero the other 6000 test steps covered ongoing new features in the project and more robust testing of the older application functions this report discusses how i helped grow this system and the things we learned along the way that helped it be such a successful ongoing project the report covers initial conditions and test design discusses issues in application feature coverage how and when to grow the system quickly a couple of test design smells that caused us problems along the way how we treat continuous integration in a system like this and how we coped when significant parts of the user interface were completely re engineered from cmmi and isolation to scrum agile lean and collaboration this paper describes a journey from 2004 to 2008 when software people in denmark together with a partner from bangladesh established a subsidiary company more than 7000 km away from denmark we hired 20 people in one week in bangladesh and started to use cmmi processes to integrate development teams between the two locations with the goal of receiving a cmmi level 3 certification in 1 5 years after some challenging time we stopped the cmmi project and switched back to agile and lean techniques with more collaboration here we describe our experience with implementing global big bang scrum and building a kaizen culture a journey from long running projects technical dept and integration nightmares to small batches of work continuous integration and faster delivery of business value this is reported by hans baggesen team lead for one of the danish r d teams 2007 2008 and mads troels hansen cto and co founder 2004 2007 a unified test framework for continuous integration testing of soa solutions the quality of service oriented architecture soa solutions is becoming more and more important along with the increasing adoption of soa continuous integration testing cit is an effective technology to discover bugs as early as possible however the diversity of programming models used in an soa solution and the distribution nature of an soa solution pose new challenges for cit existing testing frameworks more focus on the integration testing of applications developed by a single programming model in this paper a unified test framework is proposed to overcome these limitations and enable the cit of soa solutions across the whole development lifecycle this framework is designed following the model driven architecture mda the information of an executable test case is separated into two layers the behavior layer and the configuration layer the behavior layer represents the test logic of a test case and is platform independent the configuration layer contains the platform specific information and is configurable for different programming models an extensible and pluggable test execution engine is specially designed to execute the integration test cases a global test case identifier instrumentation approach is used to merge the distributed test case execution traces captured by itcam an ibm integrated management tool a verification approach supporting boolean expression and back end service interaction verification is proposed to verify the test execution result initial experiments have shown the effectiveness of this unified test framework 2009 ieee continuous integration testing service oriented architecture exploiting timed automata for conformance testing of power measurements for software development testing is still the primary choice for investigating the correctness of a system automated testing is of utmost importance to support continuous integration and regression tests on actual hardware for embedded systems power consumption is a chief performance metric which is tightly coupled to the hardware used and the software exploiting low power modes automated testing of power consumption requires to investigate its conformance to a specification we employ timed automata for specifying the expected behavior of a real sensor node application as well as for describing the power measurements obtained from its real world implementation introducing computational optimizations the presented approach allows to utilize standard model checkers for automated conformance testing of modeled systems and monitored power consumption of their implementations 2009 springer berlin heidelberg mapping cmmi level 2 to scrum practices an experience report cmmi has been adopted advantageously in large companies for improvements in software quality budget fulfilling and customer satisfaction however spi strategies based on cmmi dev require heavy software development processes and large investments in terms of cost and time that medium small companies do not deal with the so called light software development processes such as agile software development asd deal with these challenges asd welcomes changing requirements and stresses the importance of adaptive planning simplicity and continuous delivery of valuable software by short time framed iterations asd is becoming convenient in a more and more global and changing software market it would be greatly useful to be able to introduce agile methods such as scrum in compliance with cmmi process model this paper intends to increase the understanding of the relationship between asd and cmmi dev reporting empirical results that confirm theoretical comparisons between asd practices and cmmi level2 2009 springer berlin heidelberg agile software development cmmi scrum development of a flow through system for the fish embryo toxicity test fet with the zebrafish danio rerio the acute fish test is still a mandatory component in chemical hazard and risk assessment however one of the objectives of the new european chemicals policy reach registration evaluation authorization and restriction of chemicals is to promote non animal testing for whole effluent testing in germany the fish embryo toxicity test fet with the zebrafish danio rerio has been an accepted and mandatory replacement of the fish test since january 2005 for chemical testing however further optimization of the fet is required to improve the correlation between the acute fish test and the alternative fet since adsorption of the test chemical to surfaces may reduce available exposure concentrations a flow through system for the fet using modified commercially available polystyrene 24 well microtiter plates was developed thus combining the advantages of the standard fet with those of continuous delivery of test substances the advantages of the design presented include small test footprint availability of adequate volumes of test solution for subsequent chemical analysis and sufficient flow to compensate for effects of non specific adsorption within 24 h the flow through test system can also be utilized to conduct longer term embryo larval fish tests thus offering the possibility for teratogenicity testing 2009 elsevier ltd all rights reserved acute fish toxicity alternatives fish embryo test flow through zebrafish the electricidal effect is active in an experimental model of staphylococcus epidermidis chronic foreign body osteomyelitis treatment with low amperage 200 μa electrical current was compared to intravenous doxycycline treatment or no treatment in a rabbit model of staphylococcus epidermidis chronic foreign body osteomyelitis to determine if the electricidal effect is active in vivo a stainless steel implant and 10 4 cfu of planktonic s epidermidis were placed into the medullary cavity of the tibia four weeks later rabbits were assigned to one of three groups with treatment administered for 21 days the groups included those receiving no treatment n 10 intravenous doxycycline n 14 8 mg kg of body weight three times per day and electrical current n 15 200 μa continuous delivery following treatment rabbits were sacrificed and the tibias quantitatively cultured bacterial load was signif icantly reduced in the doxycycline median 2 55 range 0 50 to 6 13 log 10 cfu g of bone and electrical current median 1 09 range 0 50 to 2 99 log 10 cfu g of bone groups compared to the level for the control group median 4 16 range 3 70 to 5 66 log 10 cfu g of bone p 0 0001 moreover treatment with electrical current was statistically significantly more efficacious p 0 035 than doxycycline treatment the electricidal effect the bactericidal activity of low amperage electrical current against bacterial biofilms is active in vivo in the treatment of experimental s epidermidis chronic foreign body osteomyelitis copyright 2009 american society for microbiology all rights reserved gain of new exons and promoters by lineage specific transposable elements integration and conservation event on chrm3 gene the chrm3 gene is a member of the muscarinic acetylcholine receptor family that plays important roles in the regulation of fundamental physiological functions the evolutionary mechanism of exon acquisition and alternative splicing of the chrm3 gene in relation to transposable elements tes were analyzed using experimental approaches and in silico analysis five different transcript variants t1 t2 t3 t3 1 and t4 derived from three distinct promoter regions t1 l1hs t2 t4 original t3 t3 1 the1c were identified a placenta t1 and testis t3 and t3 1 dominated expression pattern appeared to be controlled by different tes l1hs and the1c that were integrated into the common ancestor genome during primate evolution remarkably the t1 transcript was formed by the integration event of the human specific l1hs element among the 12 different brain regions the brain stem olfactory region and cerebellum showed decreased expression patterns evolutionary analysis of splicing sites and alternative splicing suggested that the exon acquisition event was determined by a s election and conservation mechanism furthermore continuous integration events of transposable elements could produce lineage specific alternative transcripts by providing novel promoters and splicing sites taken together exon acquisition and alternative splicing events of chrm3 genes were shown to have occurred through the continuous integration of transposable elements following conservation the korean society for molecular and cellular biology and springer netherlands 2009 alternative splicing chrm3 gene exon acquisition l1hs element transposable elements new therapeutic directions presentations from the movement disorder society s 13th international congress of parkinson s disease and movement disorders the 13th international congress of parkinson s disease and movement disorders held in paris on june 7 11 2009 was attended by 5 000 people from around the world over 1 700 posters were presented in addition to the 6 plenary sessions 8 teaching courses and 19 parallel sessions as well as the 20 video sessions and skill workshops many of these presentations highlighted the strategies being investigated to overcome the drawbacks to and limitations of the treatment of parkinson s disease pd with levodopa as well as the controversies surrounding that drug s proper implementation some of the questions addressed were when should treatment be initiated how should early pd be treated how should motor complications be managed how can we deal with the noncontinuous dopamine stimulation achieved with levodopa what is the role of surgical interventions how can we inhibit disease progression what new therapies are being tested the findings of recent clinical trials on new pharmacological therapies were presented and while these were sometimes surprising and sometimes disappointing all provided important information on these strategies corporate symposia held at the congress also covered a variety of topics including 24 h symptom control or the lack of it the role of dopamine agonists and continuous delivery systems also highlighted here are a selection of poster presentations describing recent studies of drug candidates for pd the movement disorder society mds congress also focuses attention on diseases other than pd and a session providing an overview of the nature and management of cilles de la tourette s syndrome is detailed here copyright 2009 prous science s a u or its licensors all rights reserved changes in the treatment responses to artesunate mefloquine on the northwestern border of thailand during 13 years of continuous deployment background artemisinin combination treatments act are recommended as first line treatment for falciparum malaria throughout the malaria affected world we reviewed the efficacy of a 3 day regimen of mefloquine and artesunate regimen mas 3 over a 13 year period of continuous deployment as first line treatment in camps for displaced persons and in clinics for migrant population along the thai myanmar border methods and findings 3 264 patients were enrolled in prospective treatment trials between 1995 and 2007 and treated with mas 3 the proportion of patients with parasitaemia persisting on day 2 increased significantly from 4 5 before 2001 to 21 9 since 2002 p 0 001 delayed parasite clearance was associated with increased risk of developing gametocytaemia aor 2 29 95 ci 2 00 2 69 p 0 002 gametocytaemia on admission and carriage also increased over the years p 0 001 test for trend for both mas 3 efficacy has declined slightly but significantly hazards ratio 1 13 95 ci 1 07 1 19 p 0 001 although efficacy in 2007 remained well within acceptable limits 96 5 95 ci 91 0 98 7 the in vitro susceptibility of p falciparum to artesunate increased significantly until 2002 but thereafter declined to levels close to those of 13 years ago geometric mean in 2007 4 2 nm l 95 ci 3 2 5 5 the proportion of infections caused by parasites with increased pfmdr1 copy number rose from 30 12 40 in 1996 to 53 24 45 in 2006 p 0 012 test for trend conclusion artesunate mefloquine remains a highly efficacious antimalarial treatment in this area despite 13 years of widespread intense deployment but there is evidence of a modest increase in resistance of particular concern is the slowing of parasitological response to artesunate and the associated increase in gametocyte carriage 2009 carrara et al procedures and algorithms for continuous integration in an agile specification environment one of the main ideas of agile development is to perform continuous integration in order to detect and resolve conflicts among several modular units of a system as soon as possible whereas this feature is well catered for at the level of programming source code the support available in formal specification environments is still rather unsatisfactory it is possible to analyze the composition of several modular units automatically but no assistance is given to help modify them in case of problems instead the stakeholders who build the specifications are forced to attempt manual changes until reaching the desired functionality in a process that is far from being intuitive in response to that this paper presents procedures and algorithms that automate the whole process of doing integration analyses and generating revisions to solve the diagnosed problems these mechanisms serve to complete an agile specification environment presented in a previous paper which was designed around the principle of facilitating the creative efforts of the stakeholders 2009 world scientific publishing company agile software development continuous integration formal specification software development in the cloud software development in the cloud brings adaptability and flexibility by utilizing some common software development tools that introduces the concept of cloud management and development services cloud computing for development bring teams to the level of control and visibility necessary in the new landscape the users can utilize the pooled physical and virtual machine capacity of a cloud for more flexible automation and reuse across projects the cloud combines the agile best practice of continuous integration ci and some common collaborative development tooling that has been used across companies of varying sizes locations and industries basic principles for development of cloud computing include that all stakeholders should be able to see and make progress on the code a version controlled system and focus on small releases that provide the most business value teams can build their own cloud development environment with continuous integration automation and roundtrip feed back to provide on demand access and visibility across the development lifecycle continuous integration how do you know that your application still works i will demonstrate how to develop a web application and have some degree of confidence that it still works after a developer has checked in new code or made changes to the existing code base we will use java as development language mercurial as version control system maven as build system hudson as continuous integration server jboss as application server junit as primary test framework and selenium to drive all gui tests 2009 springer berlin heidelberg software product line engineering approach for enhancing agile methodologies one of the main principles of agile methodologies consists in the early and continuous delivery of valuable software by short time framed iterations after each iteration a working product is delivered according to the requirements defined at the beginning of the iteration testing tools facilitate the task of checking if the system provides the expected behavior according to the specified requirements however since testing tools need to be adapted in order to test new working products in each iteration a significant effort has to be invested this work presents a software product line engineering sple approach that allows flexibility in the adaption of testing tools with the working products in an iterative way a case study is also presented using plum product line unified modeller as the tool suite for spl implementation and management 2009 springer berlin heidelberg the power of continuous integration builds and agile development john watkins 2009 and cambridge university press 2009 synopsis this case study will examine how organizations can benefit from applying continuous integration build cib solutions it will also look at how adopting agile best practices can increase development productivity while introducing performance management techniques as well as examining the role and use of testing throughout cib introduction my name is james wilson and i am the ceo of trinem based in edinburgh united kingdom i cofounded trinem with philip gibbs in 2001 after working for a number of years as an it consultant primarily delivering software configuration management scm services and solutions since 2001 trinem has implemented scm projects that have inherited various forms of technologies methods and scale trinem has implemented processes and had their technology implemented for a wide spectrum of organizations including nasa united states bank of new york united kingdom hbos united kingdom belastingdienst holland syscom taiwan sdc denmark and ge united kingdom and holland this case study illustrates the challenges approaches and benefits that i have encountered while developing agile application development processes this case study will also explore some of the key technologies that were used both commercial and open source to augment the implementations delivered by trinem and myself agile extreme programming scrum and or iterative invariably when customers ask for consulting services to assist with the implementation of scm and application development best practices they already have an idea of what is happening in the industry developing a test automation framework for agile development and testing as software developers today we all face problems of repetitive and error prone processes a lack of a clear way of communication between stakeholders and risks of late defect discovery or release delays in order to help solve such problems we implemented an effective framework for automated testing which combines the automation features of staf stax and the ease of use based on tabular input and output of fitnesse this framework can support continuous integration as an automated testing framework to improve software development processes the greatest advantage of the framework is the agility that allows for rapid delivery of high quality software in this paper we describe the practices and benefits of using the proposed framework 2009 springer berlin heidelberg agile testing automated testing continuousintegration fit fitnesse staf stax test automation framework perceptive agile measurement new instruments for quantitative studies in the pursuit of the social psychological effect of agile practices rising interest on social psychological effects of agile practices necessitate the development of appropriate measurement instruments for future quantitative studies this study has constructed such instruments for eight agile practices namely iteration planning iterative development continuous integration and testing stand up meetings customer access customer acceptance tests retrospectives and co location the methodological approach followed the scale construction process elaborated in psychological research we applied both qualitative methods for item generation and quantitative methods for the analysis of reliability and factor structure principal factor analysis to evaluate critical psychometric dimensions results in both qualitative and quantitative analyses indicated high psychometric quality of all newly constructed scales the resulting measurement instruments are available in questionnaire form and ready to be used in future scientific research for quantitative analyses of social psychological effects of agile practices 2009 springer berlin heidelberg agile practices co location continuous integration customer acceptance tests customer access iteration planning iterative approach measurement instruments retrospectives stand up meetings test driven development an examination of the effects of offshore and outsourced development on the delegation of responsibilities to software components offshore and outsourced development are the latest facts of life of professional software building the easily identifiable advantages of these trends such as cost benefits continuous delivery and support have already been explored to considerable extent but how does offshore and outsourced development affect the delegation of responsibilities to components of a software system in this paper we investigate this question by applying the resp dist technique on a set of real life case studies our resp dist technique uses metrics and a linear programming based method to recommend the reorganization of components towards an expedient distribution of responsibilities the case studies embody varying degrees of offshore and outsourced development results from the case studies lead to some interesting observations on whether and how offshore and outsourced development influences software design characteristics 2009 springer verlag berlin heidelberg using metric visualization and sharing tool to drive agile related practices this paper presents a metric visualization and sharing tool that supports management and control of agile related practices such as test driven development continuous integration user stories and pair programming the tool is part of a larger framework but can be used as a stand alone system it integrates data coming from different sources automatic non invasive data collection plug ins bug and task tracking repositories code parsers manual user input etc the tool also provides customizable indicators that enable non experts in the domain to get the general status of the observed process or product at a glance the dashboard based implementation of the tool is tailored to support multiple user roles including developers managers and even clients 2009 springer berlin heidelberg agile dashboard indicators metrics sharing visualization duality and a farkas lemma for integer programs springer science business media llc 2009 we consider the integer program max c′x ax b x ∈ n n a formal parallel between linear programming and continuous integration and discrete summation shows that a natural duality for integer programs can be derived from the z transform and brion and vergne s counting formula along the same lines we also provide a discrete farkas lemma and show that the existence of a nonnegative integral solution x ∈ n n to ax b can be tested via a linear program counting problems duality integer programming influence of microwave irradiation on heterogeneous gas solid reaction systems dielectric heating of solid catalyst particles a well working hydrogen fuel cell needs continuous delivery of pure hydrogen instead of storing hydrogen on board an alternative is to produce it in situ via bio fuel steam reforming dielectric heating of potential catalysts for microwave enhanced steam reforming has been investigated the reproducibility and accuracy of the two most widely used temperature measurement techniques fo and ir have been tested application of a model transformation paradigm in agriculture a simple environmental system case study springer science business media llc 2009 in this chapter the authors use the methodology presented in chapter 2 to develop a system that manages the spreading of organic waste on agricultural parcels the proposed method uses a process of iterative and incremental development two complete iterations of the development process are presented starting from the analysis model and ending with the code produced by the case tools structured query language sql code generator the first iteration deals with the description of territory objects and the second one deals with the business objects used in the context of the spreading of organic waste as a result of transformations applied models are enriched with new concepts and therefore are more complex the growing complexity of the model may negatively affect an actor s understanding which may become an impediment by slowing down the analysis phase the authors show how the software development process model a modeling artifact associated with the continuous integration unified process method avoids the apparent complexity of the model and improves productivity in vivo murine model of continuous intramedullary infusion of particles a preliminary study continued production of wear debris affects both initial osseointegration and subsequent bone remodeling of total joint replacements tjrs however continuous delivery of clinically relevant particles using a viable cost effective quantitative animal model to simulate the scenario in humans has been a challenge for orthopedic researchers in this study we successfully infused blue dyed polystyrene particles similar in size to wear debris in humans to the intramedullary space of the mouse femur for 4 weeks using an osmotic pump approximately 40 of the original particle load 85 μl was delivered into the intramedullary space an estimate of 3 × 10 9 particles the visible blue dye carried by the particles confirmed the delivery this model demonstrated that continuous infusion of particles to the murine bone implant interface is possible in vivo biological processes associated using wear debris particles can be studied using this new animal model 2008 wiley periodicals inc continuous infusion murine model particles agile systems development and stakeholder satisfaction a south african empirical study the high rate of systems development sd failure is often attributed to the complexity of traditional sd methodologies e g waterfall and their inability to cope with changes brought about by today s dynamic and evolving business environment agile methodologies am have emerged to challenge traditional sd and overcome their limitations yet empirical research into am is sparse this paper develops and tests a research model that hypothesizes the effects of five characteristics of agile systems development iterative development continuous integration test driven design feedback and collective ownership on two dependent stakeholder satisfaction measures namely stakeholder satisfaction with the development process and with the development outcome an empirical study of 59 south african development projects using self reported data provided support for all hypothesized relationships and generally supports the efficacy of am iteration and integration together with collective ownership have the strongest effects on the dependent satisfaction measures copyright 2008 acm agile methods software development stakeholder satisfaction an efficient local bandwidth management system for supporting video streaming in order to guarantee continuous delivery of video streaming over best effort be forwarding network some quality of service qos strategies such as rsvp and diffserv must be used to improve the transmission performance however these methods are too difficult to be employed in practical applications since their technical complexity in this paper we design and implement an efficient local bandwidth management system to tackle this problem in ipv6 environment the system monitors local access network and provides assured forwarding af service through controlling the video streaming requests based on available network bandwidth to assess the benefit of this system we perform tests to compare its performance with that of conventional be service our test results indicate convincingly that af offers substantially better performance than be 2008 ieee is integration and business performance the mediation effect of organizational absorptive capacity in smes a fundamental result of the information technology it and business performance literature is that it is not a driver of performance per se however it can be associated with higher performance if accompanied by organizational change the identification of the variables describing organizational change is still on going work this paper focuses on organizational absorptive capacity and analyses its effects on the relationship between it and business performance in small and medium enterprises smes organizational absorptive capacity measures the ability of an organization to complete a learning process a significant learning effort is typically associated with it as it represents a complex technology to cope with it s complexity implementation is typically incremental and is accompanied by a continuous integration effort of data and applications the degree of integration of a company s information system is called is integration is a proxy of it maturity and quality in this study we explore the effect of is integration on business performance through absorptive capacity that is we hypothesize that absorptive capacity has a mediation role between is integration and business performance the proposed research model is tested with data surveyed from 466 smes sited in italy for which exports constitute more than half of their revenues results indicate that organizational absorptive capacity has a mediation effect alternative models attributing to absorptive capacity a role different from mediation are found to be non significant 2008 jit palgrave macmillan all rights reserved business performance cfa is integration organizational absorptive capacity sem smes preparation of ophthalmic insert of acyclovir using ethylcellulose rate controlling membrane the aim of the present investigation was to prepare controlled release ocular inserts of a polar drug acyclovir for continuous delivery to the eyes for 5 days reservoir type ocular inserts comprising a reservoir film of sodium alginate and rate controlling membrane of ethylcellulose in different concentrations were prepared by film casting technique on teflon coated petri dishes and tested for drug content physical characteristics interaction between drug and polymers due to sterilization by gamma radiations and in vitro drug release all formulations contained 2mg acyclovir reservoir film containing 2 5 sodium alginate and 48 peg 400 by weight of polymer as plasticizer was considered best for the formulation of the ocular insert because of its maximum folding endurance 20 ± 2 and percentage elongation at break 18 ± 0 57 on the basis of in vitro drug release studies the formulation containing 2 ethylcellulose was found to be better than other formulations with 91 drug release in 120 hr it was therefore selected and subjected to in vivo and stability studies a high in vitro in vivo release correlation 0 989 was observed for the formulation the concentration of acyclovir in aqueous humour reached above the reported css of 1 7μg ml after 8hr and remained almost constant up to 5 days however acyclovir concentration could not be detected after 4hr on administration of 3 ophthalmic ointment ocular inserts were found to be stable with no interaction due to sterilization by gamma radiation thus ocular inserts conclusively demonstrated controlled release of acyclovir with a constant concentration in the aqueous humour for 5 days acyclovir aqueous humour ethylcellulose ocular insert ophthalmic insert sodium alginate automated continuous integration of component based software an industrial experience when a software product is composed of dozens of or even hundreds of components with complicated dependency relationship among each other one component s change can affect lots of other components behavior sometimes therefore the stabilization job with multiple updated components drives the people who are responsible for integration and release the software into an integration hell to avoid this kind of integration hell we established integration procedure which encourages the developers frequent and small releases we also created an automated integration system which continuously runs integration process in an incremental way so as to create and maintain an up to minute reasonably stable version of the product release candidate in this paper we introduce our integration procedure and automated integration system for a software product with hundreds of components and a few lessons learned from the implementing and applying the system as well 2008 ieee enhancement of offset analgesia during sequential testing interruption of a continuous noxious heat by a relatively greater noxious heat evokes reductions in pain experience when the original noxious heat returns the reduction is greater than that evoked by continuous delivery of noxious heat this disproportionate reduction in pain experience known as offset analgesia is presumably mediated by a mechanism different to adaptation or habituation reduction in pain experience to an equivalent noxious stimulus however has also been demonstrated when applying the same stimulus over a number of days this reduction due to repeated days of stimulation is known as attenuation in order to distinguish further the mechanisms of offset analgesia and attenuation we applied noxious heat resulting in an experience of low medium or high pain to the volar forearm of 16 subjects comparing pain intensity ratings for increases and decreases in temperature repeated over 3 days offset analgesia was consistently demonstrated but the effects of attenuation were more complex there was no attenuation effect for the unchanging stimuli delivered across the 3 days of testing but attenuation effects enhanced the offset analgesia resulting in a larger offset analgesia effect on days 2 and 3 it is possible that offset analgesia and attenuation are mediated by inter related mechanisms further studies might investigate whether offset analgesia involves inhibitory structures such as the pag rvm 2008 european federation of chapters of the international association for the study of pain attention chronic pain inhibition nociception somatic managing module dependencies to facilitate continuous testing developing large commercial software systems is complex techniques have been proposed to deal with such large scale systems development one approach that has had some success is the combination of continuous integration and unit testing large systems are often divided into modules based on an area of functionality with potentially different teams developing each module invariably these modules rely on and interact with each other problems arise when teams wishing to test their module depend on code from other teams that is in development or under modification during maintenance this paper describes an approach to managing module dependencies that allows teams to test their code in isolation or in conjunction with other modules this facilitates the ability to continually run tests without being negatively impacted by the state of other modules an additional side effect is that the approach exceeds the requirements of the gnu lesser general public license by allowing a software vendor to easily provide a limited amount of source code rather than potentially releasing source code for large portions of their product suite or allowing reverse engineering of potentially large portions of their proprietary product 2008 elsevier b v all rights reserved continuous integration software design and implementation software engineering software licensing testing team pace keeping build times down the use of automated build and continuous integration systems by software teams is a well established practice and has been shown to provide significant benefits however to qualify the value of continuous integration practices it is necessary to compare their cost with the associated benefit when considering automated builds or continuous integration in an agile team this cost is the time taken by developers to run the build script and automated tests before code can be committed and or deployed this paper discusses the effect of this time on team behaviour by comparing two projects with significantly different build times 2008 ieee pushing the boundaries of testing and continuous integration agile and extreme programming practices have popularised concepts of test driven development and continuous build cycles to the software community such practices are typically adopted to implement and deliver functionality early in the development process however some types of applications such as the one described in this report also require continuous tests for performance and robustness this report shows the experiences of the authors in extending the continuous build loop to include additional tests for performance and robustness with the intention of overcoming limitations of standard testing frameworks when applied to highly concurrent and real time applications it also describes how they went about building the appropriate framework to support the execution and verification of the test results 2008 ieee extending continuous integration into alm continuous integration ci the practice made popular by agile methodologies has been constrained so that it is localized within the lifecycle and provides only a partial picture of software quality continuous integration is a practice made up of two components including team members integrating their work frequently and integration not degrading code quality continuous integration has the promise of providing the automation framework that is needed to decrease the turnaround time on the longer running tests a continuous integration build is a process that extracts source code from the source code manager scm compiles it packages it and then runs some tests on the resulting artifacts one of the defining properties of staged ci is that each loop is a different build type this means that each stage builds the source code in addition to running one or more processes the staged ci approach results in multiple stages where each stage is a different build type handmade devices for continuous delivery of hypochlorite for well disinfection during the cholera outbreak in douala cameroon 2004 well disinfection is generally recommended as an emergency response measure during cholera outbreaks however few studies have been carried out to document chlorination techniques prove the efficacy of chlorination or determine how often disinfection should be performed the purpose of this study was to test a handmade device for continuous chlorination to measure the initial concentration of free residual chlorine and monitor chlorine concentration to determine when renewal is necessary eighteen wells in 2 neighbors of douala cameroon i e 9 wells neighborhood were tested testing included daily measurement of water volume ph and residual chlorine for a period of two weeks after installing the handmade device composed of river sand and hypochlorite in a pre perforated plastic bag that was renewed after disappearance of free residual chlorine the maximum concentration of residual chlorine was reached after 1 day in 31 out of 36 chlorinations or 2 days in 5 out of 36 on day 4 the chlorine level was less than 0 2 mg l in half of the wells the chlorine concentration was higher in family than community wells notwithstanding feasibility and acceptability issues the device allowed chlorination at effective nontoxic levels for 3 days these findings open the possibility of developing devices allowing longer diffusion at tower cost for use within the framework of integrated cholera epidemic control programs chlorination cholera outbreak well dependence of hearing changes on the dose of intratympanically applied gentamicin a meta analysis using mathematical simulations of clinical drug delivery protocols objective hypothesis to establish safe dosing protocols for the treatment of patients with meniere s disease with intratympanic gentamicin study design a validated computer model of gentamicin dispersion in the inner ear fluids was used to calculate cochlear drug levels resulting from specific clinical delivery protocols dosing in the cochlea was compared with changes of hearing sensitivity for 568 patients reported in 19 publications methods cochlear drug levels were calculated based on the concentration and volume of gentamicin applied the time the drug remained in the middle ear and on the specific timing of injections time courses were quantified in terms of the maximum concentration c max and the area under the curve of the drug at specific cochlear locations results drug levels resulting from single oneshot injections were typically lower than those from repeated or continuous application protocols comparison of hearing sensitivity changes with gentamicin dosing revealed a flat curve with a near zero mean for lower doses suggesting that hearing changes with doses over this range were probably unrelated to the applied drug higher intracochlear doses were generated with repeated or continuous delivery protocols which in some cases caused substantial hearing losses and an increased incidence of deafened ears one shot application protocols produce gentamicin doses in the cochlea that have minimal risk to hearing at the frequencies tested repeated or continuous application protocols result in higher doses that in some cases damage hearing the high variability of hearing changes even with low gentamicin doses calls into question the rationale for using individual hearing changes to titrate the applied dose 2008 the american laryngological rhinological and otological society inc endolymphatic hydrous intratvmrjanic gentamicin meniere s disease ototoxicity surrogate a simulation apparatus for continuous integration testing in service oriented architecture cit continuous integration testing has been widely studied in the testing research field in order to start some levels of integration test as early as possible one challenge of cit lies in how to simulate the behavior of those unavailable components existing methods like stud and mock fail to provide the advanced component simulation capabilities required by cit from perspectives like diversified program artifacts behavior transitivity and configurability this paper proposes a new simulation apparatus namely surrogate to address this problem the surrogate generator generates platform specific code skeleton from definition of the component to be simulated the generated code communicates with surrogate engine and returns simulated platform specific behaviors the surrogate engine simulates component behaviors including both output and possible invocation on dependent components moreover it provides platform independent interfaces and configuration model early implementations of surrogate generator and surrogate engine are introduced in detail to validate the value of surrogate technology in cit a case study has been carried out with careful analysis the result shows that this technology really helps identify some bugs at early stage of development 2008 ieee continuous integration testing service oriented architecture simulation stochastic versus deterministic kernel based superposition approaches for dose calculation of intensity modulated arcs dose calculations for radiation arc therapy are traditionally performed by approximating continuous delivery arcs with multiple static beams for 3d conformal arc treatments the shape and weight variation per degree is usually small enough to allow arcs to be approximated by static beams separated by 5° 10° but with intensity modulated arc therapy imat the variation in shape and dose per degree can be large enough to require a finer angular spacing with the increase in the number of beams a deterministic dose calculation method such as collapsed cone convolution superposition will require proportionally longer computational times which may not be practical clinically we propose to use a homegrown monte carlo kernel superposition technique mcks to compute doses for rotational delivery the imat plans were generated with 36 static beams which were subsequently interpolated into finer angular intervals for dose calculation to mimic the continuous arc delivery since mcks uses random sampling of photons the dose computation time only increased insignificantly for the interpolated static beam plans that may involve up to 720 beams ten past imrt cases were selected for this study each case took approximately 15 30 min to compute on a single cpu running mac os x using the mcks method the need for a finer beam spacing is dictated by how fast the beam weights and aperture shapes change between the adjacent static planning beam angles mcks however obviates the concern by allowing hundreds of beams to be calculated in practically the same time as for a few beams for more than 43 beams mcks usually takes less cpu time than the collapsed cone algorithm used by the pinnacle 3 planning system 2008 institute of physics and engineering in medicine backtracking incremental continuous integration failing integration builds are show stoppers development activity is stalled because developers have to wait with integrating new changes until the problem is fixed and a successful build has been run we show how backtracking can be used to mitigate the impact of build failure s in the context of component based software development this way even in the face of failure development may continue and a working version is always available 2008 ieee build management software configuration management software maintenance building a robust development environment stephen rylander a software engineer at cdw focusing on web scalability and best practices provides strategies and insights for source code management build automation and human factors rylander states that ownership of source code is necessary for its maintenance and is also essential to help stakeholders understand various scenarios for structuring source re engineering a source control tool or folder structure includes continuous integration ci to minimize the number of branches that are necessary for development isolation the ccnet server is used to label the source code after a build and provide traceability of the component for production rylander highlighted that a platform upgrade from the net 1 1 framework to the net 3 5 framework requires maintainance of two versions of the source for a period of time without interfering with each other and be built with different compilers overview of the comprehensive automated maintenance environment optimized cameo system the comprehensive automated maintenance environment optimized cameo system provides an adaptable joint service capability that supports continuous integration and automation of operational maintenance and logistical processes to improve aircraft readiness for the war fighter community the cameo system was originally initiated as a project to co locate and integrate the v 22 s aircraft maintenance event ground station amegs and interactive electronic technical manual ietm systems for increased efficiency of the usmc and usaf maintainers and reduced computer hardware and logistics footprint the effort was subsequently expanded to include airframe and dynamic component fatigue life tracking through integration with the navy s structural appraisal of fatigue effects safe program the unique nature of the cameo system is that it is an open source non proprietary automated maintenance environment ame allowing maximum flexibility for rapid incorporation of emerging technology and new requirements as well as inexpensive re use across other aircraft programs interested in leveraging off of an existing ame solution or collaborating on a common ame solution regardless of the airframe oem or data recording system vendor copyright 2008 by the american helicopter society international inc all rights reserved continuous integration in open source software development commercial software firms are increasingly using and contributing to open source software thus they need to understand and work with open source software development processes this paper investigates whether the practice of continuous integration of agile software development methods has had an impact on open source software projects using fine granular data from more than 5000 active open source software projects we analyze the size of code contributions over a project s life span code contribution size has stayed flat we interpret this to mean that open source software development has not changed its code integration practices in particular within the limits of this study we claim that the practice of continuous integration has not yet significantly influenced the behavior of open source software developers 2008 international federation for information processing quantum corrected monte carlo simulation of double gate silicon on insulation transistors the trend towards continuous integration of the nanometer scale and the emergence of non conventional device concepts such as multi gate transistors present important challenges for the semiconductor community simulation tools have to be adapted to this new scenario where classical approaches are not sufficiently accurate and quantum effects must be taken into account this review presents the different methods for including quantum corrections in monte carlo mc simulations without solving the schrödinger equation and is focused on the most recent and accurate technique the multi valley effective conduction band edge method mv ecbe the approach considers the effects of an arbitrary effective mass tensor describing valley characteristics and confinement directions while avoiding the use of effective mass as a fitting parameter the performance of the mv ecbe method is tested using an ensemble mc emc simulator to study benchmark devices for next itrs technological nodes and including steady state transient and quasi ballistic situations copyright 2008 american scientific publishers all rights reserved electron transport monte carlo simulation multigate transistors quantum simulation silicon on insulator cicuts combining system execution modeling tools with continuous integration environments system execution modeling sem tools provide an effective means to evaluate the quality of service qos of enterprise distributed real time and embedded dre systems sem tools facilitate testing and resolving performance issues throughout the entire development life cycle rather than waiting until final system integration sem tools have not historically focused on effective testing new techniques are therefore needed to help bridge the gap between the early integration capabilities of sem tools and testing so developers can focus on resolving strategic integration and performance issues as opposed to wrestling with tedious and error prone low level testing concerns this paper provides two contributions to research on using sem tools to address enterprise dre system integration challenges first we evaluate several approaches for combining continuous integration environments with sem tools and describe cicuts which combines the cuts sem tool with the cruisecontrol net continuous integration environment second we present a case study that shows how cicuts helps reduce the time and effort required to manage and execute integration tests that evaluate qos metrics for a representative dre system from the domain of shipboard computing the results of our case study show that cicuts helps developers and testers ensure the performance of an example enterprise dre system is within its qos specifications throughout development instead of waiting until system integration time to evaluate qos 2008 ieee measuring continuous integration capability continuous integration ci is an important lean software development practice measuring the capability of your ci environment provides a road map for improvement and an aid for sharing practices between projects the ci grid introduced in this article is a simple question based metric for checking the current ci capability of a project software builders the tools and processes we use to transform our system s source code into an application that we can deploy or ship have always been important but nowadays they can mean the difference between success and failure software building s golden rule is that you should automate all build tasks the most popular tool options for doing this are the facilities that your integrated development environment ide provides the various implementations of make and apache ant and maven using an ide can be problematic ant and maven provide a portable solution for java developers while make is more flexible and transparent you can optimize the build process by appropriate dependency tracking and parallelization once an automated build process is in place you can use it as a basis for continuous builds 2008 ieee ant automation build process computer bugs continuous integration dependency tracking documentation java make marine vehicles maven process control software tinderbox follicle stimulating hormone does not impact male bone mass in vivo or human male osteoclasts in vitro bone loss in the elderly is mainly caused by osteoclast induced bone resorption thought to be causally linked to the decline in estrogen and testosterone levels in females and males recently involvement of follicle stimulating hormone fsh in this process has been suggested to explain in part the etiology of the disease in females whereas its role in males has never been examined in this study the direct impact of fsh on bone mass of 16 week old c57bl 6j male mice by either daily intermittent application of 6 or 60 μg kg of fsh or continuous delivery via miniosmotic pump of a dose of 6 μg kg over the course of a month was assessed femoral peripheral quantitative computed tomographic and microcomputed tomographic analyses at 0 2 and 4 weeks of fsh treated mice did not reveal any differences in cancellous and cortical bone compared to sham treated mice fsh functionality was verified by demonstrating camp induction and activation of a camp response element containing reporter cell line by fsh furthermore osteoclastogenesis from human mononuclear cell precursors and from raw 264 7 cells was not affected by fsh 3 10 30 ng ml compared to control no direct effect of fsh on gene regulation was observed by affymetrix gene array on raw 264 7 cells lastly no expression of fsh receptor fshr mrna or fshr was observed by quantitative polymerase chain reaction and western blot in either human male osteoclasts or raw 264 7 cells these data show that fsh does not appear to modulate male bone mass regulation in vivo and does not act directly on osteoclastogenesis in vitro 2008 springer science business media llc bone remodeling follicle stimulating hormone mouse osteoclast osteoporosis spheramine for treatment of parkinson s disease spheramine bayer schering pharma ag berlin germany is currently being tested as a new approach for the treatment of parkinson s disease pd it consists of an active component of cultured human retinal pigment epithelial hrpe cells attached to an excipient part of cross linked porcine gelatin microcarrriers spheramine is administered by stereotactic implantation into the striatum of pd patients and the use of immunosuppression is not required current pharmacologic therapies of pd are oriented to the administration of dopaminergic medications human rpe cells produce levodopa and this constitutes the rationale to use spheramine for the treatment of pd the preclinical development of spheramine included extensive biologic pharmacologic and toxicologic studies in vitro and in animal models of pd the first clinical trial in humans evaluated the safety and efficacy of spheramine implanted in the postcommissural putamen contralateral to the most affected side in six patients with advanced pd this open label study demonstrated good tolerability and showed sustained motor clinical improvement a phase ii double blind randomized multicenter placebo controlled sham surgery study is underway to evaluate safety tolerability and efficacy of spheramine implanted bilaterally into the postcommissural putamen of patients with advanced pd spheramine represents a treatment approach with the potential of supplying a more continuous delivery of levodopa to the striatum in advanced pd than can be achieved with oral therapy alone 2008 the american society for experimental neurotherapeutics inc cell transplantation cellular therapies neurodegeneration neurotherapeutics parkinson s disease retinal pigment epithelial cells continuous integration and performance testing continuous integration systems is used to perform integration performance and load testing continuous integration is a software engineering process where an application under development is completely rebuilt and tested frequently and automatically a continuous integration process is dependent on several factors including a single code repository automated build process with self testing and continuous integration server continuous performance management implements performance and scalability testing within a continuous integration environment which can perform integration and load testing extending continuous integration to different types of performance testing makes the investment in continuous integration even more worthwhile enteral levodopa carbidopa infusion in advanced parkinson disease long term exposure objectives in patients with advanced parkinson disease levodopa carbidopa formulated as a gel suspension duodopa permits continuous delivery into the small intestine using a portable pump resulting in less variability in levodopa concentrations and fewer motor fluctuations and dyskinesias than with oral levodopa administration this is a retrospective analysis of the long term clinical experience with this agent methods all but 1 of the patients who had received enteral levodopa infusion treatment between january 1 1991 and june 30 2002 consented to a review of their hospital charts results of the 65 patients with initial testing of the treatment 86 opted for continued treatment via percutaneous endoscopic gastrostomy or gastrojejunostomy total exposure to levodopa infusion was 216 patient years mean 3 7 years maximum treatment duration was 10 7 years fifty two patients were treated for 1 year or longer the adverse effect profile of levodopa carbidopa infusion was similar to that observed with oral administration of levodopa seven deaths occurred all considered unrelated to the treatment intestinal tube problems including dislocation of the intestinal tube to the stomach were the most common technical problem occurring in 69 of the patients during the first year the optimal daily dose of levodopa decreased by an average of 5 during follow up conclusions the safety of enteral infusion of levodopa carbidopa formulated as a gel suspension was found acceptable for most patients the technical challenges posed by the enteral infusion system were offset by the improvement in motor fluctuations and dyskinesias offered by this technique 2008 lippincott williams wilkins inc dosage duodopa levodopa infusion parkinson safety validation and quantification of an in vitro model of continuous infusion of submicron sized particles wear particles produced from total joint replacements have been shown to stimulate a foreign body and chronic inflammatory reaction that results in periprosthetic osteolysis most animal models that simulate these events have used a single injection of particles which is not representative of the clinical scenario in which particles are continuously generated the goal of this study was to evaluate the feasibility of an osmotic pump for the continuous delivery of clinically relevant submicron sized particles over an extended period of time blue dyed polystyrene particles and retrieved ultra high molecular weight polyethylene uhmwpe particles both suspended in mouse serum were loaded into an alzet mini osmotic pump pumps were attached to vinyl tubing that ended with hollow titanium rods simulating a metal implant which was suspended in a collection vessel the number of particles collected was evaluated over 2 and 4 week time periods delivery of both the polystyrene and uhmwpe particles was feasible over pump concentrations of 10 9 to 10 11 particles per pump furthermore delivery efficiency of polystyrene particles decreased with increasing initial particle concentration whereas delivery efficiency of uhmwpe particles increased slightly with increasing initial particle concentration for uhmwpe approximately one third of the particles in the pump were collected at 4 weeks this in vitro study has quantified the efficiency of a unique particle pumping system that may be used in future in vivo investigations to develop a murine model of continuous particle infusion 2007 wiley periodicals inc animal model osteolysis polyethylene uhmwpe prosthetic loosening wear debris 14th international conference on distributed multimedia systems dms 2008 the proceedings contain 70 papers the special focus in this conference is on distributed multimedia systems i distributed multimedia systems ii distributed multimedia computing distributed multimedia systems iii and digital home and healthcare the topics include a recommendation system for segmented video streaming content utilizing categorized events and stimuli a comparison of distributed data parallel multimedia computing over conventional and optical wide area networks a lightweight augmented reality system measuring reliability component in business process automation optimizing the architecture of adaptive complex applications using genetic programming a soft real time scheduling algorithm videoconference support system using printed and handwritten keywords reporting events in a multimedia content distribution and consumption system distributed interactive multimedia for technology enhanced learning and automated content production and distribution unifying quality standards to capture architectural knowledge for web services domain tassonomy and review of complex content models testing multi agent systems for deadlock detection based on uml models ip surveillance system using interactive dual camera hand off control with fov boundary detection action patterns probing for dynamic service composition in home network a flexibile contxt aware middleware for developing context aware applications projects and goals for the eclipse italian community enhancing rationale management with second life meetings a method to diagnose self weaknesses for software development organizations a distributed system for continuous integration with jini transforming uml sequence diagrams to class diagrams and goal driven design transformation by applying design patterns monte carlo calculation of helical tomotherapy dose delivery helical tomotherapy delivers intensity modulated radiation therapy using a binary multileaf collimator mlc to modulate a fan beam of radiation this delivery occurs while the linac gantry and treatment couch are both in constant motion so the beam describes from a patient phantom perspective a spiral or helix of dose the planning system models this continuous delivery as a large number 51 of discrete gantry positions per rotation and given the small jaw fan width setting typically used 1 or 2 5 cm and the number of overlapping rotations used to cover the target pitch often 0 5 the treatment planning system tps potentially employs a very large number of static beam directions and leaf opening configurations to model the modulated fields all dose calculations performed by the system employ a convolution superposition model in this work the authors perform a full monte carlo mc dose calculation of tomotherapy deliveries to phantom computed tomography ct data sets to verify the tps calculations all mc calculations are performed with the egsnrc based mc simulation codes beamnrc and dosxyznrc simulations are performed by taking the sinogram leaf opening versus time of the treatment plan and decomposing it into 51 different projections per rotation as does the tps each of which is segmented further into multiple mlc opening configurations each with different weights that correspond to leaf opening times then the projection is simulated by the summing of all of the opening configurations and the overall rotational treatment is simulated by the summing of all of the projection simulations commissioning of the source model was verified by comparing measured and simulated values for the percent depth dose and beam profiles shapes for various jaw settings the accuracy of the mlc leaf width and tongue and groove spacing were verified by comparing measured and simulated values for the mlc leakage and a picket fence pattern the validated source and mlc configuration were then used to simulate a complex modulated delivery from fixed gantry angle further a preliminary rotational treatment plan to a delivery quality assurance phantom the cheese phantom ct data set was simulated simulations were compared with measured results taken with an a1sl ionization chamber or edr2 film measurements in a water tank or in a solid water phantom respectively the source and mlc mc simulations agree with the film measurements with an acceptable number of pixels passing the 2 1 mm gamma criterion 99 8 of voxels of the mc calculation in the planning target volume ptv of the preliminary plan passed the 2 2 mm gamma value test 87 0 and 66 2 of the voxels in two organs at risk oars passed the 2 2 mm tests for a 3 3 mm criterion the ptv and oars show 100 93 2 and 86 6 agreement respectively all voxels passed the gamma value test with a criterion of 5 3 mm the tomotherapy tps showed comparable results 2008 american association of physicists in medicine convolution superposition monte carlo tomotherapy tps verification cruisecontrol rb continuous integration the rails way cruisecontrol rb is reincarnation of cruisecontrol an open source continuous integration tool written in ruby its basic purpose in life is to alert members of a software project when one of them checks something into source control that breaks the build cruisecontrol rb was created by thoughtworks to meet the needs of a growing number of ruby projects in the company it is small and simple takes about 10 minutes to install has a streamlined web interface and can be easily modified this application takes the spirit and values of ruby on rails web development framework to the field of continuous integration continuous integration cruisecontrol rails ruby an empirical study examining the usage and perceived importance of xp practices extreme programming xp is a well known agile software development methodology which is ideal for projects featured as highly unpredictable in tasks with limited resources the continuous discussion on the usage and importance of each xp practice lead us to explore what are the most important xp practices to be applied in certain projects this study examined the actual usage amount and perceived importance of each xp practice by means of a cross sectional anonymous survey conducted in local organizations which have implemented xp in their projects results indicate that continuous integration and collective ownership as the most important collective ownership continuous integration pair programming planning game and sustainable pace are used the most both practitioners and researchers can build upon these findings agile methods extreme programming software development training future software developers to acquire agile development skills students related to software development are motivated to acquire agile techniques by guiding them towards delivering large scale systems agile techniques are rapidly transforming into adopted development methodology commercially an advanced course information systems development project isdp is offered to the students of software engineering database systems and other related fields in which for the first time they experienced a simulated but realistic software development environment observations show that the agile techniques has positive impact on development progress and system quality some additional recommendations are derived few of them are mandate function list being pragmatic conceptualizing unit testing before coding and continuous integration of the system buildbot robotic monitoring of agile software development teams in this paper we describe buildbot a robotic interface developed to assist with the continuous integration process utilized by co located agile software development teams buildbot s physical nature allows us to engage the agile software development team members through vision hearing and touch in this way buildbot becomes an active part of the development process by bringing together human robot interaction human group dynamics and software engineering concepts through a number ofinteraction modalities in this paper we describe the design and implementation ofthe buildbot prototype a robotic interface that can sense virtual stimuli in this case the state of a software build and react accordingly in a physical way via vision sound and touch we present an early evaluation comparing buildbot to two other tools used by an agile team to monitor the continuous integration process we also show preliminary results indicating that buildbot may be more noticeable to the developers and contribute to a fun and lighthearted atmosphere we argue that by increasing awareness of the state of the software build buildbot can assist in the self supervision ofagile software engineering teams and can help the team achieve its goals in a more engaging and sociable manner 2007 ieee extreme programming in action a longitudinal case study rapid application development rad has captured interest as a solution to problems associated with traditional systems development describing the adoption of agile methods and extreme programming by a software start up we find that all xp principles were not adopted equally and were subject to temporal conditions small releases on site customer continuous integration and refactoring were most vigorously advanced by management and adopted by developers paired programming on the other hand was culturally avoided springer verlag berlin heidelberg 2007 agile methods extreme programming rapid application development software amp systems engineering process and tools for the development of autonomous driving intelligence when a large number of people with heterogeneous knowledge and skills run a project together it is important to use a sensible engineering process this especially holds for a project building an intelligent autonomously driving car to participate in the 2007 darpa urban challenge in this article we present essential elements of a software and systems engineering process for the development of artificial intelligence capable of driving autonomously in complex urban situations the process includes agile concepts like test first approach continuous integration of every software module and a reliable release and configuration management assisted by software tools in integrated development environments however the most important ingredients for an efficient and stringent development are the ability to efficiently test the behavior of the developed system in a flexible and modular simulator for urban situations testing of changing requirement in an agile environment a case study of telecom project this paper is focused on before and after changes of agile implementation and describing the successful testing goals of agile implementation in flow product project telecom project after implementation of agile methodology in tech mahindra team would really gain a drastic change in testing activity continuous delivery customer satisfaction and also help to meet challenges of changing requirements and new functionalities 2007 ieee the hurie method a case study combining requirements gathering and user interface evaluation this chapter discusses that as the usability team evaluates the emerging user interface ui design for a game like tool for teaching battle command software through distance learning they found themselves in the uneasy position of having started to plan for two pluralistic usability walk throughs wts with two different but equally time constrained user samples and a new discovery that even the product development team members were uncertain as to the product requirements the team could have postponed the ui evaluations but instead chose to try to take full advantage of the test participants already scheduled and to conduct a hybrid method wherein they in parallel gathered user requirements information and evaluated the ui prototype they selected the pluralistic usability wt as their evaluation method both because it allowed them to collect data from a number of these valuable and hard to schedule users at the same time and that it would allow for a dialogue between these representative users and the product developers thus speeding up the back and forth that was necessary if they were to be engaged in an iterative design process the team added a requirements gathering exercise onto the front of the pluralistic usability wt to collect user requirements data before biasing the pluralistic usability wt participants with the ui prototype the early delivery of a ui prototype for evaluation helps meet the agile principle of early and continuous delivery 2007 elsevier inc all rights reserved test driven design methodology for component based system for modern systems there is growing proof that serial traditional approaches such as the traditional waterfall model and model driven architecture are ineffective and development lifecycles need to be iterative and incremental in this presentation we discuss the iterative and incremental approach for software design methodology called test driven design tdd the tdd development cycle starts with the requirement specification and therefore captures defects much earlier in the development cycle tdd requires that no production code be written until first a unit test is written we compare tdd with the traditional methods and describe in detail the tdd method we cover continuous integration acceptance testing system wide testing for each iteration test frameworks cost of change roi benefits and limitations of the new test driven design and provide evidence from industry that tdd leads to higher programmer productivity with higher code quality the future work investigations will extend the reach and effectiveness of tdd by using latest technologies to generate tests from message sequence charts and generating code thru use of a model compiler leading to an advanced test driven design methodology further investigations will also look at the concurrency issues by use of ltsa labelled transition analyzer technology 2007 ieee cruisin and chillin testing the java based distributed ground data system chill with cruisecontrol this paper describes the design of the development test environment for the mission data processing and control subsystem mpcs code named chill mpcs chill is currently in development to support the mars science laboratory msl scheduled for launch in 2009 chill is a linux based ground data system which includes both telemetry and command functions the development test configuration consists of five levels unit testing end to end testing user interface testing external interface testing and installation deployment testing this paper will focus primarily on the automation of the lowest two levels unit and end to end testing mpcs chill s continuous integration process is provided by its adaptation of cruisecontrol which is an open source framework for a continuous build process cruisecontrol is configured on the dedicated build machine a linux workstation which is the target platform chill has configured cruisecontrol into two project loops the first fetches the latest version of software from a central repository builds it and performs unit tests junit the second loop runs scripted end to end tests which are performed on the results of the first build results are reported via email notification and a web interface provides the details of the current and previous builds for each loop the evolution of test definition is requirements feed into design design leads to use cases and tests are derived from use cases thus leading to the mapping of tests to requirements unit tests operate on internal components while end to end tests operate at a higher level of abstraction and therefore can be traced to requirements the evolutionary process ensures that we are testing to requirements mpcs chill presents a model for testing java based distributed ground systems in a semi automated manner the mpcs model is highly applicable to other projects looking to automate their testing in addition to achieving continuous integration 2007 ieee the sisyphus continuous integration system integration hell is a prime example of software evolution gone out of control the sisyphus continuous integration system is designed to prevent this situation in the context of component based software configuration management we show how incremental and backtracking techniques are applied to strike a balance between maximal feedback and being up to the minute and how these techniques enable automation of release and delivery 2007 ieee ideal a 6 month double blind placebo controlled study of the first skin patch for alzheimer disease the rivastigmine patch is the first transdermal treatment for alzheimer disease ad by providing continuous delivery of drug into the bloodstream over 24 hours transdermal delivery may offer benefits superior to those of oral administration this study compared the efficacy safety and tolerability of rivastigmine patches with capsules and placebo ideal investigation of transdermal exelon in alzheimer s disease was a 24 week double blind double dummy placebo and active controlled study patients with ad were randomized to placebo or one of three active treatment target dose groups 10 cm rivastigmine patch delivering 9 5 mg 24 hours 20 cm rivastigmine patch 17 4 mg 24 hours or 6 mg bid rivastigmine capsules primary efficacy measures were the alzheimer s disease assessment scale cognitive subscale and alzheimer s disease cooperative study clinical global impression of change secondary outcome measures assessed a range of domains including behavior cognitive performance at tention executive functions and activities of daily living a total of 1 195 ad patients participated all rivastigmine treatment groups showed significant improvement relative to placebo the 10 cm patch showed similar efficacy to capsules with approximately two thirds fewer reports of nausea 7 2 vs 23 1 and vomiting 6 2 vs 17 0 incidences statistically not significantly different from placebo 5 0 and 3 3 for nausea and vomiting respectively the 20 cm patch showed earlier improvement and numerically superior cognitive scores vs the 10 cm patch with similar tolerability to capsules local skin tolerability was good the transdermal patch with rivastigmine may offer additional therapeutic benefits and may prove to be the best delivery system for this drug to treat ad 2007aan enterprises inc accelerated angiogenesis by continuous medium flow with vascular endothelial growth factor inside tissue engineered trachea objective to test the effects of a continuous medium flow inside degrapol scaffolds on the reepithelialization and revascularization processes of a tissue engineered trachea prosthesis methods in this proof of principle study a continuous medium flow was maintained within a tubular degrapol scaffold by an inserted porous catheter connected to a pump system the impact of the intra scaffold medium flow on the survival of a tracheal epithelial sheet wrapped around and on chondrocyte delivery to the degrapol scaffold was studied in the chick embryo chorioallantoic membrane cam model angiogenesis within the biomaterial was investigated results scanning electronic microscopy sem images showed an intact epithelial layer after a 2 week support by continuous medium flow underneath on histology three dimensional cell growth was detected in the continuous delivery group the cam assay showed that angiogenesis was enhanced within the degrapol scaffolds when vascular endothelial growth factor vascular permeability factor vegf vpf was added to the perfusate conclusions taken together these results demonstrated that the built in perfusion system within degrapol scaffolds was able to maintain an intact tracheal epithelial layer allowed a continuous delivery of cells and kept an efficient vegf vpf expression level which accelerated angiogenic response in the cam assay this design combines the in vitro and in vivo parts of tissue engineering and offers the possibility to be used as an in vivo bioreactor implanted for the tissue engineered reconstruction of trachea and of other organs 2007 european association for cardio thoracic surgery bioreactor reepithelialization revascularization tissue engineering tracheal prosthesis agile documentation strategies the planning strategies implemented by agile software developers for documentation are discussed most of agile software developers used technical practices such as regression testing refactoring and continuous integration instead of techniques such as modeling and governance for documentation they identified the requirement of documentation of stakeholders such as business management it management operations staff and enterprise architects agile software developers developed documentation as a stakeholder requirement agile software developers used the principle of literate programming of writing source code that contain embedded documentation they also used the just in time jit concept for meeting the requirements of documentation of stakeholders fischer tropsch synthesis in a two phase reactor with presaturation the fischer tropsch synthesis fts has successfully been tested by a novel technology the so called presaturated one liquid flow polf technology in the polf process only the liquid phase enters the fixed bed reactor but the liquid is previously saturated with gaseous reactants outside the reactor in a special devise of high intensity the continuous delivery of the mixture of carbon monoxide and hydrogen to the reaction zone is provided by a liquid recirculation through the saturator such process arrangements permit to use an uncomplicated fixed bed reactor and to simplify the temperature control by the removal of reaction heat in an external heat exchanger installed in the loop in this technology there is no danger of any runaways because of the low adiabatic temperature rise in comparison to the conventional fixed bed or slurry reactors the polf reactor has a very simple design and an inherent safety which probably results in lower investment and operation costs in the present article experimental results are presented and problems concerning the catalyst behaviour and the applicability of the polf process are discussed 2007 urban verlag hamburg wien gmbh fischer tropsch synthesis in a two phase reactor with presaturation the fischer tropsch synthesis fts has successfully been tested by a novel technology the so called presaturated one liquid flow polf technology in the polf process only the liquid phase enters the fixed bed reactor but the liquid is previously saturated with gaseous reactants out side the reactor in a special devise of high intensity the continuous delivery of the mixture of carbon monoxide and hydrogen to the reaction zone is provided by a liquid recirculation through the saturator such process arrangements permit to use an uncomplicated fixed bed reactor and to simplify the temperature control by the removal of reaction heat in an external heat exchanger installed in the loop in this technology there is no danger of any runaways because of the low adiabatic temperature rise in comparison to the conventional fixed bed or slurry reactors the polf reactor has a very simple design and an inherent safety which probably results in lower investment and operation costs in the present article experimental results are presented and problems concerning the catalyst behaviour and the applicability of the polf process are discussed 2007 urban verlag hamburg wien gmbh temozolomide plga microparticles and antitumor activity against glioma c6 cancer cells in vitro the purpose of the present study was to develop implantable poly d l lactide co glycolide plga microparticles for continuous delivery of intact 3 4 dihydro 3 methyl 4 oxoimidazo 5 1 d as tetrazine 8 carboxamide temozolomide tm for about a 1 month period and to evaluate its cytotoxicity against glioma c6 cancer cells the emulsifying solvent evaporation process has been used to form tm loaded plga microparticles the influences of several preparation parameters such as initial drug loading polymer concentration and stirring rate were investigated scanning electron microscopy sem showed that such microparticles had a smooth surface and a spherical geometry i e microspheres the differential scanning calorimetry dsc and powder x ray diffraction xrd results indicated that tm trapped in the microparticles existed in an amorphous or disordered crystalline status in the polymer matrix the release profiles of tm from microparticles resulted in biphasic patterns after an initial burst a continuous drug release was observed for up to 1 month finally a cytotoxicity test was performed using glioma c6 cancer cells to investigate the cytotoxicity of tm delivered from plga microparticles it has been found that the cytotoxicity of tm to glioma c6 cancer cells is enhanced when tm is delivered from plga polymeric carrier and plga only did not affect the growth of the cells meanwhile the cytotoxic activity of tm powder disappeared within 12 h 2006 cytotoxicity emulsifying solvent evaporation microparticles poly d l lactide co glycolide plga temozolomide tm continuous testing for enterprise portal technology refresh this paper presents experiences and lessons learned detailing software testing for a technology refresh project that took place over a period of one year the paper describes the continuous testing performed while the team ported a web portal from one environment to another which will probably affect more websites and web portals as time goes on the original portlet based application was constructed four years ago and has been experiencing problems during peak demand times the overall goal of this technology refresh project is to achieve stability of the portal based on a more robust architecture this paper summarizes test plans and results for iterative functional automated regression and continuous performance testing the team designed the system for better performance monitored test results from the early stage of the lifecycle and unveiled potential issues using continuous integration and testing continuous testing enterprise portal performance testing software testing technology refresh assessing undergraduate experience of continuous integration and test driven development a number of agile practices are included in software engineering curricula including test driven development continuous integration often is not included despite it becoming increasingly common in industry to code test and integrate at the same time this paper describes a study whereby software engineering undergraduates were given a short intensive experience of test driven development with continuous integration using an environment that imitated a typical industrial circumstance assessment was made of students agile experience rather than of project deliverables using a novel set of process measures that examined students participation and performance in agile testing results showed good participation by student pairs and clear understanding of agile processes and configuration management future work will investigate automation of the assessment of continuous integration and configuration management server data agile practices assessment process continuous integration junit software engineering education test driven development a roadmap for using agile development in a traditional environment one of the newer classes of software engineering techniques is called agile development in agile development software engineers take small implementation steps and in some cases they program in pairs in addition they develop automatic tests prior to implementing their small functional piece agile development focuses on rapid turnaround incremental planning customer involvement and continuous integration agile development is not the traditional waterfall method or even a rapid prototyping method although this methodology is closer to agile development at the jet propulsion laboratory jpl a few groups have begun agile development software implementations the difficulty with this approach becomes apparent when agile development is used in an organization that has specific criteria and requirements handed down for how software development is to be performed the work at the jpl is performed for the national aeronautics and space agency nasa both organizations have specific requirements rules and processes for developing software this paper will discuss some of the initial uses of the agile development methodology the spread of this method and the current status of the successful incorporation into the current jpl development policies and processes 2006 by the american institute of aeronautics and astronautics inc constructing real time collaborative software engineering tools using caise an architecture for supporting tool development real time collaborative software engineering cse tools have many perceived benefits including increased programmer communication and faster resolution of development conflicts demand and support for such tools is rapidly increasing but the cost of developing such tools is prohibitively expensive we have developed an architecture caise to support the rapid development of cse tools it is envisaged that the architecture will facilitate the creation of a range of tools allowing the perceived benefits of collaboration to be fully realised in this paper we focus on the development of cse tools within the caise architecture we present examples to illustrate how such tools are constructed and how they support real time multi user collaborative software development we also address issues related to the number of collaborators and discuss performance aspects copyright 2006 australian computer society inc collaborative software engineering continuous integration cscw amp groupware tool construction a selective particle size sampler suitable for biological exposure studies of diesel particulate the objective of this study is the design construction and evaluation of a selective particle size sps sampler able to provide continuous delivery of diesel soot particles of specific size ranges the design of the sampler combines principles of aerosol transport phenomena and separation technologies particles smaller than a given size are removed from the exhaust by diffusional deposition while removal of particles above a given size is achieved by low pressure inertial impaction the main application of the developed sampler is the exposure of biological samples such as cell and tissue cultures to selected sizes of diesel exhaust particles by applying the sps sampler to diesel exhaust it is demonstrated that it is possible to obtain two aerosol streams with widely separated particle size distributions of nanometric dimensions suitable for biological exposure studies preliminary tests with cell cultures indicate some differences in the biological impact of smaller vs larger diesel nanoparticles copyright 2006 sae international automating functional tests using selenium ever in search of a silver bullet for automated functional testing for web applications many folks have turned to selenium selenium is an open source project for in browser testing originally developed by thoughtworks and now boasting an active community of developers and users one of selenium s stated goals is to become the de facto open source replacement for proprietary tools such as winrunner of particular interest to the agile community is that it offers the possibility of test first design of web applications red green signals for customer acceptance tests and an automated regression test bed for the web tier this experience report describes the standard environment for testing with selenium as well as modifications we performed to incorporate our script pages into a wiki it includes lessons we learned about continuous integration script writing and using the selenium recorder renamed ide we also discuss how long it took to write and maintain the scripts in the iterative development environment how close we came to covering all of the functional requirements with tests how often the tests should be and were run and whether additional automated functional testing below the gui layer was still necessary and or appropriate while no silver bullet selenium has become a valuable addition to our agile testing toolkit and is used on the majority of our web application projects it promises to become even more valuable as it gains widespread adoption and continues to be actively developed 2006 ieee software carpentry getting scientists to write better code by making them more productive the prospects of developing software skills in the university of toronto to enable emerging scientists to write better code by making them more productive are discussed a software carpentry course emphasizes on small scale and intermediate practical software issues and makes all study materials available on websites for self study purposes the course aims to teach computational scientists the methods to meet standards and meet the software quality in a systematic manner it also enables a software engineer to recreate and rerun programs and test codes that are used to produce the results version control assists project management by its ability to undo the stakes and facilitate teamwork gnu enables an engineer to build the programs by its easy documentation and file configuration the continuous integration while checking codes is attained the test suites are rerun and results are posted to the project mailing list websites the multivalley effective conduction band edge method for monte carlo simulation of nanoscale structures the trend toward continuous integration of the nanometer scale and the rise of nonconventional device concepts such as multigate transistors present important challenges for the semiconductor community simulation tools have to be adapted to this new scenario where classical approaches are not sufficiently accurate and quantum effects have to be taken into account this paper proposes a method of including quantum corrections in monte carlo mc simulations without solving the schrödinger equation the approach based on the effective conduction band edge ecbe method considers the effects of an arbitrary effective mass tensor describing valley characteristics and confinement directions while avoiding the use of effective mass as a fitting parameter the performance of the multivalley ecbe method is tested using an ensemble mc simulator to study benchmark devices for next international technology roadmap for semiconductors technological nodes a 25 nm channel length bulk mosfet and a double gate silicon on insulator mosfet in both steady state and transient situations 2006 ieee monte carlo mc methods mosfets quantum well semiconductor device modeling silicon on insulator soi technology frogi fractal components deployment over osgi this paper presents frogi a proposal to support continuous deployment activities inside fractal a hierarchical component model frogi is implemented on top of the osgi platform motivation for this work is twofold on one hand frogi provides an extensible component model to osgi developers and eases bundle providing frogi based bundles are still compatible with legacy osgi bundles that offer third party services on the other hand frogi benefits from the deployment infrastructure provided by osgi which simplifies conditioning and packaging of fractal components with frogi it is possible to automate the assembly of a fractal component application partial or complete deployment is also supported as well as performing continuous deployment and update activities springer verlag berlin heidelberg 2006 modeling longitudinal performances on the united states medical licensing examination and the impact of sociodemographic covariates an application of survival data analysis background this study models time to passing united states medical licensing examination usmle for the computer based testing cbt start up cohort using the cox proportional hazards model method the number of days it took to pass step 3 was treated as the dependent variable in the model covariates were 1 gender 2 native language english or other 3 medical school location united states or other and 4 citizenship united states or other results examinees were 59 times as likely to pass usmle if they were trained abroad additionally examinees who reported having english as their primary language and u s citizenship were more likely to ultimately pass usmle finally though gender was also associated with passing usmle its practical significance was very small conclusion cox regression provides a useful tool for modeling performances in a continuous delivery model findings suggest that passing the usmle sequence tends to be associated with native english speaking usmgs who also hold u s citizenship continuous release and upgrade of component based software we show how under certain assumptions the release and delivery of software updates can be automated in the context of component based systems these updates allow features or fixes to be delivered to users more quickly furthermore user feedback is more accurate thus enabling quicker response to defects encountered in the field based on a formal product model we extend the process of continuous integration to enable the agile and automatic release of software components component from such releases traceable and incremental updates are derived we have validated our solution with a prototype tool that computes and delivers updates for a component based software system developed at cwi copyright 2005 acm follow the sun distributed extreme programming development in early 2004 a global company brought together three independent development regions united states of america seattle united kingdom poole and singapore to form one 24×5 around the clock extreme programming team a year after merging into one team the group is effectively using full extreme programming practices across the world on a single codebase this experience report describes the challenges we face in this environment lessons learned and how we resolved issues such as global continuous integration cultural differences and conflicting priorities across regions 2005 ieee simulation based validation and defect localization for evolving semi formal requirements models when requirements models are developed in an iterative and evolutionary way requirements validation becomes a major problem in order to detect and fix problems early the specification should be validated as early as possible and should also be revalidated after each evolutionary step in this paper we show how the ideas of continuous integration and automatic regression testing in the field of coding can be adapted for simulation based automatic revalidation of requirements models after each incremental step while the basic idea is fairly obvious we are confronted with a major obstacle requirements models under development are incomplete and semi formal most of the time while classic simulation approaches require complete formal models we present how we can simulate incomplete semi formal models by interactively recording missing behavior or functionality however regression simulations must run automatically and do not permit interactivity we therefore have developed a technique where the simulation engine automatically resorts to the interactively recorded behavior in those cases where it does not get enough information from the model during a regression simulation run finally we demonstrate how the information gained from model evolution and regression simulation can be exploited for locating defects in the model 2005 ieee a software methodology for applied research extreme researching applied research is by necessity a distributed collaborative process to be useful research methodologies must not only be applicable in such an environment but must also be adaptive to the needs of human resources and specific research area requirements this paper introduces extreme researching xr an adaptation of extreme programming xp by ericsson to support distributed telecommunications research and development xr builds on xp and tailors it to meet the needs of applied industrial research it adopts and extends the most useful elements of xp collective ownership planning game continuous integration and metaphor and shows how they are applicable in multi site research projects xpweb is developed as a tool to facilitate xr in a distributed research environment xpweb and xr are actively used by ericsson applied research and have been shown to significantly increase output and efficiencies in multi disciplinary research projects copyright 2005 john wiley sons ltd agile processes extreme programming process engineering telecommunications evaluation of the male reproductive organs after treatment with continuous sustained delivery of statin for fracture healing the 3 hydroxy 3 methylglutaryl coenzyme a hmg coa reductase inhibitors statins are widely used for the treatment of hyperlipidemia and recent in vitro and animal data suggest that statins promote bone formation and increase bone strength we examined the relationship between sustained continuous delivery of statin and fracture healing rates in adult male animals with femoral segmental fractures because statin affects the production of cholesterol we also evaluated the influence of statin on adrenal and testicular steroidogenesis and the morphology of the reproductive tract tissues in animals receiving statin for a period of 12 weeks post surgery simvastatin significantly increased fracture healing and without significant influence on the body weights and the weights of the reproductive organs basal plasma lh fsh and testosterone levels were not affected by active treatment with simvastatin reproductive tissue morphology was unchanged by local sustained release of statin in conclusion long term simvastatin treatment delivered at a fracture target site did not influence testicular reproductive and endocrine function but was able to effectively heal complicated segmental fracture copyright 2005 isa all rights reserved drug delivery system fracture healing osteogenic agent simvastatin statins tricalcium phosphate lysine test driven development concepts taxonomy and future direction the test driven development strategy requires writing automated tests prior to developing functional code in small rapid iterations xp is an agile method that develops object oriented software in very short iterations with little upfront design although not originally given this name tdd was described as an integral xp practice necessary for analysis design and testing that also enables design through refactoring collective ownership continuous integration and programmer courage along with pair programming and refactoring tdd has received considerable individual attention since xp s introduction developers have created tools specifically to support tdd across a range of languages and have written numerous books explaining how to apply tdd concepts researchers have begun to examine tdd s effects on defect reduction and quality improvements in academic and professional practitioner environments and educators have started to examine how to integrate tdd into computer science and software engineering pedagogy hollow metal microneedles for insulin delivery to diabetic rats the goal of this study was to design fabricate and test arrays of hollow microneedles for minimally invasive and continuous delivery of insulin in vivo as a simple robust fabrication method suitable for inexpensive mass production we developed a modified liga process to micromachine molds out of polyethylene terephthalate using an ultraviolet laser coated those molds with nickel by electrodepostion onto a spotter deposited seed layer and released the resulting metal microneedle arrays by selectively etching the polymer mold mechanical testing showed that these microneedles were sufficiently strong to pierce living skin without breaking arrays containing 16 microneedles measuring 500 μm in length with a 75 μm tip diameter were then inserted into the skin of anesthetized diabetic hairless rats insulin delivery through microneedles caused blood glucose levels to drop steadily to 47 of pretreatment values over a 4 h insulin delivery period and were then approximately constant over a 4 h postdelivery monitoring period direct measurement of plasma insulin levels showed a peak value of 0 43 ng ml together these data suggest that microneedles can be fabricated and used for in vivo insulin delivery 2005 ieee drug delivery systems laser machining micromachining evaluation of the male reproductive organs after treatment with continuous sustained delivery of statin for fracture healing the 3 hydroxy 3 methylglutaryl coenzyme a hmg coa reductase inhibitors statins are widely used for the treatment of hyperlipidemia and recent in vitro and animal data suggest that statins promote bone formation and increase bone strength we examined the relationship between sustained continuous delivery of statin and fracture healing rates in adult male animals with femoral segmental fractures because starin affects the production of cholesterol we also evaluated the influence of statin on adrenal and testicular steroidogenesis and the morphology of the reproductive tract tissues in animals receiving statin for a period of 12 weeks post surgery simvastatin significantly increased fracture healing and without significant influence on the body weights and the weights of the reproductive organs basal plasma lh fsh and testosterone levels were not affected by active treatment with simvastatin reproductive tissue morphology was unchanged by local sustained release of statin in conclusion long term simvastatin treatment delivered at a fracture target site did not influence tesricular reproductive and endocrine function but was able to effectively heal complicated segmental fracture copyright 2005 isa all rights reserved drug delivery system fracture healing osteogenic agent simvastatin statins tricalcium phosphate lysine selective glial cell line derived neurotrophic factor production in adult dopaminergic carotid body cells in situ and after intrastriatal transplantation glial cell line derived neurotrophic factor gdnf exerts a notable protective effect on dopaminergic neurons in rodent and primate models of parkinson s disease pd the clinical applicability of this therapy is however hampered by the need of a durable and stable gdnf source allowing the safe and continuous delivery of the trophic factor into the brain parenchyma intrastriatal carotid body cb autografting is a neuroprotective therapy potentially useful in pd it induces long term recovery of parkinsonian animals through a trophic effect on nigrostriatal neurons and causes amelioration of symptoms in some pd patients moreover the adult rodent cb has been shown to express gdnf here we show using heterozygous gdnf lacz knock out mice that unexpectedly cb dopaminergic glomus or type i cells are the source of cb gdnf among the neural or paraneural cells tested glomus cells are those that synthesize and release the highest amount of gdnf in the adult rodent as measured by standard and in situ elisa furthermore gdnf expression by glomus cells is maintained after intrastriatal grafting and in cb of aged and parkinsonian 1 methyl 4 phenyl 1 2 3 6 tetrahydropyridine treated animals thus glomus cells appear to be prototypical abundant sources of gdnf ideally suited to be used as biological pumps for the endogenous delivery of trophic factors in pd and other neurodegenerative diseases copyright 2005 society for neuroscience carotid body gdnf expression gdnf knock out mice glomus cells intrastriatal transplants parkinsonian models cardioprotection by ε protein kinase c activation from ischemia continuous delivery and antiarrhythmic effect of an ε protein kinase c activating peptide background we previously showed that a selective activator peptide of ε protein kinase c pkc ψεrack conferred cardioprotection against ischemia reperfusion when delivered ex vivo before the ischemic event here we tested whether in vivo continuous systemic delivery of ψεrack confers sustained cardioprotection against ischemia reperfusion in isolated mouse hearts and whether ψεrack treatment reduces infarct size or lethal arrhythmias in porcine hearts in vivo methods and results after ψεrack was systemically administered in mice either acutely or continuously hearts were subjected to ischemia reperfusion in an isolated perfused model whereas ψεrack induced cardioprotection lasted 1 hour after a single intraperitoneal injection continuous treatment with ψεrack induced a sustained preconditioned state during the 10 days of delivery there was no desensitization to the therapeutic effect no downregulation of εpkc and no adverse effects after sustained ψεrack delivery porcine hearts were subjected to ischemia reperfusion in vivo and ψεrack was administered by intracoronary injection during the first 10 minutes of ischemia ψεrack treatment reduced infarct size 34 ± 2 versus 14 ± 1 control versus ψεrack and resulted in fewer cases of ventricular fibrillation during ischemia reperfusion 87 5 versus 50 control versus ψεrack conclusions the εpkc activator ψεrack induced cardioprotection both in vivo and ex vivo reduced the incidence of lethal arrhythmia during ischemia reperfusion and did not cause desensitization or downregulation of εpkc after sustained delivery thus ψεrack may be useful for patients with ischemic heart disease in addition the ψεrack peptide should be a useful pharmacological agent for animal studies in which systemic and sustained modulation of εpkc in vivo is needed arrhythmia ischemia ischemic preconditioning peptides protein kinase c role of hippocampal m2 muscarinic receptors in the estrogen induced enhancement of working memory we have previously demonstrated that acetylcholine acting at m2 muscarinic receptors mediates the estradiol induced increase in hippocampal n methyl d aspartate receptor binding and the associated enhancement in working memory the goal of present experiment was to investigate the role of hippocampal m2 receptors in the behavioral aspects of these effects ovariectomized rats were trained to locate a hidden escape platform on a matching to place version of the water maze in which the platform was moved to a new location for each session of four daily trials following 18 days of training rats were randomly assigned to receive one of the following treatments 1 injections of oil vehicle delivered 72 and 48 h before testing and continuous delivery of vehicle into the dorsal hippocampus via bilateral cannulae implants connected to osmotic minipumps 2 injections of estradiol benzoate eb delivered 72 and 48 h before testing and continuous delivery of vehicle into the hippocampus 3 injections of eb delivered 72 and 48 h before testing and continuous delivery of the m2 muscarinic receptor antagonist afdx 116 into the hippocampus and 4 injections of eb delivered 72 and 48 h before testing and continuous delivery of afdx 116 into a control site in the cortex chronic administration of afdx 116 into the hippocampus but not the cortex significantly attenuated an estrogen induced enhancement in performance on a working memory task in the water maze as indicated by increased latency and increased path length to locate an escape platform during a test trial when a 90 min delay was imposed between the first and second trials these results indicate that acetylcholine acts at m2 muscarinic receptors located in the hippocampus to mediate the positive effects exerted by estrogen on working memory 2005 published by elsevier ltd on behalf of ibro acetylcholine ovarian hormones water maze experimental study of an automated system for the delivery of eyedrops using a microinfusion pump purpose to determine the feasibility of using a commercially available microinfusion pump for the continuous delivery of eye drops using a rabbit model design laboratory investigation methods tear secretion was measured after attaching a microinfusion pump to the superior fornix of a rabbit the pump was set to deliver artificial tears continuously a rabbit eye was first chemically burned with 1n naoh the pump was then set to deliver 0 1 fluorometholone continuously results were compared with those obtained using 0 1 fluorometholone results schirmer tests indicated that an average of 22 3 mm in eyes supported by a pump and an average of 10 3 mm in eyes without pump moreover eyes treated with corticosteroid delivered by pump recovered faster than those treated with topical corticosteroid conclusions the continuous delivery of eye drops by a microinfusion pump could be applicable to patients with severe dry eyes or ocular surface diseases further study should be needed 2005 by elsevier inc all rights reserved reliability support for the model driven architecture reliability is an important concern for software dependability quantifying dependability in terms of reliability can be carried out by measuring the continuous delivery of a correct service or equivalently the mean time to failure the novel contribution of this paper is to provide a means to support reliability design following the principles of the model driven architecture mda by doing this we hope to contribute to the task of consistently addressing dependability concerns from the early to late stages of software engineering additionally we believe mda can be a suitable framework to realize the assessment of those concerns and therefore semantically integrate analysis and design models into one environment springer verlag berlin heidelberg 2004 patch application of the dopamine agonist rotigotine to patients with moderate to advanced stages of restless legs syndrome a double blind placebo controlled pilot study efficacy and safety of the dopamine agonist rotigotine rtg was investigated in patients with moderate to severe idiopathic restless legs syndrome rls including daytime symptoms three fixed doses of rotigotine 1 125 mg 2 25 mg and 4 5 mg and placebo were applied by patches size 2 5 cm 2 per 1 125 mg in a double blind randomized parallel group multicenter 1 week proof of principle trial the primary efficacy measure was the total score on the international restless legs syndrome scale irls additionally the rls 6 scale the clinical global impressions cgi and a sleep diary were used of 68 enrolled patients 63 mean age 58± 9 years 64 women were randomly assigned rls severity improved related to dose by 10 5 1 125 mg rtg die p 0 41 12 3 2 25 mg rtg die p 0 18 and 15 7 points 4 5 mg rtg die p 0 01 on the irls compared to placebo 8 points according to the rls 6 scales daytime symptoms significantly improved with all rotigotine doses the cgi items supported the favorable efficacy of the 4 5 mg dose skin tolerability of the patches and systemic side effects were similar between rotigotine and placebo this pilot study suggests that continuous delivery of rotigotine by means of a patch may provide an effective and well tolerated treatment of rls symptoms both during night and day 2004 movement disorder society dopamine agonist irls restless legs syndrome rls 6 rotigotine therapy continuous integration amp net part ii the establishment of a continuous integration environment using standard tools by microsoft is discussed the concurrent versions systems cvs an open source version control system which runs on linux and windows was used for this establishment cruisecontrol net is an open source tool from sourceforge designed to meld several other open source building and testing tools and completely automate the build process the plans to introduce fxcop and other net framework design guidelines are underway and these tools would be applied with continuous integration process continuous integration amp net part i a continuous integration system which describes an automated process that lets teams build and test their software many times a day is discussed the complete continuous integration solution encompasses automated builds data driven unit testing documentation version control and code coverage a change in the build file and the addition of appropriate documetation comments to the source code are needed to get the documentation automatically generated the basic tools supporting continuous integration help to put together a process to automate the building testing and documentation of net applications restriction of neuroblastoma angiogenesis and growth by interferon α β purpose we tested the hypothesis that the antiangiogenic activity of the type i interferons ifns could affect tumor engraftment and growth in murine xenograft models of neuroblastoma methods subcutaneous and retroperitoneal human neuroblastoma xenografts were established in scid mice five days after tumor cell inoculation daily subcutaneous injections of human ifn α at several different doses were initiated and continued for 30 days the effectiveness of continuous delivery of low dose interferon was then tested using a gene therapy approach in which an adeno associated virus vector encoding ifn β raav ifn β was used to mediate expression prior to retroperitoneal tumor implantation results subcutaneous and retroperitoneal tumors were significantly smaller in ifn α treated mice as compared with control mice intratumoral basic fibroblast growth factor and vascular endothelial growth factor expression were also decreased as was mean intratumoral endothelial cell density interestingly the lower doses of ifn α were more effective than the higher dose no tumors developed in any of the mice given raav ifn β whereas each of the mice that received control vector developed large tumors conclusions treatment with ifn had a significant impact on neuroblastoma engraftment and growth in mice particularly when delivered continuously using a gene therapy approach this activity appears to be mediated in part by inhibition of tumor induced angiogenesis through the downregulation of tumor elaborated factors including basic fibroblast growth factor and vascular endothelial growth factor comparison of intraperitoneal continuous infusion of floxuridine and bolus administration in a peritoneal gastric cancer xenograft model purpose to identify the optimal schedule for intraperitoneal i p infusion of floxuridine fudr against peritoneal micrometastases from gastric cancer methods the efficacy of continuous i p infusion of fudr was compared with that of bolus i p administration in peritoneal gastric cancer mkn45 xenografts the fudr continuous delivery system in this study was in the form of injectable poly lactic coglycolic acid plga microspheres intended for i p injection animals were treated by continuous i p infusion using fudr loaded microspheres or bolus i p administration of fudr results in vitro testing demonstrated that fudr was released slowly from the microspheres at a rate of approximately 5 of the total encapsulated drug per day in in vivo studies the peritoneal level was found to persist and was approximately 5 to 50 fold higher than that of plasma for more than 2 weeks following a single injection of the microspheres an in vitro mtt assay showed that exposure time clearly influenced th e cytotoxic potency of fudr in vivo continuous infusion was more effective against peritoneal tumor than bolus administration at equivalent doses however compared with bolus administration toxicity was increased resulting in a reduced maximum tolerated dose mtd with continuous infusion when the treatment was carried out at each mtd continuous 1 mg kg bolus 600 mg kg continuous infusion had no advantage in inhibiting tumor growth conclusions owing to the higher toxicity and the equal efficacy of continuous infusion compared with bolus administration continuous infusion is not recommended in i p fudr treatment springer verlag 2004 adjuvant chemotherapy drug delivery system fudr microsphere plga hepatocellular nodules in cirrhosis focus on diagnostic criteria on liver biopsy a western experience the spectrum of so called space occupying small 0 5 2 5 cm sizable nodules arising in the cirrhotic liver includes a series of hyperplastic large regenerative dysplastic low and high grade dysplastic and malignant hepatocellular well differentiated hepatocellular carcinoma hcc nodules major progress in their classification and understanding was achieved through image analysis techniques and careful histological dissection of explanted native livers needless to say the actual understanding of their natural history is crucial to a proper histological classification the differential diagnosis of these hepatocellular nodules is difficult particularly on biopsy specimens of focal liver lesions revealed by ultrasound us taken during the follow up of cirrhotic patients in this study we attempted to summarize on the basis of our experience essential clinicopathological features useful to distinguish the different nodules on needle biopsy synoptic tables of differential diagnosis and figures of elementar lesions which have to be looked for are provided only the continuous integration of clinical features image analysis information of pathological findings and follow up data allows establishing the autonomy of these polymorphic and controversial entities and the boundaries between them copyright 2004 by the american association for the study of liver diseases scaling continuous integration springer verlag berlin heidelberg 2004 of all the extreme programming practices continuous integration is one of the least controversial – the benefits of an integrated streamlined build process is something that software developers immediately recognise however as a project scales up in size and complexity continuous integration can become increasingly hard to practice successfully by focussing on the problems associated with a growing project this paper describes a variety of strategies for successfully scaling continuous integration enterprise continuous integration using binary dependencies springer verlag berlin heidelberg 2004 continuous integration ci is a well established practice which allows us as developers to experience fewer development conflicts and achieve rapid feedback on progress ci by itself though becomes hard to scale as projects get large or have independent deliverables enterprise continuous integration eci is an extension to ci that helps us regain the benefits of ci when working with separately developed yet interdependent modules we show how to develop an eci process based upon binary dependencies giving examples using existing net tools continuous integration net scalability tools and techniques fishbowl xp tools springer verlag berlin heidelberg 2004 this session is an opportunity to learn more about the tools that enable teams to be extreme using a fishbowl format participants will discuss and debate the pros and cons of such tools as eclipse intellij and visual studio nunit and csunit continuous integration and cruise control fit and fitnesse and more if you re looking for practical advice on tools for xp teams – their selection usage and whatever improvements are desired you ll enjoy this session 5th international conference on extreme programming and agile processes in software engineering xp 2004 the proceedings contain 60 papers the special focus in this conference is on acceptance testing scalability issues new insights refactoring and social issues the topics include putting a motor on the canoo webtest acceptance testing framework generative acceptance testing for difficult to test software distributed product development using extreme programming efficient markets efficient projects and predicting the future agile principles and open source software development agile specification driven development towards a proper integration of large refactorings in agile software development the oregon software development process empirical analysis on the satisfaction of it employees comparing xp practices with other software development methodologies agile processes enhancing user participation for small providers of off the shelf software self adaptability of agile software processes enterprise continuous integration using binary dependencies automated generation of unit tests for refactoring test driven development and software process improvement in china a comparison of software development process experiences literate programming to enhance agile methods application of lean and agile principles to workflow management assistance for supporting xp test practices in a distributed cscw environment combining ad hoc and regression testing complete test generation for extreme programming conditional test for javabeans components agile methods in software engineering education extreme programming in a university project a selection framework for agile methodologies designing the ultimate acceptance testing framework and xp and organizational change cruisecontrol net continuous integration for net with the release of microsoft s net framework a large number of agile tools are being ported to take advantage of the new platform to support development on its net projects thought works has produced a functional port of cruisecontrol its popular continuous integration build server to the net platform cruisecontrol net replicates the majority of the functionality included in the latest 2 0 release of cruisecontrol and adds a number of new features to provide integration with net tools and technologies cruisecontrol net has recently been released as open source by thought works under a bsd style license and is freely available for download at http ccnet thoughtworks com springer verlag berlin heidelberg 2003 reengineering legacy application to e business with modified rational unified process experience in reengineering a legacy application into a web based j2ee system with modified rational unified process rup is presented rup is adopted into an onsite offshore development model along with iso 9001 and sei cmm level 5 standards the new application has above 2500 code components and the effort is about 100 person years for the benefit of software development community some of our experiences in design development testing and project management are elaborated as generalized concepts we have demonstrated that development process could be improved with lessons learnt from the initial iterations the three views of a web application are explained and the translations between the layers are discussed benefit of continuous integration is highlighted various types of dependencies to be taken into account for sequencing the development are elaborated the levels of testing in iterative development are mentioned the importance of adaptive team structure and various parameters guiding iteration planning are dealt with a simple estimation model based on types of transactions is presented finally a fine grained risk management concept that can integrate with the development process is proposed 2003 ieee a testing checklist for database programs managing risk in an agile environment quoin inc has been using agile testing methodologies such as continuous integration and unit testing in its development of sql based java software since 1998 based on that experience we present a checklist containing twenty six database related items to consider when testing such software the checklist is annotated with examples of good and bad development and testing practices while this paper targets projects that use both sql and java most of the checklist items are applicable to any database transactions in any language managers and developers can use this checklist as a starting point for discussion of what types of tests to require for their particular project especially when operating in an agile environment such as xp referring to the checklist will enable the project to develop more robust code with less effort springer verlag berlin heidelberg 2003 agile java sql testing xp issues in scaling agile using an architecture centric approach a tool based solution agile software development processes are best applied to small teams on small to medium sized projects scaling agile methodologies is desired in order to bring the benefits of agile to larger more complex projects one way to scale agile methods is via an architecture centric approach in which a project is divided into smaller modules on which sub teams can use agile effectively however a problem with architecture centric modifications to agile methods is the introduction of non agile elements for instance up front design and integration difficulties these issues are discussed and a tool based solution is presented facilitating the adoption of the architecture centric agile approach springer verlag berlin heidelberg 2003 agile methods automated testing continuous integration cruisecontrol scaling test driven design parallel processing approaches in ret and mdp new hybrid multithreading and distributed technology for optimum throughput in a hierarchical flow the continuous integration trend in design and broad deployment of resolution enhancement techniques ret have a tremendous impact on circuit file size and pattern complexity increasing design cycle time has drawn attention to the data manipulation steps that follow the physical layout of the design the contributions to the total turn around time for a design are twofold the time to get the data ready for the hand off to the mask writer is growing but also the time it takes to write the mask is heavily influenced by the size and complexity of the data in order to reduce the time that is required for the application of ret and the export of the data to mask writer formats massively parallel processing approaches have been described this paper presents such computing algorithms for the hierarchical implementation of ret and mask data preparation mdp we focus on the parallel and flexible deployment of a new hybrid multithreaded and distributed processing scheme in homogeneous and heterogeneous computer networks called mtflex we describe the new methodology and discuss corresponding hardware and software configurations the application of this ì mtflexî computing scheme to different tasks in post tapeout data preparation is shown in examples high performance fracturing for variable shaped beam mask writing machines mask manufacturing for the 100 and 65nm nodes is accompanied by an increasing deployment of vsb mask writing machines the continuous integration trend in design and broad deployment of ret have a tremendous impact on file size and pattern complexity the impact on the total turn around time for a design is twofold the time to get the data ready for the hand off to the mask writer is growing but also the time it actually takes to write the mask is heavily influenced by the size and complexity of the data different parameters are measures of how the flow and the particular tooling impact both portions the efficiency of the data conversion flow conducted by a software tool can be measured by the output file size the scalability of the computing during parallel processing on multiple processors and the total cpu time for the transformation the mask writing of a particular data set is affected by the file size and the shot count the latter one is the total amount of shots that are required to expose all patterns on the mask the shot count can be estimated based on the figure count by type and their dimensions the results of the fracturing have an impact on the mask quality in particular the grid size and the number and locations of small figures norfloxacin releasing urethral catheter for long term catheterization norfloxacin releasing urethral catheters were prepared for the purpose of preventing urinary tract infections during long term catheterization the outer and inner surfaces of the catheters were coated with poly ethylene co vinyl acetate eva and an amphiphilic multiblock co polymer peo 2k pdms composed of poly ethylene oxide and poly dimethyl siloxane norfloxacin a fluoroquinolone synthetic antibiotic was impregnated into a coating layer the in vitro drug release behavior was monitored for 30 days the surface topography was investigated using scanning electron microscopy sem and the antibacterial activity against different bacteria implicated in urinary tract infection was evaluated by the in vitro inhibition zone test all the coated catheters showed continuous delivery of norfloxacin for up to 30 days owing to hydrophobic natures of norfloxacin and eva peo 2k pdms incorporated in a coating layer produced a smooth and uniform surface the coated catheters created considerable inhibition zones for 10 days against escherichia coli klebsiella pneumoniae and proteus vulgaris indicating the continuous release of norfloxacin overall it was evident that the catheters coated with eva peo 2k pdms blends containing norfloxacin have a promising potential for the clinical use in patients undergoing long term catheterization controlled release eva inhibition zone long term catheterization multiblock co polymer norfloxacin urethral catheter spasticity in a child with myelomeningocele treated with continuous intrathecal baclofen patients with myelomeningocele may often suffer from severe spasticity surgical treatment of the underlying pathology such as hydromyelia and tethered cord may be successful but failures are not uncommon those cases may offer a surgical challenge since further therapeutic options are limited we present the case of a 7 year old boy with myelomeningocele and related conditions suffering from severe spasticity and pain in his lower limbs surgical efforts with untethering and posterior fossa decompression failed to improve the symptoms a test with 25 μg intrathecally delivered baclofen showed a total relief of spasticity and pain so that a pump for continuous baclofen delivery was implanted during 32 months of follow up his spasticity has been under excellent control on 55 157 μg baclofen per day continuous delivery of intrathecal baclofen may be a surgical option to consider in patients with myelomeningocele and severe spasticity copyright 2003 s karger ag basel intrathecal baclofen myelomeningocele spasticity increased renal medullary h2o2 leads to hypertension we have recently reported that exaggerated oxidative stress in the renal medulla due to superoxide dismutase inhibition resulted in a reduction of renal medullary blood flow and sustained hypertension the present study tested the hypothesis that selective scavenging of o 2 in the renal medulla would prevent hypertension associated with this exaggerated oxidative stress an indwelling aortic catheter was implanted in nonnephrectomized sprague dawley rats for daily measurement of arterial blood pressure and a renal medullary interstitial catheter was implanted for continuous delivery of the superoxide dismutase inhibitor diethyldithiocarbamic acid detc 7 5 mg · kg 1 · d 1 and a chemical superoxide dismutase mimetic 4 hydroxytetramethyl piperidine 1 oxyl tempol 10 mg · kg 1 · d 1 renal medullary interstitial infusion of tempol completely blocked detc induced accumulation of o 2 · 1 in the renal medulla as measured by the conversion rate of dihydroethidium to ethidium in the dialysate and by urinary excretion of 8 isoprostanes however tempol infusion failed to prevent detc induced hypertension unless catalase 5 mg · kg 1 · d 1 was coinfused direct infusion of h 2 o 2 into the renal medulla resulted in increases of mean arterial pressure from 115±2 5 to 131±2 1 mm hg which was similar to that observed in rats receiving the medullary infusion of both tempol and detc the results indicate that sufficient catalase activity in the renal medulla is a prerequisite for the antihypertensive action of tempol and that accumulated h 2 o 2 in the renal medulla associated with exaggerated oxidative stress might have a hypertensive consequence antioxidants blood pressure oxidative stress renal disease sodium a double lumen intrathecal catheter for studies of modulation of spinal opiate tolerance studies of spinal opioid tolerance frequently employ a spinal infusion model in which a single lumen intrathecal it catheter is connected to an osmotic mini pump we have modified this model by developing a double lumen catheter system that permits continuous delivery of the toleragen to the it space and allows for examination of the effects of concurrent it drug administration without interruption of the ongoing infusion the catheter is constructed of two pieces of pe10 tubing fused to the lumens of a dual lumen catheter 8 cm with one pe10 tube used as an infusion line connected to an osmotic pump and the other for injection the catheter is inserted through the cisterna magna most implanted rats 66 out of 73 showed full recovery of motor and sensory function without detectable neurological deficit the profile of the tolerance development and the response to drug manipulation using the double lumen catheter are similar to previous findings in the spinal infusion model which used a single lumen catheter most importantly we demonstrate that concurrent probe drug testing and or concurrent drug treatment can be achieved without interruption of spinal infusion of morphine using the double lumen catheter model expands the range of possibility for studies of spinal opiate tolerance and spinal drug delivery 2003 elsevier science b v all rights reserved intrathecal catheter opiate osmotic pump infusion spinal cord tolerance extreme programming turning the world upside down established software engineering practices which are applied to the demands of developing today s software is discussed it starts by describing the sort of broken process which is pandemic in the industry and then explains the nature of xp in terms of how it can address this problem this paper discusses some of the more controversial xp practices such as eschewing a formal requirements document and test driven development improved pain control after cardiac surgery results of a randomized double blind clinical trial objective we sought to determine whether a continuous regional infusion of a local anesthetic delivered to the operative site would result in decreased levels of postoperative pain and narcotic requirements for patients who undergo a standard median sternotomy for cardiac surgery methods a double blind randomized controlled trial was conducted at a single center patients who were undergoing elective coronary artery bypass graft surgery alone or combined with laser transmyocardial revascularization received bilateral intercostal nerve blocks with either ropivacaine or saline at wound closure 2 catheters with multiple side openings were inserted percutaneously and placed directly over the sternum the same agent ropivacaine vs saline was then administered as a continuous regional infusion for 48 hours through an elastomeric pump requirements for postoperative systemic narcotic analgesics and pain assessment scores were recorded for 72 hours after the operation secondary outcome measures were hospital length of stay and pulmonary function test results pain scores and narcotic use on the second postoperative day were also compared to avoid the confounding influence of anesthesia administered at the time of the operation results the total amount of narcotic analgesia required by the ropivacaine group was significantly less than that of the control group 47 3 vs 78 7 mg respectively p 038 the ropivacaine group required less narcotics on postoperative day 2 as well 15 5 vs 29 4 mg p 025 the mean overall pain scores for the ropivacaine group were significantly less than the mean overall scores for the normal saline group 1 6 vs 2 6 respectively p 005 patients receiving ropivacaine had a mean length of stay of 5 2 days compared with 8 2 days for patients in the normal saline group p 001 excluding the data from outliers length of stay 39 days the normal saline group mean length of stay was 6 3 days p 01 there was no difference in assessment of pulmonary function conclusion continuous delivery of local anesthetics significantly improved postoperative pain control while decreasing the amount of narcotic analgesia required in patients who underwent standard median sternotomy there was also a significant decrease in hospital length of stay which is likely to result in significant cost reductions complex fluid dynamics in biomems devices modeling of microfabricated microneedles mems technologies promises to revolutionize health care by providing precise control of biological fluids for both diagnoses and treatments for example microneedles can be used for sample collection for biological analysis delivery of cell or cellular extract based vaccines and sample handling providing interconnection between the microscopic and macroscopic world microneedles may be used for low flow rate continuous drug delivery such as the continuous delivery of insulin to a diabetic patient microneedles are interesting from a design perspective not only because of their small size but because they provide a range of geometries and flow characteristics this paper uses microneedle design as an example of the potential interaction between experiment and computation for the improved design of microfabricated microfluidic devices in general previously fluid flow in microneedles was studied experimentally here we use computational modeling capabilities in concert with experimental results to optimize the design of medical microneedles and thus to shorten the whole design fabrication cycle we compare cfd simulations to analysis and experiments for flows in three microneedle geometries straight bent and filtered fig 1 the bent microneedle was found to have the highest fluid carrying capacity of 0 082 ml sec at 138 kpa with a reynolds number of 738 a microneedle with a built in microfilter 192 μm wide 110μm high and 7mm long also had flow rates of 0 07 ml sec fig 2 although the throughput of these microneedles is low they still compare favorably with other microneedle designs laminar flow models were found to accurately predict the flow behavior through the microneedles all computational modeling was performed with the cfdrc cfd ace suite of software tools complex fluid dynamics in biomems devices modeling of microfabricated microneedles mems technologies promises to revolutionize health care by providing precise control of biological fluids for both diagnoses and treatments for example microneedles can be used for sample collection for biological analysis delivery of cell or cellular extract based vaccines and sample handling providing interconnection between the microscopic and macroscopic world microneedles may be used for low flow rate continuous drug delivery such as the continuous delivery of insulin to a diabetic patient microneedles are interesting from a design perspective not only because of their small size but because they provide a range of geometries and flow characteristics this paper uses microneedle design as an example of the potential interaction between experiment and computation for the improved design of microfabricated microfluidic devices in general previously fluid flow in microneedles was studied experimentally 1 here we use computational modeling capabilities in concert with experimental results to optimize the design of medical microneedles and thus to shorten the whole design fabrication cycle we compare cfd simulations to analysis and experiments for flows in three microneedle geometries straight bent and filtered fig 1 the bent microneedle was found to have the highest fluid carrying capacity of 0 082 ml sec at 138 kpa with a reynolds number of 738 a microneedle with a built in microfilter 192 urn wide 110μm high and 7 mm long also had flow rates of 0 07 ml sec fig 2 although the throughput of these microneedles is low they still compare favorably with other microneedle designs laminar flow models were found to accurately predict the flow behavior through the microneedles all computational modeling was performed with the cfdrc cfd ace suite of software tools cytopathological changes in early stages of benign prostatic hyperplasia upon exposure to sustained delivery of androgens the objective of this study was to assess the possible morphological changes that occur in the columnar epithelial cells of the prostate peripheral zone during early development of benign prostatic hyperplasia in this study benign prostatic hyperplasia bph was encouraged by the continuous delivery of androgens using tcpl drug delivery devices a total of 16 adult male rats were randomly divided into four groups n 4 group 1 served as an intact control and groups 2 4 were implanted with tricalcium phosphate lysine tcpl capsules designed to deliver continuous physiologic 40mg doses of specific androgens as follows group 2 testosterone test group 3 dihydrotestosterone dht group 4 androstenedione aed respectively upon completion of the study the columnar epithelial cells were targeted for morphometric analysis the means number of cells per high power field cell length cell area nuclear area and nuclear to cytoplasmic ratio were measured using image analysis techniques the results of this study showed that 1 the number of cells per high power field were not changed with treatment 2 the cell length was decreased with treatment of all androgens 3 the cell area was decreased with treatment 4 the nuclear area was increased with treatment of test and dht and 5 the n c ratio was increased with all three androgens these results suggest that very early in the development of bph remarkable changes occur in the nucleus of the columnar epithelial cells these changes may indicate a specific physiological response to irritation arising from the continuous delivery of androgens androstenedione benign prostatic hyperplasia continuous drug delivery dihydrotestosterone morphometric analysis prostate testosterone effect of insulin like growth factor 1 igf 1 plus alendronate on bone density during puberty in igf 1 deficient midi mice insulin like growth factor 1 igf 1 increases both bone formation and bone resorption processes to test the hypothesis that treatment with an antiresorber along with igf 1 during the pubertal growth phase would be more effective than igf 1 alone to increase peak bone mass we used an igf 1 midi mouse model which exhibits a 60 reduction in circulating igf 1 levels we first determined an optimal igf 1 delivery by evaluating igf 1 administration 2 mg kg body weight day by either a single daily injection three daily injections or by continuous delivery via a minipump during puberty of the three regimens the three daily igf 1 injections and igf 1 through a minipump produced a significant increase in total body bone mineral density bmd 6 0 and 4 4 respectively and in femoral bmd 4 3 and 6 2 respectively compared with the control group single subcutaneous s c administration did not increase bmd we chose igf 1 administration three times daily for testing the combined effects of igf 1 and alendronate 100 μg kg per day the treatment of igf 1 alendronate for a period of 2 weeks increased total body bmd at 1 week and 3 weeks after treatment 21 1 and 20 5 respectively and femoral bmd by 29 at 3 weeks after treatment these increases were significantly greater than those produced by igf 1 alone igf 1 but not alendronate increased bone length igf 1 and or alendronate increased both periosteal and endosteal circumference combined treatment caused a greater increase in the total body bone mineral content bmc and periosteal circumference compared with individual treatment with igf 1 or alendronate our data demonstrate that 1 inhibition of bone turnover during puberty increases net bone density and 2 combined treatment with igf 1 and alendronate is more effective than igf 1 or alendronate alone in increasing peak bone mass in an igf 1 deficient midi mouse model 2002 by elsevier science inc all rights reserved alendronate bone density bone size insulin like growth factor igf 1 midi mice puberty oxidative stress inhibits caveolin 1 palmitoylation and trafficking in endothelial cells during normal and pathological conditions endothelial cells ecs are subjected to locally generated reactive oxygen species produced by themselves or by other vessel wall cells in excess these molecules cause oxidative injury to the cell but at moderate levels they might modulate intracellular signalling pathways we have investigated the effect of oxidative stress on the palmitoylation and trafficking of caveolin 1 in bovine aortic ecs exogenous h 2 o 2 did not alter the intracellular localization of caveolin 1 in ecs however metabolic labelling experiments showed that h 2 o 2 inhibited the trafficking of newly synthesized caveolin 1 to membrane raft domains several mechanisms potentially responsible for this inhibition were examined impairment of caveolin 1 synthesis by h 2 o 2 was not responsible for diminished trafficking similarly the inhibition was independent of h 2 o 2 induced caveolin 1 phosphorylation as shown by the markedly different concentration dependences we tested the effect of h 2 o 2 on palmitoylation of caveolin 1 by the incorporation of 3 h palmitic acid exposure of ecs to h 2 o 2 markedly inhibited the palmitoylation of caveolin 1 comparable inhibition was observed after treatment of cells with delivered either as a bolus or by continuous delivery with glucose and glucose oxidase kinetic studies showed that h 2 o 2 did not alter the rate of caveolin 1 depalmitoylation but instead decreased the on rate of palmitoylation together these results show for the first time the modulation of protein palmitoylation by oxidative stress and suggest a cellular mechanism by which stress might influence caveolin 1 dependent cell activities such as the concentration of signalling proteins and cholesterol trafficking caveolae hydrogen peroxide post translational modification continuous antigen delivery from controlled release implants induces significant and anamnestic immune responses two continuous delivery injectable silicone implants were tested to determine if they were capable of delivering vaccines in a single shot the type a implant delivers antigen in vitro over a 1 month period and the type b over several months vaccination studies in sheep were designed to compare the responses induced by the type a and b implants alzet™ mini osmotic pumps and conventional antigen delivery a model antigen avidin was used along with il 1β or alum as adjuvants sheep were immunised with various formulations and the titre and isotype of the antigen specific antibodies monitored the type b implant induced antibody ab titres of greater magnitude and duration than soluble vaccines or the type a implant with adjuvant but only if il 1β was included in the formulation both implants induced antibodies of igg1 and igg2 isotype a memory response to soluble antigen challenge was induced by the type b il 1β implant which was predominantly of an igg1 isotype 2002 elsevier science ltd all rights reserved humoral immunity single dose vaccine vaccine delivery system new architectures for uav flight control avionics commercial and military aircraft utilize proven data bus standards such as mil std 1553b and arinc 429 however future avionics systems may take greater advantage of commercial hardware and networking technology with the increased processors and speeds it is possible to begin challenge the fundamental avionics architectures used for navigation and flight control this is particularly true for uavs where there is currently rapid change goals of new architectures include reduced development time cost increased testability lead to quicker flight validation and higher integrality of these the concepts addressed in a new architecture include scheduling tasks and continuous integration semen retrieval in men with spinal cord injury is improved by interrupting current delivery during electroejaculation purpose based on the findings of a previous study of pressure differentials in the external and internal urinary sphincters during electroejaculation we determined whether semen retrieval in men with spinal cord injury would be improved by interrupting current delivery during electroejaculation materials and methods we tested continuous versus interrupted current delivery in the same group of 12 men with spinal cord injury patients underwent a mean of 4 randomly assigned continuous or interrupted trials 4 to 8 weeks apart antegrade and retrograde semen parameters were analyzed per trial multiple trials of each method per patient were averaged and semen parameters by the continuous and interrupted methods were compared results interrupted delivery resulted in significantly greater mean antegrade volume versus continuous delivery 2 versus 0 9 cc in this antegrade fraction mean total sperm count and mean total motile sperm was higher for interrupted 130 million and 35 million versus continuous 79 million and 26 million respectively delivery the mean retrograde total sperm count was 4 fold higher for continuous 120 million versus interrupted 29 million delivery in the total ejaculate of the combined antegrade and retrograde fractions the mean total sperm count and mean total motile sperm were not significantly different for the 2 methods conclusions each method resulted in a similar mean total sperm count and total motile sperm in the total ejaculate but a higher proportion of sperm was found in the antegrade fraction using the interrupted method we recommend interrupted current delivery as the technique of choice when electroejaculation is performed to obtain sperm for fertilization electric stimulation infertility male semen spinal cord injuries testis continuous delivery of neurotrophin 3 by gene therapy has a neuroprotective effect in experimental models of diabetic and acrylamide neuropathies neurotrophic factors nfs are promising agents for the treatment of peripheral neuropathies such as diabetic neuropathy however the value of treatment with recombinant nf is limited by the short half lives of these molecules which reduces efficiency and by their potential toxicity we explored the use of intramuscular injection of a recombinant adenovirus encoding nt 3 adnt 3 to deliver sustained low doses of nt 3 we assessed its effect in two rat models streptozotocin stz induced diabetes a model of early diabetic neuropathy characterized by demyelination and acrylamide experimental neuropathy a model of diffuse axonal neuropathy which like late onset human diabetic neuropathy results in a diffuse sensorimotor neuropathy with dysautonomy treatment o f stz diabetic rats with adnt 3 partially prevented the slowing of motor and sensory nerve conduction velocities p 0 01 and p 0 0001 respectively treatment with adnt 3 of acrylamide intoxicated rats prevented the slowing of motor and nerve conduction velocities p 0 001 and p 0 0001 respectively and the decrease in amplitude of compound muscle potentials p 0 0001 an index of denervation acrylamide intoxicated rats treated with nt 3 had higher than control levels of muscle choline acetyltransferase activity p 0 05 suggesting greater muscle innervation in addition treatment of acrylamide intoxicated rats with adnt 3 significantly improved behavioral test results treatment with adnt 3 was well tolerated with minimal muscle inflammation and no detectable general side effects therefore our results suggest that nt 3 delivery by adenovirus based gene therapy is a promising strategy for the prevention of both early diabetic neuropathy and axonal neuropathies especially late axonal diabetic neuropathy continuously infused intrathecal baclofen over 12 months for spastic hypertonia in adolescents and adults with cerebral palsy objective to determine if the continuous intrathecal delivery of baclofen will control spastic hypertonia caused by long standing cerebral palsy cp design case series setting tertiary care outpatient and inpatient rehabilitation center directly attached to a university hospital patients thirteen cp patients average age 25 yr range 13 43 yr with intractable spastic hypertonia and quadriparesis one of whom had predominate diplegia who had not responded to oral medications including baclofen intervention patients were screened via a bolus injection of haclofen intrathecally those who dropped an average of 2 points on their lower extremity le ashworth scores were offered computer controlled pump implantation for 12 months of continuous delivery of intrathecal baclofen itb main outcome measures ashworth rigidity scores spasm scores and deep tendon reflex scores were collected for both the upper extremities ues and les differences over time were assessed via descriptive statistics and wilcoxon s signed rank test results after 1 year of continuous itb treatment the average le ashworth score ± standard deviation decreased from 3 4 ± 1 2 to 1 5 ± 0 7 p 0001 spasm score from 1 4 ± 1 6 to 0 6 ± 1 2 p 1024 and reflex score from 2 5 ± 1 2 to 0 7 ± 1 1 p 0001 the average ue ashworth score decreased from 3 0 ± 1 2 to 1 7 ± 1 0 p 0001 spasm score from 1 2 ± 1 6 to 0 2 ± 0 6 p 0135 and reflex score from 2 3 ± 0 7 to 0 5 ± 0 9 p 0001 the average itb dose required to attain these effects at 1 year was 263 ± 91 μg continuously infused per day conclusion continuously infused itb can reduce spastic hypertonia in the ues and les associated with long standing cp this reduction in tone will allow more freedom of movement and the potential for improved function adolescents adults baclofen cerebral palsy dystonia muscle hypertonia muscle spasticity rehabilitation chronic 60 week toxicity study of duros leuprolide implants in dogs the toxicity and pharmacodynamics of leuprolide acetate delivered from subcutaneously implanted duros leuprolide implants were examined in sexually mature male beagle dogs the duros leuprolide implant is a sterile nonpyrogenic nonerodible single use implantable osmotically driven drug delivery system for the palliative treatment of advanced prostate cancer it contains 65 mg of leuprolide and is designed to deliver leuprolide continuously at a nominal rate of 120 μg per day for at least 12 months serum drug and testosterone concentrations were compared to values from dogs receiving monthly intramuscular injections of lupron depot 3 75 mg or no treatment sham operated the local tissue response induced by subcutaneously implanted duros leuprolide implants was also evaluated beagles were implanted with a duros leuprolide implant for 52 weeks followed by removal and implantation of a new duros leuprolide implant for an additional 8 weeks no mortality or morbidity occurred in this study no treatment related changes occurred in mean body weights blood chemistry or hematology during the study treatment related atrophy of the testes epididymides and prostate gland consistent with the known pharmacological effects of the drug was observed in all dogs receiving the duros leuprolide implant or the lupron depot histological examination of these organs showed no distinguishable difference between dogs treated with the duros leuprolide implant or lupron depot weekly serum samples from dogs with duros leuprolidemplants indicated continuous leuprolide delivery over 60 weeks while some samples from the lupron depot group fell below measurable concentrations analysis of serum samples collected every 28 days just before lupron depot injection showed that 80 of these samples had leuprolide concentrations below the limit of quantitation 0 1 ng ml serum testosterone concentrations were below castrate levels 50 ng dl by 4 weeks after implantation of duros leuprolide implant and remained so for the duration of the study lupron depot 3 75 mg also effectively lowered serum testosterone concentrations but required reinjection every 28 days all local tissue reactions to the duros leuprolide implant at implant sites were classified as mild following macroscopic examination microscopic site scores were mild to moderate the duros leuprolide implant was shown to be safe to provide continuous leuprolide delivery and to effectively lower serum testosterone concentrations below castrate levels continuous delivery leuprolide toxicity effect of duration of infusion of stress like concentrations of cortisol on follicular development and the preovulatory surge of lh in sheep stress like levels of cortisol suppress follicular growth and development and block or delay the preovulatory surge of lh when cortisol is continuously administered during the late luteal and early follicular phases of the ovine oestrous cycle we postulated that cortisol infusion of shorter duration would have a similar effect to test this hypothesis the oestrous cycles of mature ewes were synchronized using progestin treated vaginal pessaries ewes were randomly assigned to one of four treatment groups animals received cortisol 0 1 mg kg h n 8 or vehicle alone n 8 beginning 5 days before and continuing for 5 days after pessary removal pr additional groups received cortisol only during the 5 days period before n 7 or the 5 days period after n 8 pr continuous delivery of cortisol established stable serum concentrations of cortisol of 72 0 ± 2 5 ng ml within 6 h of initiation of infusion serum concentrations of oestradiol increased progressively during the period after pr in control animals receiving vehicle alone and the preovulatory surge of lh was evident in all control animals eight of eight 55 5 ± 5 0 h after pr in contrast follicular development and the preovulatory surge of lh were evident during the period of cortisol infusion in only one of eight animals receiving stress like levels of cortisol over the entire 10 day infusion period similarly neither follicular development nor surge like secretion of lh were evident during the infusion period in animals zero of eight receiving cortisol during the 5 day period after pr this cortisol dependent suppression of ovarian activity in sheep receiving stress like levels of cortisol during the 5 days after pr was temporary and follicular development the ovulatory surge of lh and subsequent luteal function were evident in six of eight ewes after cessation of cortisol delivery similarly follicular development and the preovulatory surge of lh were noted within 5 days after pr in four of seven ewes receiving cortisol only during the 5 day period prior to pr collectively these data indicate that stress like levels of cortisol reduce fertility of sheep by suppressing follicular development and the preovulatory surge of lh additionally cortisol delivery during the follicular phase has a more profound suppressive effect on follicular development than cortisol administration during the luteal phase 2000 elsevier science b v cortisol follicular development lh surge oestradiol sheep stress in vitro and in vivo evaluation of a somatostatin analogue released from plga microspheres the purpose of this study was to design poly lactide co glycolide plga microspheres for the continuous delivery of the somatostatin analogue vapreotide over 2 4 weeks the microspheres were produced by spray drying and the desired characteristics i e high encapsulation efficiency and controlled release over 2 4 weeks achieved through optimizing the type of polymer processing solvent and co encapsulated additive the in vitro release was tested in fetal bovine serum preserved with 0 02 of thiomersal furthermore formulations were injected intramuscularly into rats to obtain pharmacokinetic profiles encapsulation efficiency was between 34 and 91 depending on the particular formulation the initial peptide release within 6 h was lowest i e 20 when acetic acid was used as processing solvent and highest i e 57 with dichloromethane the various co encapsulated additives generally lowered the encapsulation efficiency by 15 30 the best formulation in terms of low burst and effective drug serum levels 1 ng ml over 21 28 days in rats was the one made with end group uncapped plga 50 50 the solvent acetic acid and the additive polyethyleneglycol in conclusion the optimization of formulation parameters allowed us to produce vapreotide loaded plga microspheres of suitable characteristics for therapeutic use copyright c 2000 elsevier science b v microspheres pla plga plasma levels release kinetics somatostatin analogue restoration of cognitive and motor functions by ciliary neurotrophic factor in a primate model of huntington s disease huntington s disease hd is an inherited disorder characterized by cognitive impairments motor deficits and progressive dementia these symptoms result from progressive neurodegenerative changes mainly affecting the neostriatum this pathology is fatal in 10 to 20 years and there is currently no treatment for hd early in the course of the disease initial clinical manifestations are due to striatal neuronal dysfunction which is later followed by massive neuronal death a major therapeutic objective is therefore to reverse striatal dysfunction prior to cell death using a primate model reproducing the clinical features and the progressive neuronal degeneration typical of hd we tested the therapeutic effects of direct intrastriatal infusion of ciliary neurotrophic factor cntf to achieve a continuous delivery of cntf over the full period of evaluation we took advantage of the macroencapsulation technique baby hamster kidney bhk cells previously engineered to produce human cntf were encapsulated into semipermeable membranes and implanted bilaterally into striata we show here that intracerebral delivery of low doses of cntf at the onset of symptoms not only protects neurons from degeneration but also restores neostriatal functions cntf treated primates recovered in particular cognitive and motor functions dependent on the anatomofunctional integrity of frontostriatal pathways that were distinctively altered in this hd model these results support the hypothesis that cntf infusion into the striatum of hd patients not only could block the degeneration of neurons but also alleviated motor and cognitive symptoms associated with persistent neuronal dysfunction continuous erythropoietin delivery by muscle targeted gene transfer using in vivo electroporation it has been demonstrated that gene transfer by in vivo electroporation of mouse muscle increases the level of gene expression by more than 100 fold over simple plasmid dna injection we tested continuous rat erythropoietin epo delivery by this method in normal rats using plasmid dna expressing rat epo pcaggs epo as the vector a pair of electrodes was inserted into the thigh muscles of rat hind limbs and 100 μg of pcaggs epo was injected between the electrodes eight 100 v 50 msec electric pulses were delivered through the electrodes each rat was injected with a total of 400 μg of pcaggs epo which was delivered to the medial and lateral sides of each thigh the presence of vector derived epo mrna at the dna injection site was confirmed by rt pcr the serum epo levels peaked at 122 2 ± 33 0 mu ml on day 7 and gradually decreased to 35 9 ± 18 2 mu ml on day 32 the hematocrit levels increased continuously from the preinjection level of 49 5 ± 1 1 to 67 8 ± 2 2 on day 32 p 0 001 in pcaggs epo treated rats endogenous epo secretion was downregulated on day 32 in a control experiment intramuscular injection of pcaggs epo without subsequent electroporation did not significantly enhance the serum epo levels these results demonstrate that muscle targeted pcaggs epo transfer by in vivo electroporation is a useful procedure for the continuous delivery of epo word repetitions in japanese spontaneous speech this paper examines several hypotheses based on a strategic view of word repetitions in english we test whether these hypotheses also apply to japanese with its fundamentally different syntax analyses of 10 task oriented japanese dialogues reveal two effects first pauses are more frequent before and just after a word at a suspension of the speech than after a repetition of that word second the first token of the repeated word is abnormally prolonged these results support the strategic view of repetitions speakers often suspend speaking after making a preliminary commitment to a constituent but they prefer to produce that constituent with a continuous delivery these findings suggest the generality of these strategies across languages extreme programming extreme programming xp is a lightweight design method developed by kent beck ward cunningham and others after notable successes xp has been generating huge interest and no small amount of controversy much of the interest stems from xp s pragmatic approach to development key practices include pair programming writing tests upfront frequent refactoring and rebuild continuous integration and testing key principles incremental and iterative development working with the simplest solution cutting out extraneous documentation and collective code ownership continuous delivery of venous 5 fluorpuracil and arterial 5 fluorodeoxyuridine for hepatic métastases from colorectal cancer feasibility and tolerance in a randomized phase ii trial comparing flat versus chronomodulated infusion high dose chemotherapy combining regional hepatic artery infusion hai of fluorodeoxyuridine hai fudr and systemic venous infusion of 5 fluorouracil i v 5 fu was delivered against liver métastases from colorectal cancer the hypothesis that chronomodulation of delivery rate along the 24 h time scale would improve the tolerable doses of both drugs was tested combined hai fudr 80 mg m 2 day and i v 5 fu 1200 mg m 2 day were administered for five consecutive days every 3 weeks either as a constant rate infusion schedule a 27 patients or as chronotherapy schedule b 29 patients this latter regimen consisted of a sinusoidal modulation of the delivery rate over the 24 h scale with a maximum at 16 00 for fudr and 4 00 for 5 fu intrapatient dose escalation up to the individual maximum tolerated doses mtd was planned for both drugs in the absence of any previous grade 3 or 4 toxicity all patients had metastatic colorectal cancer with adjuvant or palliative chemotherapy given to six patients 22 on schedule a and 12 patients on schedule b 41 severe stomatitis occurred in 71 of the patients and was dose limiting no hepatic toxicity was encountered dose reductions of 5 fu and or fudr were required for 17 of 27 patients on schedule a 63 as compared to 11 of 29 patients on schedule b 38 following reaching the individual mtd p 0 05 over the first six cycles patients on schedule b received higher doses mg m 2 cycle fudr 522±85 versus 499±50 p 0 004 and 5 fu 5393962 versus 5136±963 p 0 009 and higher dose intensities mg m 2 week fudr 164±46 versus 151±52 p 0 018 and 5 fu 1652±478 versus 1553±535 p 0 041 of both drugs than patients on schedule a as a result the number of courses with doses of 5 fu above 1200 mg m 2 day and or fudr above 110 mg m 2 day was larger in group b than in group a 5 fu a 67 of 268 25 versus b 133 of 321 41 and fudr a 86 of 268 32 versus b 155 of 321 48 p 0 001 objective responses were observed in 13 patients on schedule a 48 and 11 patients on schedule b 38 the results support the need for further exploration of chronotherapy of colorectal cancer liver métastases with combined arterial and venous fluoropyrimidine chemotherapy 1999 lippincott williams wilkins ambulatory medicine chronotherapy circadian rhythms colorectal cancer hepatic artery infusion liver métastases the effect of season and melatonin on gnrh induced lh secretion in oestradiol treated orchidectomized sheep the biphasic effect of oestradiol e 2 on gonadotrope responsiveness is clearly evident in orchidectomized sheep wethers receiving e 2 and hourly pulses of gnrh we hypothesized that the duration of e 2 induced reduction in gonadotrope responsiveness differed between the breeding november and anoestrous may seasons in sheep to test this hypothesis wethers n 6 group were infused i v with e 2 2 μg 50 kg per h and received hourly pulses of gnrh 200 ng 50 kg per pulse or saline in may or november the pattern of lh secretion during the 72 h infusion period was determined serum concentrations of lh did not differ with season in control wethers receiving vehicle alone similarly continuous infusion of e 2 resulted in a 3 fold reduction in serum lh irrespective of season this e 2 induced suppression of serum lh was reversed by concurrent episodic delivery of gnrh the interval between initiation of infusion and return of pretreatment concentrations of lh was taken as a measure of the duration of e 2 induced suppression of gonadotrope responsiveness the duration of this e 2 dependent response varied with season with suppression of gonadotrope responsiveness more prolonged p 0 05 in may 36 7 ± 2 9 h than in november 14 3 ± 1 1 h in a companion study we examined the effect of melatonin on the duration of e 2 induced suppression of gonadotrope responsiveness wethers received blank or melatonin containing implants in march sixty days after implant insertion mid may wethers received e 2 2 μg 50 kg per h and hourly pulses of gnrh 200 ng 50 kg per pulse or saline for 72 h continuous delivery of e 2 alone resulted in a 3 fold decrease in serum concentrations of lh in both control and melatonin treated wethers the duration of e 2 induced suppression of gonadotrope responsiveness in animals receiving e 2 and gnrh was extended p 0 05 in wethers with blank implants 48 0 ± 0 7 h relative to the duration of suppression in melatonin treated wethers 14 5 ± 1 0 h taken together these data indicate that e 2 induced suppression of gonadotrope responsiveness is more extended during the anoestrous season however this seasonal effect can be reversed by continuous administration of melatonin multicentre double blind controlled clinical study on the efficacy of diclofenac epolamine tissugel plaster in patients with epicondylitis we tested the efficacy and tolerability of the new topical nsaid continuous delivery system diclofenac epolamine tissugel plaster dhep plaster in the treatment of humero radial epicondyl pains of a strictly tendinopathic nature in a multicentre double blind study a total of 85 patients 44 treated with dhep and 41 with placebo were observed during the 14 day treatment period and the 14 day treatment free follow up period all the parameters evaluated spontaneous pain pain on pressure pain on muscular testing showed a favourable evolution in both groups but there was a significantly better trend and longer lasting effects in the dhep treated group compared with the placebo group in addition tolerability of the active principle can be qualified as remarkable diclofenac epolamine epicondylitis nsaid local therapy adenoviral gene transfer of ciliary neurotrophic factor and brain derived neurotrophic factor leads to long term survival of axotomized motor neurons the neurotrophic factors ciliary neurotrophic factor and brain derived neurotrophic factor can prevent motor neuron cell death during development and after nerve lesion in neonatal rodents however local and systemic application of these factors to newborn rats with damaged motor nerves rescues motor neurons only transiently during the first two weeks after axotomy in order to test the effect of continuous delivery of these factors the effect of localized injection of cntf or bdnf transducing recombinant adenoviruses into the lesioned nerves was investigated under such conditions survival of axotomized motor neurons is maintained for at least 5 weeks this way of delivery corresponds to the physiological situation in adult rodents under which endogenous cntf is present in the cytosol of schwann cells and bdnf expression is upregulated after nerve lesion making these factors available to the damaged motor neurons recent results show that overexpression of muscle derived neurotrophin 3 prevents degeneration of axons and motor endplates but has only little effect on the number of motor neuron cell bodies in a murine animal model of motor neuron disease therefore techniques suitable for tonic exposure to both nerve and muscle derived neurotrophic factors may have implications for the design of future therapeutic strategies against human motor neuron disease superoxide release by human polymorphonuclear leukocytes in the presence of deferoxamine background and objective anecdotal reports in patients with acute and chronic iron overload have recently indicated that the efficacy and safety of an alternative chelation program including intravenous and or continuous delivery of deferoxamine dfo may be in contrast with the risk of developing lung injury production of oxygen radicals has been postulated to be an important mechanism by which polymorphonuclear leukocytes pmns could cause tissue injury in patients undergoing this alternative treatment method methods pmns obtained from healthy donors were incubated at 37°c for 30 min with dfo across the drug concentration 0 125 to 10 mg ml superoxide o 2 production was measured by superoxide inhibitable cytochrome c reduction as well as by an nbt densitometric kinetic test in the same run the effect of lipid peroxidation was demonstrated by means of a malonyldialdehyde mda assay results preincubation of pmns with any study concentration of dfo significantly enhanced o 2 release as well as mda production upon pma stimulation maximal intracellular and extracellular o 2 release as well as mda production occurred at certain drug concentrations interpretation and conclusions our in vitro findings suggest that o 2 release may be an additional detrimental contribution to tissue injury in some patients who develop pulmonary toxic effects while on intravenous and or continuous dfo administration cytochrome c reduction nbt reactive oxygen species superoxide ion noncoherent hybrid acquisition of ds cdma signals a noncoherent signal acquisition scheme using hybrid active correlation for a direct sequence code division multiple access ds cdma receiver is presented a test based on sequential hypothesis testing called the m ary sequential probability ratio test msprt and several fixed sample size fss tests are considered for the testing stage continuous integration of in phase and quadrature signal components is used to minimize noncoherent combining losses the msprt is adapted accordingly to accommodate dependent samples from the integrator output results of analysis and simulation show that the msprt provides significantly better performance than the fss schemes it has been found that use of the msprt with continuous integration results in negligible noncoherent implementation loss compared to coherent acquisition when a moderate number of correlators are used a new technique for continuous injection into stems of field grown corn plants chronic addition of nutrients metabolites growth regulators or toxins to intact plants is a useful way to study numerous aspects of plant physiology however continuous delivery of large amounts of test solutions into plant tissues has been difficult stem infusion methods have proven less destructive and more effective than other methods such as leaf and root feeding for supplying nutrients and other materials to developing plants infusion into solid stemmed plants has been limited by short delivery periods and the damage at the delivery site a held experiment was conducted to evaluate a new technique for supplying sucrose solutions or water to corn zea mays l stems this injection technique delivered pressurized solutions or water through syringe needles sealed to the stem with latex the pressure was applied to the syringe plunger with ceramic construction bricks solutions containing sucrose at 0 150 and 300 g l 1 were injected over a 32 d period the average solution uptake rate was 5 1 ml d 1 plant 1 distilled water was more easily delivered than sucrose solutions no difference in uptake rates were observed between 150 and 300 g l 1 sucrose solutions this injection system is a simple efficient and inexpensive method that can be used easily in the field or greenhouse development of a ceramic device for continuous delivery of acetylsalicylic acid the development of biodegradable implantable ceramic device for continuous delivery of effective amounts of aspirin for seven to ten days could alleviate the risk of injury to both the animal and laboratory veterinary personnel experiments conducted to data have shown that aspirin can be delivered in effective amounts by a ceramic matrix device in rats for six days thus additional variables had to be tested in an attempt to slow the rate of release the variable tested in this paper are compression load at which the ceramic matrix is pressed sintering the ceramic the ceramic to aspirin ratio incorporation of vitamin e oil in the ceramic and insertion of a compressed aspirin pellet in the ceramic matrix results of the experiments are presented effectiveness of esd training using multimedia opinions are formulated based on the information collected by the ussm united states semiconductor manufacturing training and development group at digital equipment corporation regarding the implementation of an esd training program using multimedia technology it is based in particular on observation pre post tests and interviews with a cross section of disciplines within the manufacturing population of the company the feedbacks gathered assure that the multimedia is a state of the art training approach that will not only enhance learning but also replace some of the classroom training programs it is further believed that courses requiring continuous delivery will be more consistent when multimedia is used multimedia lends itself very well to verification of subject matter and timely delivery across multiple shifts membrane with controllable permeability for drugs membranes find application in medicine for the continuous delivery of drugs the control of the delivered amount would be of great advantage therefore a composite membrane comprising a conductive polymer as an active separation layer is developed by applying various potentials to the membrane system a controlled permeability through the membrane is achieved depending on the redox state of the conducting polymer different rates of permeation for water soluble substances are observed dopamine a neurotransmitter is chosen for testing this new composite membrane the amount of permeated dopamine is detected on line with a thermospray mass spectrometer changes in the rate of permeation up to 40 are attained 1995 fluid delivery from infusion pump syringes fluid delivery rates of five small volume infusion pump syringes were compared the study consisted of a comparison of the infusion pump syringes in their respective infusion pumps 1 set for continuous delivery at 1 ml hr 2 set for continuous delivery at 3 ml hr and 3 set to deliver 1 ml bolus volumes during continuous delivery at 4 ml hr the lifecare prefilled 30 ml syringe abbott the dbl 30 ml syringe no 770205 dbl inc and the pump jet 30 ml syringe no 1931 international medication systems were tested in the lifecare pca plus ii infusion pump no 4100 abbott the 30 ml pump jet syringe no 1911 international medication systems and the dbl 30 ml syringe no 709700 dbl inc were tested in the stratofuse pca infusion pump baxter the infusion pumps were set to deliver fluid continuously at 1 ml hr for 30 hours and the solutions were collected separately and weighed the procedure was repeated with the infusion rate set at 3 ml hr for 10 hours for the third part of the study each syringe was tested to deliver 1 ml boluses with 0 5 15 and 25 ml removed from the syringe the solutions were collected and weighed before and after each bolus was delivered the volume of solution collected was calculated by using the specific gravity of the solution the syringes delivered significantly different volumes during the first hour of infusion at both the 1 and 3 ml hr rates differences also existed across time for most of the syringes bolus volumes varied greatly after infusion of 0 or 5 ml of fluid but were acceptable for the remainder of the infusions significant differences were also observed between the pumps for all tests the five infusion pump syringes tested did not deliver fluid accurately or consistently for 30 hours at 1 ml hr or 10 hours at 3 ml hr the syringes delivery of intermittent 1 ml bolus volumes during continuous delivery at 4 ml hr depended on the amount of fluid already delivered and the infusion pump used devices drug administration rate injections syringes volume noncoherent sequential acquisition of pn sequences for ds ss communications with without channel fading in this paper we study sequential acquisition schemes of m sequences based on the sequential probability ratio test sprt and a truncated sprt tsprt with noncoherent demodulation most reported results on sequential acquisition schemes assume that independent samples are available for the decision process the assumption of independent and identically distributed i i d samples requires the integrator in the receiver to be reset periodically this introduces loss in the effective signal to noise ratio degrading the performance in this paper on the contrary our two sequential schemes use continuous integration it can be shown that the likelihood ratio is monotonic consequently the tests can be easily implemented in real time methods are proposed for designing the decision thresholds to achieve the desired false alarm and miss probabilities performances of the proposed schemes are obtained and they suggest that the tsprt is more desirable the effect of slowly varying channel fading is also investigated results show that fading does not affect the false alarm probabilities but it can drastically reduce the probability of detecting the alignment of the two pn sequences especially when the fading is severe 1995 ieee continuous delivery of azidothyimidine by hydroxyapatite or tricalcium phosphate ceramics a ceramic drug delivery system was developed for continuous release of azidothymidine azt tricalcium phosphate tcp and hydroxyapatite ha ceramics were used to fabricate ceramic devices each group of devices consisted of three replicates ceramic and azt mixtures were compressed at a load of 3000 lbs in a 10 mm diameter die using a hydraulic press for in vitro studies vitamin e oil was incorporated in six different ratios in devices containing ha 500mg and azt 250mg each device was suspended in phosphate buffered saline ph7 4 devices containing oil released azt in significantly lower amounts than devices containing no oil in vivo studies were conducted with devices composed of homogenous mixtures of 250mg azt and 500mg tcp with and without vitamin e oil in the following combinations azt and tcp group i oil saturated tcp and azt group ii and azt pellet inserted in an oil saturated tcp shell group iii these devices were implanted subcutaneously in sprague dawley rats for two weeks group ii and iii devices delivered azt at a significantly lower rate than group i devices results of both studies suggest that treatment of ceramic devices with oil decreases the release rate and prolongs the delivery of azt azidothymidine calcium phosphates ceramic drug delivery hydroxyapatite implant matrix sustained release the spinal loop dialysis catheter characterization of use in the unanesthetized rat to permit long term measurement of time dependent changes in levels of dialyzable drugs and transmitters in the spinal intrathecal i t space of the unanesthetized rat we developed a dialysis catheter for chronic placement this was accomplished by constructing a loop probe 9 cm in length from 0 3 mm diameter dialysis tubing that was made impermeable except for the distal loop this loop catheter was readily inserted though an incision in the cisternal membrane and passed to the lumbar enlargement the ends of the catheter were then externalized on the top of the head to permit i t injections an additional i t catheter could also be inserted simultaneously by the same route for dialysis an external end of the loop catheter was connected to a syringe pump and perfused with artificial csf 10 μl min and the out flow collected a series of studies were performed to demonstrate the characteristics and utility of this technique 1 stability of resting release glutamate and glucose concentrations in spinal dialysate showed no significant changes from 3 to 10 days after implantation 2 spinal cord ischemia ischemia induced by aortic occlusion or cardiac arrest evoked a time dependent increase in retrieved glutamate 3 spinal cord compression caused a time dependent glutamate aspartate and pge 2 increase 4 noxious afferent stimulation induced by the injection of formalin into the hindpaw resulted in a rapid and transient increase in dialysate glutamate concentration 5 direct activation of spinal excitatory amino acids receptors by i t injection of kainic acid 1 μg evoked a significant increase in aspartate and taurine 6 continuous delivery of spinal opiate alfentanil via dialysis resulted in a maintained concentration dependent elevation in the thermal escape latencies in the unanesthetized rat the loop dialysis catheter provides a robust experimental tool for studying time dependent changes in the concentration of diffusible substances in spinal csf over an extended post implantation interval and allows comparison of these changes with concurrently assessed behavioral indices 1995 amino acid release formalin test intrathecal dialysis intrathecal kainic acid spinal cord spinal ischemia spinal trauma polymer encapsulated cell lines genetically engineered to release ciliary neurotrophic factor can slow down progressive motor neuronopathy in the mouse ciliary neurotrophic factor cntf has recently generated great interest due to its potential as a therapeutic agent for the treatment of human neurodegenerative diseases such as amyotrophic lateral sclerosis als because the systemic half‐life of cntf is only in the order of a few minutes continuous delivery of this trophic factor could be attractive or even necessary in the therapy of these diseases one promising technique involves the polymer encapsulation of cells which have been genetically modified to secrete neurotrophic factors the polymer capsules can be implanted into animals and effect the slow release of the protein for several months the encapsulation technique immuno‐isolates the foreign cells from host immune cells and at the same time prevents tumour formation by the transplanted cells in this study we have used progressive motoneuronopathy pmn mice to determine the extent to which encapsulated cell lines secreting cntf could alter the course of the disease pmn pmn homozygotes present severe loss of myelinated motor fibres and a significant reduction of facial motoneuron cell bodies the mice develop weakness of the hindlimbs and die during the sixth week after birth we found that cntf delayed the disease progression by increasing the survival time by 40 and by improving motor function as assessed by three behavioural tests moreover histological counts of the phrenic nerve myelinated axons and facial nucleus motoneurons indicated a significant reduction of motoneuron loss these results suggest that polymer‐encapsulated cells releasing neurotrophic factors may provide a potential delivery system for treating neurodegenerative diseases such as als copyright 1995 wiley blackwell all rights reserved cntf motoneuron disease model neurodegenerative disease pmn mice continuous delivery of ions or drugs into single cells by a diffusional microburet the diffusional microburet dmb is a pulled glass capillary with a diffusion membrane at its tip 1 it is a useful tool to analyze ultramicro chemical samples and to study intracellular activities in single cells in this work a numerical simulation of a complete transport model for the dmb cell interaction is obtained a simplified analytical solution is also derived by assuming a uniform intracellular reagent drug distribution and a quasi steady state diffusion in the dmb the simulation results compare well with the simplified analytical solution more complicated cellular membrane transports are also discussed neointimal proliferation in canine coronary arteries a model of restenosis permitting local and continuous drug delivery background a number of experimental preparations have been used to elucidate the pathophysiology of restenosis after percutaneous transluminal coronary angioplasty however few models have been advanced that address restenosis in coronary arteries and none provides an effective means of continuous local drug delivery in this report we describe a model of restenosis in coronary arteries with the provision for local continuous delivery of cytotoxic and or antiproliferative agents experimental design an ameroid constrictor was placed on the left circumflex coronary artery of 17 normocholesterolemic dogs one month later after substantial collateral development had ensued a segment of the left circumflex coronary artery distal to the ameroid was mechanically compressed using surgical forceps for 10 n 4 15 n 4 20 n 2 or 30 minutes n 5 in two dogs an indwelling left circumflex catheter and implanted pump maintained a continuous infusion of saline at the injury site in addition the pump side port provided transcutaneous access for serial selective coronary arteriography the animals were maintained on a normal diet without cholesterol or fat supplementation results three weeks after vascular injury significant neointimal proliferation was observed in all dogs that was morphologically similar to the proliferation seen after percutaneous transluminal coronary angioplasty in human coronary arteries the extent of neointimal formation was linearity related to the duration of injury neointimal medial area ratios were 0 35 ± 0 10 0 46 ± 0 10 0 58 ± 0 03 and 1 16 ± 0 26 mean ± se after 10 15 20 and 30 minutes of mechanical compression injury respectively conclusions this model produces striking neointimal proliferation in the coronary arteries of normocholesterolemic dogs morphologically similar to that seen in human coronary restenosis specimens the model appears suitable to test the efficacy of agents with the potential to inhibit neointimal formation providing continuous intracoronary drug delivery as well as transcutaneous access for serial selective arteriography coronary circulation restenosis smooth muscle cells vascular disease continuous delivery of drugs by infusion pump presentation of an infusion rate computing programme intensive therapy includes continuous delivery of drugs with infusion pump this article describes an infusion rate computing programme using a macintosh micro computer and the hyper card 2 software the latter permits the classification of data onto cards collected in stacks the card that is displayed when the programme is launched shows fifteen buttons assigned to various drugs click upon a button opens the corresponding card which displays the presentation of the drug and suggests a dilution the next step associates input of patient s weight into the appropriate field and a click on the main button a display then indicates for various standard dosages the corresponding rate of infusion in ml · h 1 and the daily consumption of the drug in order to avoid causal or voluntary changes of the setting soft securities have been inserted in the programme 1994 masson paris administration mode d autoadministration équipement seringue autopulsée informatique pharmacokinetics of levodopa and carbidopa in rats following different routes of administration this study examined the pharmacokinetics of levodopa and carbidopa in the rat after different modes of administration the drugs were given simultaneously by the intravenous intraarterial oral duodenal and intraperitoneal routes as single doses the ratio of levodopa to carbidopa given was always 4 1 two iv doses 5 and 15 mg kg of levodopa were given to test for nonlinearity three ip doses of levodopa were given 5 7 5 and 15 mg kg and the 15 mg kg dose was given in three volumes 2 4 and 20 ml kg one oral dose and two intraduodenal doses of 15 mg kg were given the drugs were dissolved in saline in one of the intraduodenal doses and suspended in 1 8 methylcellulose in the other the elimination of levodopa was nonlinear there was a comparatively high degree of interindividual variability in absorption with the oral route but this was substantially reduced when levodopa was given intraduodenally there was also much less variability with the intraperitoneal route compared to the oral and the degree of absorption was generally high there was a significantly higher extent and slower rate of absorption when levodopa was administered ip in a large volume of vehicle these results suggest that the oral route may not be the optimal method of delivering levodopa to patients who have a fluctuating response and that a continuous delivery system via the intraperitoneal or intraduodenal routes might be a better alternative 1994 plenum publishing corporation all rights reserved absorption carbidopa levodopa pharmacokinetics rat modular hard and software system for the analysis of cardiac signals application to real time pressure volume loops there is a growing need for semi real time measurement of ventricular function which can be quantified by the left ventricular pressure volume p v relationship in the catheterization labatory left ventricular volume is routinely derived from planimetry of contrast ventriculagrams ventriculography however does not lend itself to repeated assessment as even 40 60 cc of contrast injection can alter baseline ventricular variables non invasive techniques to quantify ventricular volumes combined with electrical pigtail pressure measurement would eliminate these limitations such real time volume measurements can be achieved by either doppler flow measurement or planimetry by 2 d echo is used we report on a modular system which enables continuous integration of these rapidly evolving techniques into the existing environment of the catheterization labatory for on line generation of p v loops gonadotrope responsiveness in orchidectomized sheep iii effect of estradiol withdrawal after continuous infusion the magnitude of gnrh induced 1600 ng hourly pulse for 24 h preovulatory surge like secretion of lh was assessed in orchidectomized sheep wethers during infusion of estradiol e 2 5 μg h in 10 ethanol saline vehicle or at 0 12 24 or 48 h after e 2 withdrawal n 6 wethers group in one group infusion of e 2 was continued for 48 h with concurrent circhoral delivery of gnrh during the final 24 h of the e 2 delivery period in other treatment groups infusion of e 2 or vehicle was continued for 24 h circhoral delivery of gnrh was initiated at the conclusion of e 2 delivery or 12 24 or 48 h thereafter or at 48 h after cessation of vehicle infusion total gnrh induced lh secretion in wethers receiving concurrent e 2 was significantly augmented relative to the gnrh induced secretory response in wethers receiving vehicle alone the magnitude of gnrh induced lh release was significantly reduced in animals in which e 2 delivery was halted at the beginning of the gnrh challenge period further reductions in gonadotrope responsiveness were noted 12 24 and 48 h after cessation of e 2 delivery responsiveness 48 h after halting the e 2 infusion did not differ p 0 05 from the responsiveness of wethers that had not been treated with e 2 in a companion study anterior pituitary tissue was collected at the end of e 2 infusion 5 μg h for 24 h or at 12 24 or 48 h thereafter n 6 wethers group anterior pituitary tissue of control animals n 6 was collected after halting the infusion of vehicle alone when compared with values in wethers receiving vehicle alone pituitary stores of lh and concentration of gnrh receptor were increased by infusion of e 2 the tissue concentration of gnrh receptor was significantly reduced 12 h after e 2 withdrawal and decreased to pretreatment levels 48 h after halting of the e 2 infusion in contrast tissue stores of lh remained elevated during the 48 h period after e 2 withdrawal neither e 2 infusion nor e 2 withdrawal affected the tissue concentration of mrna for the lhβ subunit however continuous delivery of e 2 decreased tissue stores of fsh and the concentration of fshβ mrna the tissue concentration of fsh remained depressed during the 48 h period after e 2 withdrawal but the tissue concentration of fshβ mrna was returned to the pretreatment level within 24 h of e 2 withdrawal these data indicate that the stimulatory effect of e 2 on gonadotrope responsiveness and gnrh receptor concentration and on e 2 induced inhibition of pituitary concentrations of fshβ mrna are short lived phenomena sustained augmentation of gonadotrope responsiveness and gnrh receptor concentration is likely to require continued estrogenic support measured savings from time or demand based temperature controls on service water heaters in apartment buildings multifamily buildings larger than 40 units commonly have a pump on the service hot water system than constantly circulates heated water through a piping loop such a system ensures continuous delivery of hot water but high piping losses decrease overall efficiency to investigate the strategy of reducing system temperatures during periods of low demand time based and demand based temperature controls were installed in three apartment buildings the purpose was to assess savings potential operation and tenant acceptability tests were conducted using an alternating mode design monitors by a computerized data acquisition system therapeutic systems and drug delivery 4 osmotic minipump for continuous delivery of substances to conscious experimental animals a miniature osmotic pump was developed these pumps can be implanted subcutaneously or intraperitoneally then with self powered force osmosis substance is released with constant rate over period 1 4 weeks model pending these osmotic minipumps are widely used for investigation of varioussubstances e g anesthetics anticancer agents catecholamines immunomodulators peptides steroids by the means of osmotic minipumps until now more than 300 substances were applied and over 4 000 papers were published on the use of osmotic minipumps in pharmacology toxicology oncology et a continuous intravesical drug delivery system for the rat we describe a self contained system for the continuous infusion of drugs into the rat urinary bladder a reversible model of hydronephrosis is used to prepare one renal unit for nephrostomy tube placement an 0 8 mm silastic nephrostomy tube is introduced into the hydronephrotic kidney via a 16 gauge angiocath the nephrostomy tube is then connected to an alzet mini osmotic pump which is implanted in a subcutaneous location the ability of this system to deliver a continuous dose of a test agent into the bladder was evaluated pumps were filled with a 1 solution of methylene blue in phosphate buffered saline following pump implantation urinary samples were collected on a daily basis and subsequently analyzed for their concentration of methylene blue at the completion of the experiment specimens of the kidney ureter and bladder were histologically examined results demonstrated an average of 102 recovery of the theoretically delivered dose over a 14 day period renal histology demonstrated chronic inflammatory changes at the site of nephrostomy tube placement no upper or lower tract urothelial changes were identified this model provides a system for the continuous delivery of drugs in the rat urinary tract and results in no histological alteration to the lower urinary tract drug therapy urinary bladder drug delivery systems 6 transdermal drug delivery transdermal drug delivery system has been in existence for a long time in the past the most commonly applied systems were topically applied creams and ointments for dermatological disorders the occurrence of systemic side effects with some of these formulations is indicative of absorption through the skin a number of drugs have been applied to the skin for systemic treatment in a broad sense the term transdermal delivery system includes all topically administered drug formulations intended to deliver the active ingredient into the general circulation transdermal therapeutic systems have been designed to provide controlled continuous delivery of drugs via the skin to the systemic circulation the relative impermeability of skin is well known and this is associated with its functions as a dual protective barrier against invasion by micro organisms and the prevention of the loss of physiologically essential substances such as water elucidation of factors that contribute to this impermeability has made the use of skin as a route for controlled systemic drug delivery possible basically four systems are available that allow for effective absorption of drugs across the skin the microsealed system is a partition controlled delivery system that contains a drug reservoir with a saturated suspension of drug in a water miscible solvent homogeneously dispersed in a silicone elastomer matrix a second system is the matrix diffusion controlled system the third and most widely used system for transdermal drug delivery is the membrane permeation controlled system a fourth system recently made available is the gradient charged system additionally advanced transdermal carriers include systems such as iontophoretic and sonophoretic systems thermosetting gels prodrugs and liposomes many drugs have been formulated in transdermal systems and others are being examined for the feasibility of their delivery in this manner e g nicotine antihistamines beta blockers calcium channel blockers non steroidal anti inflammatory drugs contraceptives anti arrhythmic drugs insulin antivirals hormones alpha interferon and cancer chemotherapeutic agents research also continues on various chemical penetration enhancers that may allow delivery of therapeutic substances for example penetration enhancers such as azone may allow delivery of larger sized molecules such as proteins and polypeptides a combination spectrophotometer for measuring electronic absorption natural circular dichroism and magnetic circular dichroism spectra the design construction and evaluation of a combination spectrometer for measuring electronic absorption ea natural circular dichroism cd and magnetic circular dichroism mcd are described around the optical components of a jasco ord uv 5 spectropolarimeter a new ea cd mcd instrument was built with the realized intentions of increasing sensitivity and upgrading the analog tube type circuitry to a solid state digitally computer controlled spectrophotometer it is a flexible dynamic and user controllable system interfaced to an apple ii plus computer for studying instrument and signal parameters the monochromator m photoelastic modulator pem photomultiplier tube applied voltage pmhv and photomultiplier tube dc output current pmdc are under complete and independent software control our system has two unique aspects for obtaining the circular dichroism first the ac signal is measured with a voltage to frequency v f converter and second both the ac and the dc are independently recorded and their ratio is digitally calculated this design has several advantages which include the elimination of voltage divider integrated circuits or division electronics a wide dynamic range a greater precision of ac values at low percentages of full scale and the capability of continuous integration over long time periods also both types of spectra ea and cd or mcd are obtained from the current output of the pm this paper not only describes the design of the instrument for obtaining the two types of spectra but also compares four methods of obtaining the circular dichroism sensitivities of ∼1×10 7 δa units are achievable as determined by measuring cd spectra of the well known enantiomer co en 3 3 suppression of spermatogenesis by means of continuous delivery of danazol in combination with dihydrotestosterone from alcap drug delivery devices the objectives of this investigation were to evaluate the release of danazol d and dihydrotestosterone tht from nonimpregnated and polylactic acid pla impregnated alcap ceramic reservoirs implanted in male rats and to study the effects of delivered androgens on the reproductive system of male rats a total of 120 sprague dawley male albino rats were distributed equally into three groups two alcap capsules one nonimpregnated and the other impregnated with polylactic acid pla were implanted into each rat in group i and ii capsules implanted into group i rats were loaded with a mixture of 20 mg d and 20 mg dht group ii rats were implanted with two empty capsules sham group and group iii animals served as unimplanted controls eight rats from each group were euthanized at the end of one three six nine and twelve months following the implantation of the ceramics no significant change in the weights of vital organs of rats were observed among any of the three groups vas deferens and epididymal fluid were devoid of normal spermatozoa within three months of implanting the steroid containing ceramics testicular and epididymal weights decreased significantly in the rats implanted with alcap containing steroid and the seminiferous tubules became oligospermic after one month and azoospermic after three months the levels of circulating testosterone luteinizing hormone and follicle stimulating hormone were suppressed in rats implanted with steroid containing ceramics data collected in this study suggest that 1 alcap ceramic capsules are capable of delivering d and dht continuously for 12 months 2 the amount of d and dht released by alcap capsules was sufficient to suppress spermatogenesis and induce azoospermia in rats mechanism of ethanol enhanced estradiol permeation across human skin in vivo the influence of ethanol on the permeation of 17β estradiol estradiol across viable human skin in vivo was investigated with the human skin sandwich flap model maintaining continuous delivery of a constant concentration of the solute in phosphate buffered saline ph 7 4 pbs or mixtures of ethanol in pbs to the skin surface revealed that steady state flux of estradiol was achieved within 30–60 min and maintained throughout 4 hr the 10 fold decrease in in vivo flux and permeability coefficient k p of tracer estradiol solutions in ethanol or ethanol solutions compared with pbs vehicle reflected the 10 fold difference in the apparent partition coefficients k m of estradiol from the respective vehicles into isolated human stratum corneum neither the stratum corneum thickness nor the diffusion coefficient of estradiol was significantly different among the vehicles tested in vivo flux of estradiol in ethanol or ethanol solutions across viable human skin was increased with saturated solutions of estradiol further in vivo flux of estradiol from vehicles such as pbs ethanol and ethanol mixtures which minimally alter the rate limiting barrier can be successfully predicted with knowledge of only two physicochemical parameters the estradiol concentration in the vehicle and the k m of estradiol from the vehicle into isolated human stratum corneum 1990 plenum publishing corporation all rights reserved estradiol ethanol human percutaneous absorption permeation enhancers skin design of a new multiple unit controlled release formulation of metoprolol metoprolol cr a new controlled release cr formulation of the β 1 selective adrenoceptor antagonist metoprolol 1 has been developed aiming at an even 24 h pharmacological effect in order to achieve this using a once daily dose factors such as absorption characteristics physicochemical properties and technological aspects had to be considered the new formulation called metoprolol cr is a disintegrating tablet consisting of several hundred coated pellets of metoprolol succinate each pellet being its own cr delivery unit in vitro testing and in vivo studies in healthy volunteers show that the new cr formulation gives continuous delivery of metoprolol throughout the day resulting in smooth plasma concentration profiles without peaks and troughs the release of the drug is independent of ph and other physiological variables such as food intake which do not seem to alter the biopharmaceutical properties of the formulation 1988 springer verlag controlled release formulation metoprolol pharmacokinetics plasma concentration profile noncoherent sequential acquisition of ds waveforms two different versions of multiple dwell noncoherent sequential acquisition of direct sequence waveforms are presented one involves recentering the test statistic after each dwell whereas the other consists of continuous integration over all dwells both of these sequential acquisition algorithms are shown to outperform their fixed dwell counterparts in addition the two and three dwell sequential acquisition schemes outperform single dwell sequential acquisition schemes by as much as 4 db bursting pressures of co inf 2 inf laser‐welded rabbit lleum in this study the short‐term bond strength of laser‐welded new zealand white rabbit ileum was examined forty‐eight longitudinally oriented 0 5‐cm transmural scalpel incisions were reanastomosed solely through the use of the co 2 continuous wave laser at low energy levels random power levels of 250 500 750 or 1 000 mw were delivered to weld sites by either continuous application for 30 seconds or a pulsating application ie 0 5 seconds on 0 5 seconds off for 60 seconds this provided 53 6 107 1 160 7 and 214 3 j cm 2 respectively with the aid of a plexiglass clamp and pressure monitored infusion system each type of weld was tested six times to determine the intraluminal hydrostatic pressure required to burst the welded seam 1 minute after completing the weld for the welds made with 107 1 160 7 and 214 3 j cm 2 in both lasing modes the mean bursting pressure was 40 7 mmhg sd ± 24 9 with no statistically significant difference in weld strengths at these energy densities there was also no difference between continuous and pulsating delivery methods however the energy density of 53 6 j cm 2 in either method produced a mean bursting pressure significantly lower than those produced by the higher energy densities tested and below the estimated basal ileal intraluminal pressure of approximately 9 mmhg since 107 1 j cm 2 energy density caused the minimal gross tissue changes while producing an equally strong bond and since continuous is faster than pulsating 500 mw of continuous delivery was considered the optimal setting for the co 2 laser welding of rabbit ileum copyright 1986 wiley‐liss inc a wiley company sutureless anastomosis tissue welding constant intrarenal infusion of pge1 into a canine renal transplant using a totally implantable pump a method was devised whereby pge1 could be administered to a canine renal transplant recipient on a chronic basis pge1 was stored in the reservoir of an implantable pump and delivered continuously in high doses directly into the renal transplant artery in the model studied a contralateral untreated transplant from the same donor served as a control sequential renal scans were used to study the effect of intraarterial pge1 on the rejection process continuous delivery of pge1 into the renal transplant artery did not prevent allograft failure under these conditions blood flow diminished similarly in both pge1 treated and untreated transplants there were however striking differences in the histologic appearance of treated and untreated transplants pge1 perfusion resulted in the appearance of large numbers of polymorphonuclear leukocytes but few lymphocytes in the untreated control allograft however the findings were typical of lymphocyte mediated acute rejection the distinctive differences noted histologically suggested that the local administration of pge1 influenced the mechanism by which graft failure occurred the ability to manipulate cell populations infiltrating an allograft represents a potentially important means for modifying the immune response 1984 by the williams wilkins co secretory rhythm of growth hormone regulates sexual differentiation of mouse liver the secretory pattern of growth hormone gh differs between the sexes in males it is more pulsatile than in females experiments were performed to test the hypothesis that differences in the secretory rhythm of gh are responsible for sex dependent liver functions of mice continuous delivery of gh was achieved either by introducing metallothionein gh fusion genes into the germ line or by implanting minipumps pulsatile delivery of gh was mimicked by injection the effects of these treatments on production of hepatic prolactin gh receptors albumin and major urinary protein mup were monitored the results suggest that induction of mup mrna requires pulsatile occupancy of gh receptors which is achieved naturally in males or by injection of gh whereas chronic occupancy of gh receptors is inhibitory in contrast induction of prolactin gh receptors requires chronic stimulation of gh receptors which is approximated in normal female mice or results from increased gh levels in mice with foreign genes or undergoing infusions from mini pumps 1984 similar metabolic effects of pulsatile versus continuous human insulin delivery during euglycemic hyperinsulinemic glucose clamp in normal man seven normal volunteers were studied on two different occasions during which 4 h pulsatile puls 08 mu·kg 1 ·min 1 7 5 min of 15 and continuous cont 0 4 mu·kg 1 ·min 1 intravenous i v infusions of human insulin actrapid hm novo were randomly compared a euglycemic glucose clamp was performed and a 3 3 h glucose infusion was used for determination of endogenous glucose production egp and metabolic clearance rate mcr of glucose plasma glucose was similar in both conditions plasma insulin was stable at about 29 mu l cont and fluctuated between 10 and 45 mu l mean 28 puls exogenous glucose infused was 1 137 ± 0 058 and 1 088 ± 0 099 g·kg 1 ·4 h 1 in cont and puls respectively ns egp was totally suppressed in both conditions glucose mcr increased similarly to a maximum of 6 71 ± 0 19 cont and 6 79 ± 0 59 puls ml·kg 1 ·min 1 during the fourth hour c peptide plasma levels remained stable whereas plasma glucagon free fatty acids and 3 hydroxybutyrate were similarly suppressed in both tests thus under these conditions pulsatile and continuous insulin infusions have similar metabolic effects these data contrast with those of matthews et al 1983 who reported that at lower plasma concentrations 5 19 mu l pulsatile insulin had greater hypoglycemic effect than did continuous delivery it is concluded that pulsatile insulin shows no greater activity under normoglycemic moderately hyperinsulinemic conditions in man pulsatile insulin has greater hypoglycemic effect than continuous delivery the relative hypoglycemic effect of pulsatile versus steadily infused insulin have been examined in six normal subjects in whom pancreatic insulin output was suppressed by somatostatin 14 soluble insulin was infused continuously overnight on one occasion and on another occasion the same quantity was given in pulses of 2 min duration with a gap of 11 min the mean plasma glucose concentrations were lower when pulsed insulin was given mean for the last hour 4 66 ± 0 08 mmol l ± sem versus 5 53 ± 0 06 mmol l ± sem for steady infusion diverging significantly p 0 05 paired t test 7 h after the start of the study the specific binding of 125 i a14 mono iodo insulin to monocytes was greater after pulsed insulin 2 9 with pulsed versus 2 4 with steadily infused insulin at tracer only point p 0 02 paired t test thus intravenous insulin has greater hypoglycemic effect when pulsed possibly mediated by greater insulin receptor binding a nitrogen powered continuous delivery all‐glass‐teflon pumping system for ground‐water sampling from below 10 meters a nitrogen powered all‐glass‐teflon continuous delivery noncontaminating pump system is described continuous flow rates up to 45 gal‐hr ‐1 2 84 1‐sec ‐1 are obtainable the system was developed and field tested to sample water wells with static heads greater than 32 ft 10 m for measurements of trace level organics the system is equally applicable to environments such as marine lake and other unconfined water or fluid systems also the analysis can be extended to inorganic and microbial assays in that the sample obtained is unaltered with respect to chemical physical and biological properties copyright 1980 wiley blackwell all rights reserved co dose meter for working places exposed to extreme peaks of co contamination for certain marginal conditions the product of carbonmonoxide concentration c and time t of exposure c t determines the amount of co load which affects subjects working in several working places the determination of c t is easy whenever c remains constant if c varies the following methods can be used 1 continuous integration of c over the time elapsed im 2 collection of aliquot quantities of gas samples during exposition and following multiplication of the mean concentration c with the time elapsed t sm advantages and disadvantages of both lastmentioned methods are discussed as consequence a 3 modified sampling method is introduced msm it combines essential advantages of the im and sm especially accuracy reliability in field tests and validity 1978 walter de gruyter all rights reserved