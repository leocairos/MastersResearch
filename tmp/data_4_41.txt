predicting change using software metrics a review 2015 ieee software change prediction deals with identifying the classes that are prone to changes during the early phases of software development life cycle prediction of change prone classes leads to higher quality maintainable software with low cost this study reports a systematic review of change prediction studies published in journals and conference proceedings this review will help researchers and practitioners to examine the previous studies from different viewpoints metrics data analysis techniques datasets and experimental results perspectives besides this the research questions formulated in the review allow us to identify gaps in the current technology the key findings of the review are i less use of method level metrics machine learning methods and commercial datasets ii inappropriate use of performance measures and statistical tests iii lack of use of feature reduction techniques iv lack of risk indicators used for identifying change prone classes and v inappropriate use of validation methods change prediction empirical validation machine learning software maintenance software metrics predicting software maintenance effort using neural networks 2015 ieee software maintenance is an important phase of software development lifecycle which starts once the software has been deployed at the customer s end a lot of maintenance effort is required to change the software after it is in operation therefore predicting the effort and cost associated with the maintenance activities such as correcting and fixing the defects has become one of the key issues that need to be analyzed for effective resource allocation and decision making in view of this issue we have developed a model based on text mining techniques using machine learning method namely radial basis function of neural network we apply text mining techniques to identify the relevant attributes from defect reports and relate these relevant attributes to software maintenance effort prediction the proposed model is validated using browser application package of android operating system receiver operating characteristics roc analysis is done to interpret the results obtained from model prediction by using the value of area under the curve auc sensitivity and a suitable threshold criterion known as the cut off point it is evident from the results that the performance of the model is dependent on the number of words considered for classification and therefore shows the best results with respect to top 100 words the performance is irrespective of the type of effort category defect reports machine learning receiver operating characteristics software maintenance effort prediction text mining dynamic metrics are superior than static metrics in maintainability prediction an empirical case study 2015 ieee software metrics help us to make meaningful estimates for software products and guide us in taking managerial and technical decisions like budget planning cost estimation quality assurance testing software debugging software performance optimization and optimal personnel task assignments many design metrics have proposed in literature to measure various constructs of object oriented oo paradigm such as class coupling cohesion inheritance information hiding and polymorphism and use them further in determining the various aspects of software quality however the use of conventional static metrics have found to be inadequate for modern oo software due to the presence of run time polymorphism templates class template methods dynamic binding and some code left unexecuted due to specific input conditions this gap gave a cue to focus on the use of dynamic metrics instead of traditional static metrics to capture the software characteristics and further deploy them for maintainability predictions as the dynamic metrics are more precise in capturing the execution behavior of the software system in the current empirical investigation with the use of open source code we validate and verify the superiority of dynamic metrics over static metrics four machine learning models are used for making the prediction model while training is performed simultaneously using static as well as dynamic metric suite the results are analyzed using prevalent prediction accuracy measures which indicate that predictive capability of dynamic metrics is more concise than static metrics irrespective of any machine learning prediction model results of this would be helpful to practitioners as they can use the dynamic metrics in maintainability prediction in order to achieve precise planning of resource allocation dynamic metrics machine learning software maintainability prediction software quality static metrics an exploratory study about the cross project defect prediction impact of using different classification algorithms and a measure of performance in building predictive models 2015 ieee predicting defects in software projects is a complex task especially in the initial phases of software development because there are a few available data the use of cross project defect prediction is indicated in such situation because it enables to reuse data of similar projects in order to find and group similar projects this paper proposes the construction of cross project prediction models using a measure of performance achieved through the application of classification algorithms to do so we studied the combined application of different algorithms of classification of feature selection and clustering data applied to 1270 projects aiming to building different cross project prediction models in this study we concluded that naive bayes algorithm obtained the best performance with 31 58 of satisfactory predictions in 19 models created with its use this proposal seems to be promise once the local predictions considered satisfactory reached 31 58 against 26 31 of global predictions cross project defect prediction models defect prediction models software maintenance software quality an implementation of just in time fault prone prediction technique using text classifier 2015 ieee since the fault prediction is an important technique to help allocating software maintenance effort much research on fault prediction has been proposed so far the goal of these studies is applying their prediction technique to actual software development in this paper we implemented a prototype fault prone module prediction tool using a text filtering based technique named fault prone filtering our tool aims to show the result of fault prediction for each change i e commits as a probability that a source code file to be faulty the result is shown on a web page and easy to track the histories of prediction a case study performed on three open source projects shows that our tool could detect 90 percent of the actual fault modules i e the recall of 0 9 with the accuracy of 0 67 and the precision of 0 63 on average fault prediction machine learning mining software repository software development support tool software maintenance spam filter prediction approach of software fault proneness based on hybrid artificial neural network and quantum particle swarm optimization 2015 published by elsevier b v the identification of a module s fault proneness is very important for minimizing cost and improving the effectiveness of the software development process how to obtain the correlation between software metrics and module s fault proneness has been the focus of much research this paper presents the application of hybrid artificial neural network ann and quantum particle swarm optimization qpso in software fault proneness prediction ann is used for classifying software modules into fault proneness or non fault proneness categories and qpso is applied for reducing dimensionality the experiment results show that the proposed prediction approach can establish the correlation between software metrics and modules fault proneness and is very simple because its implementation requires neither extra cost nor expert s knowledge proposed prediction approach can provide the potential software modules with fault proneness to software developers so developers only need to focus on these software modules which may minimize effort and cost of software maintenance ann fault prone prediction qpso software metrics mining system logs to learn error predictors a case study of a telemetry system 2014 springer science business media new york predicting system failures can be of great benefit to managers that get a better command over system performance data that systems generate in the form of logs is a valuable source of information to predict system reliability as such there is an increasing demand of tools to mine logs and provide accurate predictions however interpreting information in logs poses some challenges this study discusses how to effectively mining sequences of logs and provide correct predictions the approach integrates different machine learning techniques to control for data brittleness provide accuracy of model selection and validation and increase robustness of classification results we apply the proposed approach to log sequences of 25 different applications of a software system for telemetry and performance of cars on this system we discuss the ability of three well known support vector machines multilayer perceptron radial basis function and linear kernels to fit and predict defective log sequences our results show that a good analysis strategy provides stable accurate predictions such strategy must at least require high fitting ability of models used for prediction we demonstrate that such models give excellent predictions both on individual applications e g 1 false positive rate 94 true positive rate and 95 precision and across system applications on average 9 false positive rate 78 true positive rate and 95 precision we also show that these results are similarly achieved for different degree of sequence defectiveness to show how good are our results we compare them with recent studies in system log analysis we finally provide some recommendations that we draw reflecting on our study classification and prediction of defective log sequences data mining information gain log analysis software maintenance system logs a proposed new model for maintainability index of open source software 2014 ieee software metrics play a key role in measuring attributes that are important for the success of a software project measurements of these metrics tell us various key aspects of system this in turn supports knowledgeable decision making by which we can enhance the quality of system maintenance is a process of revisions or corrections made to software systems after their first release the key feature of software development is change hence it is important to develop software that is easy to modify and is thus maintainable this paper evaluates the existing oman and hagemeister maintainability index model which calculates maintainability index mi based on cyclomatic complexity lines of code and halsted volume for this purpose software metric datasets of lucene which is open source software of 163085 lines of code are used and it is shown that the existing oman and hagemeister maintainability index mode model is not a good a predictor of maintainability a new maintainability index model is proposed with a new set of predictor metrics the new proposed model is a marked improvement over the existing oman and hagemeister maintainability index model the coefficient of determination r 2 of the new proposed maintainability model is 0 984 and correlation coefficient r is 0 992 as compared to the oman and hagemeister model whose correlation coefficient r is 0 320 maintainability effort maintainability index maintenance metrics software maintainability prediction mining defect reports for predicting software maintenance effort 2015 ieee software maintenance is the crucial phase of software development lifecycle which begins once the software has been deployed at the customer s site it is a very broad activity and includes almost everything that is done to change the software if required to keep it operational after its delivery at the customer s end a lot of maintenance effort is required to change the software after it is in operation therefore predicting the effort and cost associated with the maintenance activities such as correcting and fixing the defects has become one of the key issues that need to be analyzed for effective resource allocation and decision making in view of this issue we have developed a model based on text mining techniques using the statistical method namely multi nominal multivariate logistic regression mmlr we apply text mining techniques to identify the relevant attributes from defect reports and relate these relevant attributes to software maintenance effort prediction the proposed model is validated using camera application package of android operating system receiver operating characteristics roc analysis is done to interpret the results obtained from model prediction by using the value of area under the curve auc sensitivity and a suitable threshold criterion known as the cut off point it is evident from the results that the performance of the model is dependent on the number of words considered for classification and therefore shows the best results with respect to top 100 words the performance is irrespective of the type of effort category defect reports machine learning receiver operating characteristics software maintenance effort prediction text mining 4th international conference on software process improvement cimps 2015 the proceedings contain 26 papers the special focus in this conference is on organizational models standards methodologies and knowledge management the topics include project management in small sized software enterprises addressing product quality characteristics using the iso iec 29110 analysis of coverage of moprosoft practices in curricula programs related to computer science and informatics a static view of a methodology for process and project alignment proposal of a hybrid process to manage vulnerabilities in web applications establishing the state of the art of frameworks methods and methodologies focused on lightening software process structure of a multi model catalog for software projects management including agile and traditional practices situational factors which have an impact on the successful usage of an agile methodology for software maintenance definition and implementation of the enterprise business layer through a business reference model using the architecture development method adm togaf towards the creation of a semantic repository of istar based context models operations research ontology for the integration of analytic methods and transactional data application of a variable order markov model to indicators management defect prediction in software repositories with artificial neural networks reverse engineering process for the generation of uml diagrams in web services composition evaluating and comparing perceptions between undergraduate students and practitioners in controlled experiments for requirements prioritization search based software engineering to construct binary test suites the use of simulation software for the improving the supply chain and breaking the user and developer language barrier a novel way of assessing software bug severity using dictionary of critical terms 2015 the authors due to increase in demands of software and decreased delivery span of software assuring the quality of software is becoming a challenge however no software can claim to be error free due to the complexity of software and inadequate testing there is a well known principle of testing which states that exhaustive testing is impossible hence maintenance activities are required to ensure smooth functioning of the software many open source software provides bug tracking systems to aid corrective maintenance task these bug tracking systems allow users to report the bugs that are encountered while operating the software however in software maintenance severity prediction has gained much attention recently bugs having higher severity should be fixed prior to the bugs having lesser severity triager analyzes the bug reports and assesses the severity based upon his her knowledge and experience but due to the presence of a large number of bug reports it becomes a tedious job to manually assign severity thus there is growing need for making the whole process of severity prediction automatic the paper presents an approach of creating a dictionary of critical terms specifying severity using two different feature selection methods namely info gain and chi square and classification of bug reports are performed using naïve bayes multinomial nbm and k nearest neighbor knn algorithms bug bug tracking system bug triaging system feature selection methods machine learning algorithms predictive modeling for shelf life estimation of sunflower oil blended with oleoresin rosemary rosmarinus officinalis l and ascorbyl palmitate at low and high temperatures 2014 elsevier ltd the induction period ip of the formation of conjugated dienes ip cdv at low 60°c and ip measured using rancimat at high 100 130°c temperatures were determined as oxidative stability measure osm of sunflower oil so samples blended with oleoresin rosemary rosmarinus officinalis l and ascorbyl palmitate the relationship between osm and compositional parameters peroxide value acid value total polar matter antioxidant capacity and total added antioxidants was established using partial least square pls regression the predicted shelf life at 60°c sl 60 using the pls and rancimat models resulted in the under prediction by 0 02 and 50 22 respectively to overcome the shortcomings of rancimat model a unified model was developed using ip cdv values as a function of ip at 100 130°c which over predicted the sl 60 by 0 08 the unified model estimated the sl 25 with an error of ±3 32 which was comparable to pls ±2 99 and lesser than rancimat ±11 22 models partial least square regression rancimat rosemary rosmarinus officinalis l shelf life prediction sunflower oil efficient and accurate approach for approximate string search in spatial dataset 2015 ieee this paper proposes a new index and method to find strings approximately in spatial databases specifically the task of candidate generation is as follows given a location name with wrong spelling the system finds location in osm dataset which are most similar to that location name which are misspelled an approximate solution is proposed using log linear model which is defined as a conditional probability distribution of a corrected word and a rule set for the correction conditioned on wrong location name an aho corasic tree which is used for storing and applying correction rules referred to as rule index and an aho corasic algorithm which is efficient and gives guarantee to find top k candidates experiment on large real osm dataset demonstrates the accuracy of proposed method upon existing methods aho corasick algorithm approximate string search osm dataset spatial databases maintainability prediction from project metrics data analysis using artificial neural network an interdisciplinary study software maintainability is an important aspect for all software engineering paradigms considering the maintainability a factor influencing the software quality and reliability the estimation can help to improve overall software quality maintainability is an indirect and derived measure which needs to predict using the other direct measures soft computing approaches have been used widely in prediction of software entities the paper analyzes the project history data with the help of artificial neural network and produces the predicted maintainability value of the software module or component from the project metrics data four influencing factors identified and neural network model is built for maintainability prediction the four simple input factors multiple condition count node count percentage comments and total lines of code can be easily calculated from the source code of the module or component of a project the less complexity of the input attributes makes the model more applicable in software industries the ann model is evaluated and validated on history data from three projects the root mean square error value shows the ann as good technique to predict the maintainability from the history data idosi publications 2014 artificial neural network maintainability maintenance prediction quality software metrics software defect prediction using relational association rule mining this paper focuses on the problem of defect prediction a problem of major importance during software maintenance and evolution it is essential for software developers to identify defective software modules in order to continuously improve the quality of a software system as the conditions for a software module to have defects are hard to identify machine learning based classification models are still developed to approach the problem of defect prediction we propose a novel classification model based on relational association rules mining relational association rules are an extension of ordinal association rules which are a particular type of association rules that describe numerical orderings between attributes that commonly occur over a dataset our classifier is based on the discovery of relational association rules for predicting whether a software module is or it is not defective an experimental evaluation of the proposed model on the open source nasa datasets as well as a comparison to similar existing approaches is provided the obtained results show that our classifier overperforms for most of the considered evaluation measures the existing machine learning based techniques for defect prediction this confirms the potential of our proposal 2014 elsevier inc all rights reserved association rule data mining defect prediction software engineering smplearner learning to predict software maintainability 2014 springer science business media new york accurate and practical software maintainability prediction enables organizations to effectively manage their maintenance resources and guide maintenance related decision making this paper presents smplearner an automated learning based approach to train maintainability predictors by harvesting the actual average maintenance effort computed from the code change history as well as employing a much richer set of 44 four level hierarchical code metrics collected by static code analysis tools we systematically evaluated smplearner on 150 observations partitioned from releases of eight large scale open source software systems our evaluation showed that smplearner not only outperformed the traditional 4 metric mi model but also the recent learning based maintainability predictors constructed based on single class level metrics demonstrating that single class level metrics were not sufficient for maintainability prediction machine learning maintenance effort software maintainability software metric object oriented class stability prediction a comparison between artificial neural network and support vector machine 2014 king fahd university of petroleum and minerals software stability is an important factor for better software quality stable classes tend to reduce the software maintenance cost and effort therefore achieving class stability is an important quality objective when developing software designers can make better decisions to improve class stability if they can predict it before the fact using some predictors in this paper we investigate the correlation between some available design measurements and class stability over versions and propose a stability prediction model using such available measurements we conducted a set of experiments using artificial neural network ann and support vector machine svm to build different prediction models we compared the accuracy of these prediction models our experiments reveal that ann and svm prediction models are effective in predicting object oriented class stability artificial intelligence class stability prediction software quality using software metrics to estimate the impact of maintenance in the performance of embedded software 2014 ieee this paper proposes a strategy to assist the designer in evaluating the impact of a design choice with respect to the non functional requirements in embedded systems we use several regression models to predict physical metrics from design metrics in order to estimate the impact on performance of software changes in the early stages of its development this prediction can be used both during maintenance and during the first design to compare alternative module decompositions or design changes before implementation such an early estimation allows an efficient design space exploration with no penalty in the development time which are crucial aspects for an embedded system design space exploration embedded systems maintenance regression analysis software metrics a process metrics based framework for aspect oriented software to predict software bugs and maintenance 2014 ieee the quality evaluation of software products e g defect measuring and identification approaches of individual versions gains importance with higher use of software applications process metrics are considered as the main predictor of defect prediction and software maintenance in numerous empirical studies for the software product in addition to that there are few models consider process metrics as an indicator of software quality too the prime objective of this paper is to design of process metrics framework for aspect oriented software this framework can be extended for defect prediction and quality evaluation of aspect oriented software aspect oriented programming aop aspect oriented software development aosd defect prediction process metrics separation of concerns soc software maintenance software metrics software quality and quality characteristics software testability a way to predict and evaluate of software maintainability based on machine learning the accurate maintainability prediction and evaluation of software applications can improve the designing management for these applications thus benefiting designing organizations therefore there is considerable research interest in development and application of sophisticated techniques which can be used to build models for both predicting and evaluating software maintainability in this paper we investigate some ideas based on machine learning natural language processing fuzzy logic and systematic model of software maintenance the idea to compute interactive index and the maintainability of software system is useful to study the relation between maintainability prediction and maintainability evaluation in the whole software process an model basing on fuzzy matrix and bp neural network is built up it s approved that there are application value of using this model based on bp neural network to predict and evaluate the software maintainability 2014 trans tech publications switzerland fuzzy matrix machine learning neural network model software maintainability classification model for maintainability prediction 2014 mikyeong park and euyseok hong software maintainability prediction is important because it enables organizations to effectively manage maintenance resources and improve design and coding most studies have concentrated on estimating the number of changes or changed lines of code during maintenance period on the other hand we propose classification models that determine maintainability levels of software units classification model is simple to use and makes it easy for developers to analyze results using widely used classification algorithms we build and evaluate two types of prediction models the experimental results show that decision tree with a cfssubseteval attribute selection method has the best performance in binary classifications and naïve bayesian outperforms others in ternary classifications classification maintainability prediction software maintainability software maintainability prediction by data mining of software code metrics 2014 ieee software maintainability is a key quality attribute that determines the success of a software product since software maintainability is an important attribute of software quality accurate prediction of it can help to improve overall software quality this paper utilizes data mining of some new predictor metrics apart from traditionally used software metrics for predicting maintainability of software systems the prediction models are constructed using static code metric datasets of four different open source software oss lucene jhotdraw jedit and jtreeview lucene contain 385 classes and is of 135241 lines of code loc oss jhotdraw contain 159 classes and is of 21802 loc oss jedit contain 275 classes and is of 104053 loc oss and jtreeview contain 60 classes and is of 11988 loc oss the metrics were collected using two different metrics extraction tools chidamber and kemerer java metric ckjm tool and intellij idea naïve bayes bayes network logistic multilayerperceptron and random forest classifiers are used to identify the software modules that are difficult to maintain random forest models are found to be most useful in software maintainability prediction by data mining of software code metrics as random forest models have higher recall precision and area under curve auc of roc curve data mining software code metrics software maintainability prediction application of group method of data handling model for software maintainability prediction using object oriented systems object oriented methodology has emerged as most prominent in software industry for application development maintenance phase begins once the product is delivered and by software maintainability we mean the ease with which existing software could be modified during maintenance phase we can improve and control software maintainability if we can predict it in the early phases of software life cycle using design metrics predicting the maintainability of any software has become critical with the increasing importance of software maintenance many authors have practiced and proved theoretical validation followed by empirical evaluation using statistical and experimental techniques for evaluating the relevance of any given metrics suite using many models in this paper we have presented an empirical study to evaluate the effectiveness of novel technique called group method of data handling gmdh for the prediction of maintainability over other models although many metrics have been proposed in the literature software design metrics suite proposed by chidamber et al and revised by li et al have been selected for this study two web based customized softwares developed using c language have been used for empirical study source code of old and new versions for both applications were collected and analysed against modifications made in every class the changes were counted in terms of number of lines added deleted or modified in the classes belonging to new version with respect to the classes of old version finally values of metrics were combined with change in order to generate data points hence in this study an attempt has been made to evaluate and examine the effectiveness of prediction models for the purpose of software maintainability using real life web based projects three models using feed forward 3 layer back propagation network ff3lbpn general regression neural network grnn and gmdh are developed and performance of gmdh is compared against two others i e ff3lbpn and grnn with the aid of this empirical analysis we can safely suggest that software professionals can use oo metric suite to predict the maintainability of software using gmdh technique with least error and best precision in an object oriented paradigm 2014 the society for reliability engineering quality and operations management sreqom india and the division of operation and maintenance lulea university of technology sweden empirical validation feed forward 3 layer back propagation network ff3lbpn general regression neural network grnn group method of data handling gmdh software maintainability cross project defect prediction models l union fait la force existing defect prediction models use product or process metrics and machine learning methods to identify defect prone source code entities different classifiers e g linear regression logistic regression or classification trees have been investigated in the last decade the results achieved so far are sometimes contrasting and do not show a clear winner in this paper we present an empirical study aiming at statistically analyzing the equivalence of different defect predictors we also propose a combined approach coined as codep combined defect predictor that employs the classification provided by different machine learning techniques to improve the detection of defect prone entities the study was conducted on 10 open source software systems and in the context of cross project defect prediction that represents one of the main challenges in the defect prediction field the statistical analysis of the results indicates that the investigated classifiers are not equivalent and they can complement each other this is also confirmed by the superior prediction accuracy achieved by codep when compared to stand alone defect predictors 2014 ieee a metric suite for predicting software maintainability in data intensive applications springer science business media dordrecht 2014 software maintainability is the vital aspect of software quality and defined as the ease with which modifications can be made once the software is delivered tracking the maintenance behaviour of a software product is very complex that is widely acknowledged by the researchers many research studies have empirically validated that the prediction of object oriented software maintainability can be achieved before actual operation of the software using design metrics proposed by chidamber and kemerer c k however the framework and reference architecture in which the software systems are being currently developed have changed dramatically in recent times due to the emergence of data warehouse and data mining field in the prevailing scenario certain deficiencies were discovered when c k metric suite was evaluated for data intensive applications in this study we propose a new metric suite to overcome these deficiencies and redefine the relationship between design metrics with maintainability the proposed metric suite is evaluated analyzed and empirically validated using five proprietary software systems the results show that the proposed metric suite is very effective for maintainability prediction of all software systems in general and for data intensive software systems in particular the proposed metric suite may be significantly helpful to the developers in analyzing the maintainability of data intensive software systems before deploying them data intensive applications empirical validation machine learning prediction models software design metric software maintainability evaluation of quantitative and mining techniques for reducing software maintenance risks 2014 abdelrafe elzamly and burairah hussin software risk is not always avoidable but it is controllable the aim of this paper is to present new techniques that were performed using quantitative and mining techniques to compare the risk management techniques to each of the software maintenance risks to identify and model if they are effective in mitigating the occurrence of each software maintenance risk in software development life cycle the model s accuracy slightly improves in fuzzy multiple regression modelling techniques than or quite equal stepwise multiple regression modelling techniques all models in fuzzy and stepwise acceptable value for mmre less than 0 25 and pred 0 25 greater or than 0 75 is desirable the study has been conducted on a group of software project management successful software project risk management will greatly improve the probability of project success mining techniques mmre pred l quantitative techniques software maintenance project software risk management transition and defect patterns of components in dependency cycles during software evolution the challenge to break existing cyclically connected components of running software is not trivial since it involves planning and human resources to ensure that the software behavior is preserved after refactoring activity therefore to motivate refactoring it is essential to obtain evidence of the benefits to the product quality this study investigates the defect proneness patterns of cyclically connected components vs noncyclic ones when they transition across software releases we have mined and classified software components into two groups and two transition states the cyclic and the non cyclic ones next we have performed an empirical study of four software systems from evolutionary perspective using standard statistical tests on formulated hypotheses we have determined the significance of the defect profiles and complexities of each group the results show that during software evolution components that transition between dependency cycles have higher probability to be defect prone than those that transition outside of cycles furthermore out of the three complexity variables investigated we found that an increase in the class reachability set size tends to be more associated with components that turn defective when they transition between dependency cycles lastly we found no evidence of any systematic cycle breaking refactoring between releases of the software systems thus these findings motivate for refactoring of components in dependency cycle taking into account the minimization of metrics such as the class reachability set size 2014 ieee defect proneness dependency cycle refactoring cross project change prediction using open source projects 2014 ieee predicting the changes in the next release of software during the early phases of software development is gaining wide importance such a prediction helps in allocating the resources appropriately and thus reduces costs associated with software maintenance but predicting the changes using the historical data data of past releases of the software is not always possible due to unavailability of data thus it would be highly advantageous if we can train the model using the data from other projects rather than the same project in this paper we have performed cross project predictions using 12 datasets obtained from three open source apache projects abdera poi and rave in the study cross project predictions include both the inter project different projects and inter version different versions of same projects predictions for cross project predictions we investigated whether the characteristics of the datasets are valuable for selecting the training set for a known testing set we concluded that cross project predictions give high accuracy and the distributional characteristics of the datasets are extremely useful for selecting the appropriate training set besides this within cross project predictions we also examined the accuracy of inter version predictions change prediction cross project inter version prediction machine learning metrics object oriented paradigm proceedings of the 7th india software engineering conference 2014 isec 2014 the proceedings contain 23 papers the topics discussed include samikshaviz a panoramic view to measure contribution and performance of software maintenance professionals by mining bug archives operational abstractions of model transforms class point approach for software effort estimation using various support vector regression kernel methods latent semantic centrality based automated requirements prioritization does increasing formalism in the use case template help a comparative study of feature ranking and feature subset selection techniques for improved fault prediction outsourcing service provision through step wise transformation probabilistic component identification and a three layer model of source code comprehension electre tri to classify items with respect to maintainability criterion the paper aims to suggest a methodology able to support the analyst in the assessment process of items on the base of a criterion inspired to the maintainability parameter and in the successive assignment phase of the items in the predefined classes the maintainability involves the continuous improvement of a system in order to improve the ability to maintain or improve the reliability of the analyzed system in particular the last one is the probability that a failed system will be restored to specified conditions within a given period of time when maintenance is performed according to prescribed procedures and resources the faced case in this research regards the software maintenance field in which the maintainability can be defined as the probability that a program will be restored to working conditions in a given period of time when it is being changed modified or enhanced furthermore the maintainability can be subdivided into diverse sub criteria as analysability how easy or difficult is to diagnose the system for deficiencies or to identify the parts that need to be modified changeability how easy or difficult is to make adaptations to the system stability how easy or difficult is to keep the system in a consistent state during the modification process testability how easy or difficult is it to test the system after modifications thus with respect to aforementioned sub criteria the present research wants to propose the electre tri method in order to assign the different analyzed items with relation to the maintainability criterion into the different predefined classes with the aim to illustrate the method validity a numerical application is shown with relation to the classification under the maintainability criterion of different modular enterprise resource planning software systems electre tri enterprise resource planning maintenaibility 2013 international conference on information and communication technology for education icte 2013 the proceedings contain 154 papers the special focus in this conference is on circuits and systems computers and information technology communication systems controls theory and applications automation and signal processing electrical power systems and computer education the topics include design of a miniaturized power divider for 3g applications multipurpose laser simulated shooting system the use of nfc in transportation information system for visually impaired persons the performance measurement of marine logistics information platform an automatic analysis system for online hotel reviews a new automatic analysis system for risk behaviors analysis of virtual desktop remote protocol low resolution surface simplification using shape operators with large scale surface analysis an efficiently orderable encryption in cloud computing analysis of examination questions and scores by using decision tree algorithm a detection and track algorithm for moving dim targets in deep space a detection and track algorithm for moving dim targets in deep space electricity market management information system based on the cloud computing an accelerated image reconstruction algorithm based on fusion of color and codeword automatic technology application in china container terminals improving education through data mining a new type of 3d reconstruction software system design based on 3d calibration board improved shuffled frog leaping algorithm for permutation flow shop scheduling reflection properties of electromagnetic waves from a moving conducting surface comparative analysis of automatic tortuosity classification algorithms an emergency medical system based on cloud computing a novel 3d visualization method for tunnels using consumer digital camera fault map query using content based image retrieval digital video steganalysis based on histogram and texture features dynamic prediction of individual customer retention a hardware accelerator approach for quantum computer modeling multilanguage translation usage in toolkit of modeling systems identifying dependencies of the source code for software maintenance implementation of camera based traffic monitoring system a heuristic design method for loop invariant template multi band sar image fusion study based on nsct and pcnn trust model based on exponential smoothing method for wmn identifying the topic of queries based on domain specify ontology consistency model for collaborative software development on cloud an extensible peer to peer platform architecture for web of things a discrete computation approach for helical pipe bending in wearable muscle supports designing a new approach to implement policies and creation of the first web observatory using concept semantic similarity for documents classification dynamic power management strategy based on weighted markov model using alternative error control codes for dvb systems generalized central particle swarm optimization design and realization of communication training simulator of certain radar the improved intra prediction mode for avs hybrid factor graph equalization for the linear distortion of satellite channels localization of random mobile vehicles application research on uwb wireless communication technology for metal mine underground study on a bound of broadcasting time for the publishing network the research on the things sns and internet dns middleware the error analysis of rotating accelerometer gravity gradiometer extraction research on the affecting factors of rfid reading performance of accessing the warehouse chinese folk instrument identification based on pitch histogram course centered knowledge management and application in online learning based on web ontology automatic mission construction system for educational game design an integrated solution for arms trade aircraft simulation training distance education oriented web instant messaging system system design and implementation of the student works showing based on flash platform technology iso osi model and data communication by animations primary exploration on teaching of linux operating system course the new experimental teaching mode of program design the rigor versus relevance debate project education method at distance teaching system college english teaching in web based computer assisted language learning a design model of digital education games based on flow theory comparison on the introductory course of computer science on the marketing strategy of consuming now and paying later a virtual reality system for vehicle accident safety education the need of incorporating cidos with facebook to facilitate online collaborative learning research and practice of innovative engineering and technical talent training mode based on cdio different approaches in teaching programming a reform proposed on numerical analysis course application of metacognitive strategy training into reading class effects of preliminary preparation and revision on student performance in learning programming the implementation of academic frontier based approach based on constructivism design and analysis of teaching case about fast malab algorithms the new approach of college ideological and political education and the research and practice of college enterprise cooperation teaching pattern in computer specialty machine learning approaches for predicting software maintainability a fuzzy based transparent model software quality is one of the most important factors for assessing the global competitive position of any software company thus the quantification of the quality parameters and integrating them into the quality models is very essential many attempts have been made to precisely quantify the software quality parameters using various models such as boehm s model mccall s model and iso iec 9126 quality model a major challenge although is that effective quality models should consider two types of knowledge imprecise linguistic knowledge from the experts and precise numerical knowledge from historical data incorporating the experts knowledge poses a constraint on the quality model the model has to be transparent in this study the authorspropose a process for developing fuzzy logic based transparent quality prediction models they applied the process to a case study where mamdani fuzzy inference engine is used to predict software maintainability theycompared the mamdani based model with other machine learning approaches the resultsshow that the mamdani based model is superior to all 2013 the institution of engineering and technology predicting bugs using antipatterns bug prediction models are often used to help allocate software quality assurance efforts software metrics e g process metrics and product metrics are at the heart of bug prediction models however some of these metrics like churn are not actionable on the contrary antipatterns which refer to specific design and implementation styles can tell the developers whether a design choice is poor or not poor designs can be fixed by refactoring therefore in this paper we explore the use of antipatterns for bug prediction and strive to improve the accuracy of bug prediction models by proposing various metrics based on antipatterns an additional feature to our proposed metrics is that they take into account the history of antipatterns in files from their inception into the system through a case study on multiple versions of eclipse and argouml we observe that i files participating in antipatterns have higher bug density than other files ii our proposed antipattern based metrics can provide additional explanatory power over traditional metrics and iii improve the f measure of cross system bug prediction models by 12 5 in average managers and quality assurance personnel can use our proposed metrics to better improve their bug prediction models and better focus testing activities and the allocation of support resources 2013 ieee antipattern bug prediction software quality how does context affect the distribution of software maintainability metrics software metrics have many uses e g defect prediction effort estimation and benchmarking an organization against peers and industry standards in all these cases metrics may depend on the context such as the programming language here we aim to investigate if the distributions of commonly used metrics do in fact vary with six context factors application domain programming language age lifespan the number of changes and the number of downloads for this preliminary study we select 320 nontrivial software systems from source forge these software systems are randomly sampled from nine popular application domains of source forge we calculate 39 metrics commonly used to assess software maintainability for each software system and use kruskal wallis test and mann whitney u test to determine if there are significant differences among the distributions with respect to each of the six context factors we use cliff s delta to measure the magnitude of the differences and find that all six context factors affect the distribution of 20 metrics and the programming language factor affects 35 metrics we also briefly discuss how each context factor may affect the distribution of metric values we expect our results to help software benchmarking and other software engineering methods that rely on these commonly used metrics to be tailored to a particular context 2013 ieee benchmark context context factor large scale metrics mining software repositories sampling software maintainability static metrics will fault localization work for these failures an automated approach to predict effectiveness of fault localization tools debugging is a crucial yet expensive activity to improve the reliability of software systems to reduce debugging cost various fault localization tools have been proposed a spectrum based fault localization tool often outputs an ordered list of program elements sorted based on their likelihood to be the root cause of a set of failures i e their suspiciousness scores despite the many studies on fault localization unfortunately however for many bugs the root causes are often low in the ordered list this potentially causes developers to distrust fault localization tools recently parnin and orso highlight in their user study that many debuggers do not find fault localization useful if they do not find the root cause early in the list to alleviate the above issue we build an oracle that could predict whether the output of a fault localization tool can be trusted or not if the output is not likely to be trusted developers do not need to spend time going through the list of most suspicious program elements one by one rather other conventional means of debugging could be performed to construct the oracle we extract the values of a number of features that are potentially related to the effectiveness of fault localization building upon advances in machine learning we process these feature values to learn a discriminative model that is able to predict the effectiveness of a fault localization tool output in this preliminary work we consider an output of a fault localization tool to be effective if the root cause appears in the top 10 most suspicious program elements we have experimented our proposed oracle on 200 faulty programs from space nanoxml xml security and the 7 programs in siemens test suite our experiments demonstrate that we could predict the effectiveness of fault localization tool with a precision recall and f measure harmonic mean of precision and recall of 54 36 95 29 and 69 23 the numbers indicate that many ineffective fault localization instances are identified correctly while only very few effective ones are identified wrongly 2013 ieee classification effectiveness prediction fault localization on the relationships between domain based coupling and code clones an exploratory study knowledge of similar code fragments also known as code clones is important to many software maintenance activities including bug fixing refactoring impact analysis and program comprehension while a great deal of research has been conducted for finding techniques and implementing tools to identify code clones little research has been done to analyze the relationships between code clones and other aspects of software in this paper we attempt to uncover the relationships between code clones and coupling among domain level components we report on a case study of a large scale open source enterprise system where we demonstrate that the probability of finding code clones among components with domain based coupling is more than 90 while such a probabilistic view does not replace a clone detection tool per se it certainly has the potential to complement the existing tools by providing the probability of having code clones between software components for example it can both reduce the clone search space and provide a flexible and language independent way of focusing only on a specific part of the system it can also provide a higher level of abstraction to look at the cloning relationships among software components 2013 ieee predicting bug fixing time an empirical study of commercial software projects for a large and evolving software system the project team could receive many bug reports over a long period of time it is important to achieve a quantitative understanding of bug fixing time the ability to predict bug fixing time can help a project team better estimate software maintenance efforts and better manage software projects in this paper we perform an empirical study of bug fixing time for three ca technologies projects we propose a markov based method for predicting the number of bugs that will be fixed in future for a given number of defects we propose a method for estimating the total amount of time required to fix them based on the empirical distribution of bug fixing time derived from historical data for a given bug report we can also construct a classification model to predict slow or quick fix e g below or above a time threshold we evaluate our methods using real maintenance data from three ca technologies projects the results show that the proposed methods are effective 2013 ieee bug fixing time bugs effort estimation prediction software maintenance studying re opened bugs in open source software bug fixing accounts for a large amount of the software maintenance resources generally bugs are reported fixed verified and closed however in some cases bugs have to be re opened re opened bugs increase maintenance costs degrade the overall user perceived quality of the software and lead to unnecessary rework by busy practitioners in this paper we study and predict re opened bugs through a case study on three large open source projects namely eclipse apache and openoffice we structure our study along four dimensions 1 the work habits dimension e g the weekday on which the bug was initially closed 2 the bug report dimension e g the component in which the bug was found 3 the bug fix dimension e g the amount of time it took to perform the initial fix and 4 the team dimension e g the experience of the bug fixer we build decision trees using the aforementioned factors that aim to predict re opened bugs we perform top node analysis to determine which factors are the most important indicators of whether or not a bug will be re opened our study shows that the comment text and last status of the bug when it is initially closed are the most important factors related to whether or not a bug will be re opened using a combination of these dimensions we can build explainable prediction models that can achieve a precision between 52 1 78 6 and a recall in the range of 70 5 94 1 when predicting whether a bug will be re opened we find that the factors that best indicate which bugs might be re opened vary based on the project the comment text is the most important factor for the eclipse and openoffice projects while the last status is the most important one for apache these factors should be closely examined in order to reduce maintenance cost due to re opened bugs 2012 springer science business media llc bug reports open source software re opened bugs prediction of change prone classes using machine learning and statistical techniques 2013 igi global for software development availability of resources is limited thereby necessitating efficient and effective utilization of resources this can be achieved through prediction of key attributes which affect software quality such as fault proneness change proneness effort maintainability etc the primary aim of this chapter is to investigate the relationship between object oriented metrics and change proneness predicting the classes that are prone to changes can help in maintenance and testing developers can focus on the classes that are more change prone by appropriately allocating resources this will help in reducing costs associated with software maintenance activities the authors have constructed models to predict change proneness using various machine learning methods and one statistical method they have evaluated and compared the performance of these methods the proposed models are validated using open source software frinika and the results are evaluated using receiver operating characteristic roc analysis the study shows that machine learning methods are more efficient than regression techniques among the machine learning methods boosting technique i e logitboost outperformed all the other models thus the authors conclude that the developed models can be used to predict the change proneness of classes leading to improved software quality statistical comparison of modelling methods for software maintainability prediction the objective of this paper is statistical comparison of modelling methods for software maintainability prediction the statistical comparison is performed by building software maintainability prediction models using 27 dierent regression and machine learning based algorithms for this purpose software metrics datasets of two dierent commercial object oriented systems are used these systems were developed using an object oriented programming language ada these systems are user interface management system uims and quality evaluation system ques it is shown that dierent measures like mmre rmse pred 0 25 and pred 0 30 calculated on predicted values obtained from leave one out loo cross validation produce very divergent results regarding accuracy of modelling methods therefore the 27 modelling methods are evaluated on the basis of statistical signicance tests the friedman test is used to rank various modelling methods in terms of absolute residual error six out of the ten top ranked modelling methods are common to both uims and ques this indicates that modelling methods for software maintainability predicton are solid and scalable after obtaining ranks pair wise wilcoxon signed rank test is performed wilcoxon sign rank test indicates that the top ranking method in uims data set is significantly superior to only four other modelling methods whereas the top tanking method in ques data set is significantly superior to 11 other modelling methods the performance of instance based learning algorithms ibk and kstar is comparable to modelling methods used in earlier studies c world scientific publishing company machine learning significance tests software maintainability prediction software effort prediction a hyper heuristic decision tree based approach software effort prediction is an important task within software engineering in particular machine learning algorithms have been widely employed to this task bearing in mind their capability of providing accurate predictive models for the analysis of project stakeholders nevertheless none of these algorithms has become the de facto standard for metrics prediction given the particularities of different software projects among these intelligent strategies decision trees and evolutionary algorithms have been continuously employed for software metrics prediction though mostly independent from each other a recent work has proposed evolving decision trees through an evolutionary algorithm and applying the resulting tree in the context of software maintenance effort prediction in this paper we raise the search space level of an evolutionary algorithm by proposing the evolution of a decision tree algorithm instead of the decision tree itself an approach known as hyper heuristic our findings show that the decision tree algorithm automatically generated by a hyper heuristic is capable of statistically outperforming state of the art top down and evolution based decision tree algorithms as well as traditional logistic regression the ability of generating a highly accurate comprehensible predictive model is crucial in software projects considering that it allows the stakeholder to properly manage the team s resources with an improved confidence in the model predictions copyright 2013 acm decision trees evolutionary algorithms head dt hyper heuristics software effort estimation using cbr and cart to predict maintainability of relational database driven software applications background relational database driven software applications have gained significant importance in modern software development given that software maintainability is an important quality attribute predicting these applications maintainability can provide various benefits to software organizations such as adopting a defensive design and more informed resource management aims the aim of this paper is to present the results from employing two well known prediction techniques to estimate the maintainability of relational database driven applications method case based reasoning cbr and classification and regression trees cart were applied to data gathered on 56 software projects from software companies the projects concerned development and or maintenance of relational database driven applications unlike previous studies all variables 28 independent and 1 dependent were measured on a 5 point bi polar scale results results showed that cbr performed slightly better at 76 8 correct predictions in terms of prediction accuracy when compared to cart 67 8 in addition the two important predictors identified were documentation quality and understandability of the applications conclusions the results show that cbr can be used by software companies to formalize and improve their process of maintainability prediction future work involves gathering more data and also employing other prediction techniques copyright 2013 acm case based reasoning classification trees maintainability prediction relational database driven software applications a comparative study of supervised learning algorithms for re opened bug prediction bug fixing is a time consuming and costly job which is performed in the whole life cycle of software development and maintenance for many systems bugs are managed in bug management systems such as bugzilla generally the status of a typical bug report in bugzilla changes from new to assigned verified and closed however some bugs have to be reopened reopened bugs increase the software development and maintenance cost increase the workload of bug fixers and might even delay the future delivery of a software only a few studies investigate the phenomenon of reopened bug reports in this paper we evaluate the effectiveness of various supervised learning algorithms to predict if a bug report would be reopened we choose 7 state of the art classical supervised learning algorithm in machine learning literature i e knn svm simple logistic bayesian network decision table cart and lwl and 3 ensemble learning algorithms i e adaboost bagging and random forest and evaluate their performance in predicting reopened bug reports the experiment results show that among the 10 algorithms bagging and decision table idtm achieve the best performance they achieve accuracy scores of 92 91 and 92 80 respectively and reopened bug reports f measure scores of 0 735 and 0 732 respectively these results improve the reopened bug reports f measure of the state of the art approaches proposed by shihab et al by up to 23 53 2013 ieee bug reports classification comparative study reopened reports supervised learning algorithms call graph based metrics to evaluate software design quality software defects prediction was introduced to support development and maintenance activities such as improving the software quality through finding errors or patterns of errors early in the software development process software defects prediction is playing the role of maintenance facilitation in terms of effort time and more importantly the cost prediction for software maintenance and evolution activities in this research software call graph model is used to evaluate its ability to predict quality related attributes in developed software products as a case study the call graph model is generated for several applications in order to represent and reflect the degree of their complexity especially in terms of understandability testability and maintenance efforts this call graph model is then used to collect some software product attributes and formulate several call graph based metrics the extracted metrics are investigated in relation or correlation with bugs collected from customers bug reports for the evaluated applications those software related bugs are compiled into dataset files to be used as an input to a data miner for classification prediction and association analysis finally the results of the analysis are evaluated in terms of finding the correlation between call graph based metrics and software products bugs in this research we assert that call graph based metrics are appropriate to be used to detect and predict software defects so the activities of maintenance and testing stages after the delivery become easier to estimate or assess call graph coupling metrics defects prediction software maintainability software metrics software testing evaluating performance of network metrics for bug prediction in software 2013 ieee code based metrics and network analysis based metrics are widely used to predict defects in software however their effectiveness in predicting bugs either individually or together is still actively researched in this paper we evaluate the performance of these metrics using three different techniques namely logistic regression support vector machines and random forests we analysed the performance of these techniques under three different scenarios on a large dataset the results show that code metrics outperform network metrics and also no considerable advantage in using both of them together further an analysis on the influence of individual metrics for prediction of bugs shows that network metrics except out degree are uninfluential bug prediction network analysis metrics performance evaluation software maintenance software metrics modeling code analyzability at method level in j2ee applications 2013 ieee one of the main reasons for improving code structure is to make the cause of error code easily identified thus developers need an analyzability prediction model to evaluate the analyzability of code in order to locate classes to be improved to identify classes to be improved developers must analyze all methods of classes for finding problem methods therefore analyzability prediction model must be created for calculating analyzability level of method currently j2ee applications are legacy systems and need to be continually maintained hence code analyzability prediction at method level in j2ee application helps developers to know which method should be improved for increasing code understanding and reducing time for finding error causes however there is a lack of analyzability prediction model for j2ee application and existing research works on analyzability prediction model do not focus on method level therefore this paper proposes how to create analyzability prediction model at method level for j2ee applications through ordinal logistic regression analyzability component j2ee maintainability software maintenance estimating the regression test case selection probability using fuzzy rules software maintenance is performed regularly for enhancing and adapting the functionalities of the existing software which modifies the software and breaks the previously verified functionalities this sets a requirement for software regression testing making it a necessary maintenance activity as the evolution of software takes place the size of the test suite tends to grow which makes it difficult to execute the entire test suite in a time constrained environment there are many existing techniques for regression test case selection some are based on dataflow analysis technique slicing based technique bio inspired techniques and genetic algorithm based techniques this paper gives a regression test case selection technique based on fuzzy model which reduces the size of the test suite by selecting test cases from existing test suite the test cases which are necessary for validating the recent changes in the software and have the ability to find the faults and cover maximum coding under testing in minimum time are selected a fuzzy model is designed which takes three parameters namely code covered execution time and faults covered as input and produces the estimation for the test case selection probability as very low low medium high and very high 2013 ieee fuzzy logic regression testing selection probability test case selection software maintainability prediction model based on fuzzy neural network due to the vast deployment of object oriented software in our day to day livings the issue of software maintainability prediction which aims at ameliorating the software design process and planning the amount constrained budget efficiently calls attention to it in this paper a fuzzy neural network fnn based software maintainability prediction model which combines the artificial neural network ann and the fuzzy logic fl is proposed to overcome the innate flaws of fnn a statistical technique e g principle component analysis pca is also used for the sake of computational simplicity the proposed fnn reinforced by pca can effectively reflect the complex relations among independent and dependable variables that is by showing relatively high prediction accuracy the empirical experimental results verify this claim in the sense that with respect to two disparate object oriented software data sets the model built by the proposed method prevails against three other typical counterparts multivariable linear regression mlr ann and support vector regression svr in terms of the prediction accuracy 2012 old city publishing inc fuzzy neural network object oriented software principle component analysis software maintainability prediction when would this bug get reported not all bugs in software would be experienced and reported by end users right away some bugs manifest themselves quickly and may be reported by users a few days after they get into the code base others manifest many months or even years later and may only be experienced and reported by a small number of users we refer to the period of time between the time when a bug is introduced into code and the time when it is reported by a user as bug reporting latency knowledge of bug reporting latencies has an implication on prioritization of bug fixing activities bugs with low reporting latencies may be fixed earlier than those with high latencies to shift debugging resources towards bugs highly concerning users to investigate bug reporting latencies we analyze bugs from three java software systems aspectj rhino and lucene we extract bug reporting data from their version control repositories and bug tracking systems identify bug locations based on bug fixes and back trace bug introducing time based on change histories of the buggy code also we remove non essential changes and most importantly recover root causes of bugs from their treatments fixes we then calculate the bug reporting latencies and find that bugs have diverse reporting latencies based on the calculated reporting latencies and features we extract from bugs we build classification models that can predict whether a bug would be reported early within 30 days or later which may be helpful for prioritizing bug fixing activities our evaluation on the three software systems shows that our bug reporting latency prediction models could achieve an auc area under the receiving operating characteristics curve of 70 869 2012 ieee towards building method level maintainability models based on expert evaluations the maintainability of software systems is getting more and more attention both from researchers and industrial experts this is due to its direct impact on development costs and reliability of the software many models exist for estimating maintainability by aggregating low level source code metrics however very few of them are able to predict the maintainability on method level even fewer take subjective human opinions into consideration in this paper we present a new approach to create method level maintainability prediction models based on human surveys using regression techniques we performed three different surveys and compared the derived prediction models our regression models were built based on approximately 150000 answers of 268 persons these models were able to estimate the maintainability of methods with a 0 72 correlation and a 0 83 mean absolute error on a continuous 0 10 2012 springer verlag comparative study iso iec 9126 regression analysis software maintainability can we predict types of code changes an empirical analysis there exist many approaches that help in pointing developers to the change prone parts of a software system although beneficial they mostly fall short in providing details of these changes fine grained source code changes scc capture such detailed code changes and their semantics on the statement level these scc can be condition changes interface modifications inserts or deletions of methods and attributes or other kinds of statement changes in this paper we explore prediction models for whether a source file will be affected by a certain type of scc these predictions are computed on the static source code dependency graph and use social network centrality measures and object oriented metrics for that we use change data of the eclipse platform and the azureus 3 project the results show that neural network models can predict categories of scc types furthermore our models can output a list of the potentially change prone files ranked according to their change proneness overall and per change type category 2012 ieee machine learning software maintenance software quality recommending relevant code artifacts for change requests using multiple predictors finding code artifacts affected by a given change request is a time consuming process in large software systems various approaches have been proposed to automate this activity e g based on information retrieval the performance of a particular prediction approach often highly depends on attributes like coding style or writing style of change request thus we propose to use multiple prediction approaches in combination with machine learning first experiments show that machine learning is well suitable to weight different prediction approaches for individual software projects and hence improve prediction performance 2012 ieee recommendation systems software maintenance predicting software maintenance effort through evolutionary based decision trees software effort prediction has been a challenge for researchers throughout the years several approaches for producing predictive models from collected data have been proposed although none has become standard given the specificities of different software projects the most commonly employed strategy for estimating software effort the multivariate linear regression technique has numerous shortcomings though which motivated the exploration of many machine learning techniques among the researched strategies decision trees and evolutionary algorithms have been increasingly employed for software effort prediction though independently in this paper we propose employing an evolutionary algorithm to generate a decision tree tailored to a software effort data set provided by a large worldwide it company our findings show that evolutionarily induced decision trees statistically outperform greedily induced ones as well as traditional logistic regression moreover an evolutionary algorithm with a bias towards comprehensibility can generate trees which are easier to be interpreted by the project stakeholders and that is crucial in order to improve the stakeholder s confidence in the final prediction 2012 acm decision trees evolutionary algorithms legal tree software effort estimation uncovering causal relationships between software metrics and bugs bug prediction is an important challenge for software engineering research it consist in looking for possible early indicators of the presence of bugs in a software however despite the relevance of the issue most experiments designed to evaluate bug prediction only investigate whether there is a linear relation between the predictor and the presence of bugs however it is well known that standard regression models cannot filter out spurious relations therefore in this paper we describe an experiment to discover more robust evidences towards causality between software metrics as predictors and the occurrence of bugs for this purpose we have relied on granger causality test to evaluate whether past changes in a given time series are useful to forecast changes in another series as its name suggests granger test is a better indication of causality between two variables we present and discuss the results of experiments on four real world systems evaluated over a time frame of almost four years particularly we have been able to discover in the history of metrics the causes in the terms of the granger test for 64 to 93 of the defects reported for the systems considered in our experiment 2012 ieee bug prediction causality granger test software metrics an empirical study on improving severity prediction of defect reports using feature selection in software maintenance severity prediction on defect reports is an emerging issue obtaining research attention due to the considerable triaging cost in the past research work several text mining approaches have been proposed to predict the severity using advanced learning models although these approaches demonstrate the effectiveness of predicting the severity they do not discuss the problem of how to find the indicators in good quality in this paper we discuss whether feature selection can benefit the severity prediction task with three commonly used feature selection schemes information gain chi square and correlation coefficient based on the multinomial naive bayes classification approach we have conducted empirical experiments with four open source components from eclipse and mozilla the experimental results show that these three feature selection schemes can further improve the predication performance in over half the cases 2012 ieee defect reports feature selection performance evaluation severity prediction estimating software maintenance effort from use cases an industrial case study software maintenance effort constitutes a major portion of the software lifecycle effort its estimation is vital for successful project planning and strategic resource allocation in this paper we conduct and report an industrial case study in this field the data set was collected from an industrial software process management tool qone formerly softpm the methodology proposed provides corresponding guidance for effort estimation in software evolutionary projects that employ use cases in capturing maintenance requirements and the model constructed using the linear regression analysis and validated by the leave one out cross validation provides an effort prediction for the future maintenance of the project the analysis results indicate that the methodology can be applied at an early stage of the project life cycle and provides a good tradeoff among simplicity early estimating and accuracy in one estimate 2011 ieee effort estimation estimate model requirement elaboration software maintenance use case impactscale quantifying change impact to predict faults in large software systems in software maintenance both product metrics and process metrics are required to predict faults effectively however process metrics cannot be always collected in practical situations to enable accurate fault prediction without process metrics we define a new metric impactscale impactscale is the quantified value of change impact and the change propagation model for impactscale is characterized by probabilistic propagation and relation sensitive propagation to evaluate impactscale we predicted faults in two large enterprise systems using the effort aware models and poisson regression the results showed that adding impactscale to existing product metrics increased the number of detected faults at 10 effort loc by over 50 impactscale also improved the predicting model using existing product metrics and dependency network measures 2011 ieee a software dp defects prediction model based on svm support vector machine software defects prediction can help raise the effectiveness and efficiency of testing activities by constructing predictive classification models from static code attributes which can identify software modules with a higher than usual probability of defects our aim is to find the best performance predictive classification model through introducing svm into dp sections 1 through 4 of the full paper explain our svm dp model and its application to analyzing the 13 data sets of nasa metrics data program mdp sections 1 through 4 are entitled iterative and incremental prediction model svm dp section 1 benchmarking data sets and code metrics section 2 effectiveness indicators section 3 experimental method and analysis of test results section 4 experimental results presented in table 4 and figs 4 through 7 and their analysis show preliminarily the effectiveness of our svm dp model analysis classification of information codes symbols data mining defects defects prediction dp efficiency evaluation experiments functions iterative methods maintenance models probability software engineering software metrics suppor vector machine svm empirical validation of human factors in predicting issue lead time in open source projects context software developers often spend a significant portion of their resources resolving submitted evolution issue reports classification or prediction of issue lead time is useful for prioritizing evolution issues and supporting human resources allocation in software maintenance however the predictability of issue lead time is still a research gap that calls for more empirical investigation aim in this paper we empirically assess different types of issue lead time prediction models using human factor measures collected from issue tracking systems method we conduct an empirical investigation of three active open source projects a machine learning based classification and statistical univariate and multivariate analyses are performed results the accuracy of classification models in ten fold cross validation varies from 75 56 to 91 the r 2 value of linear multivariate regression models ranges from 0 29 to 0 60 correlation analysis confirms the effectiveness of collaboration measures such as the number of stakeholders and number of comments in prediction models the measures of assignee past performance are also an effective indicator of issue lead time conclusions the results indicate that the number of stakeholders and average past issue lead time are important variables in constructing prediction models of issue lead time however more variables should be explored to achieve better prediction performance copyright 2011 acm bug lead time bug prediction bug triage classification model empirical study issue lead time issue resolution time regression model using data mining techniques for time estimation in software maintenance measuring and estimating are fundamental activities for the success of any project in the software maintenance realm the lack of maturity or even a low level of interest in adopting effective maintenance techniques and related metrics has been pointed out as an important cause for the high costs involved in this paper data mining techniques are applied to provide a sound estimation for the time required to accomplish a maintenance task based on real world data regarding maintenance requests some regression models are built to predict the time required for each maintenance data on the team skill and the maintenance characteristics are mapped into values that predict better time estimations in comparison to the one predicted by the human expert a particular finding from this research is that the time prediction provided by a human expert works as an inductive bias that improves the overall prediction accuracy of the models copyright 2011 inderscience enterprises ltd data mining informal reasoning metrics software maintenance neural network based effort prediction model for maintenance projects one of the most critical requirements of high maturity practices is the development of valid and usable prediction models process performance model ppm for quantitatively managing the outcome of a process multiple regression analysis is a tool generally used for model building over the last few years artificial neural networks have received a great deal of attention as prediction and classification tools they have been applied successfully in diverse fields as data analysis tools here we explore the applicability of neural network models for bug fix effort prediction in corrective maintenance project and present our findings 2011 springer verlag neural networks process performance model comparing fine grained source code changes and code churn for bug prediction a significant amount of research effort has been dedicated to learning prediction models that allow project managers to efficiently allocate resources to those parts of a software system that most likely are bug prone and therefore critical prominent measures for building bug prediction models are product measures e g complexity or process measures such as code churn code churn in terms of lines modified lm and past changes turned out to be significant indicators of bugs however these measures are rather imprecise and do not reflect all the detailed changes of particular source code entities during maintenance activities in this paper we explore the advantage of using fine grained source code changes scc for bug prediction scc captures the exact code changes and their semantics down to statement level we present a series of experiments using different machine learning algorithms with a dataset from the eclipse platform to empirically evaluate the performance of scc and lm the results show that scc outperforms lm for learning bug prediction models 2011 acm code churn nonlinear regression prediction models software bugs source code changes an empirical analysis of metrics to predict thesoftware defect fix effort software defect fix effort is an important software process metric that plays a critical role in software quality assurance it is defined as the effort required in person hours to fix a defect it can assist to focus on planning of effort duration and staff and hence costs to deliver the project on time during development as well as maintenance phase but the prediction of software defect fix effort in person hours has long been a complex problem attracting lot of researchers attention the present paper reports on an empirical study which aims to construct defect fix effort estimation model for a large object oriented system we use a set of product and process metrics as input variables to predict the fix time effort personhours software metrics based approach used to build quality model has been used both the univariate and the multivariate regression analysis have been conducted on the data set defect fix effort process metrics product metrics regression analysis proceedings 2011 15th european conference on software maintenance and reengineering csmr 2011 the proceedings contain 55 papers the topics discussed include process mining software repositories ranking refactoring suggestions based on historical volatility investigating the use of lexical information for software system clustering on the benefits of planning and grouping software maintenance requests code clone detection on specialized pdgs with heuristics analyzing term weighting schemes for labeling software clusters building domain specific dictionaries of verb object relation from source code factbase and decomposition generation software deployment activities and challenges a case study of four software product companies a uml profile and tool support for evolutionary requirements engineering reducing maintenance effort through software operation knowledge an eclectic empirical evaluation comparing mining algorithms for predicting the severity of a reported bug and on the utility of a defect prediction model during hw sw integration on the utility of a defect prediction model during hw sw integration testing a retrospective case study testing is an important and cost intensive part of the software development life cycle defect prediction models try to identify error prone components so that these can be tested earlier or more in depth and thus improve the cost effectiveness during testing such models have been researched extensively but whether and when they are applicable in practice is still debated the applicability depends on many factors and we argue that it cannot be analyzed without a specific scenario in mind in this paper we therefore present an analysis of the utility for one case study based on data collected during the hardware software integration test of a system from the avionic domain an analysis of all defects found during this phase reveals that more than half of them are not identifiable by a codebased defect prediction model we then investigate the predictive performance of different prediction models for the remaining defects the small ratio of defective instances results in relatively poor performance our analysis of the cost effectiveness then shows that the prediction model is not able to outperform simple models which order files either randomly or by lines of code hence in our setup the application of defect prediction models does not offer any advantage in practice 2011 ieee defect prediction models empirical case study software metrics towards an estimation model for software maintenance costs today there is no best practise method available to effectively estimate the maintenance costs of historically grown large scale software landscapes most cost estimation models are either not generalizable due to highly specialized scenarios or too abstract to be implemented in practice in this paper we introduce a multi level approach to create transparency estimate costs realistically based on current spending and establish a method for sustainable cost control at the heart of our approach is the deduction of meaningful indicators for estimating current and future maintenance efforts we present the first version of a statistical cost estimation model being implemented at deutsche post mail as a baseline for contract negotiations with providers 2011 ieee case study cost estimation prediction models regression models software maintenance software measurement metric selection for software defect prediction real world software systems are becoming larger more complex and much more unpredictable software systems face many risks in their life cycles software practitioners strive to improve software quality by constructing defect prediction models using metric feature selection techniques finding faulty components in a software system can lead to a more reliable final system and reduce development and maintenance costs this paper presents an empirical study of six commonly used filter based software metric rankers and our proposed ensemble technique using rank ordering of the features mean or median applied to three large software projects using five commonly used learners the classification accuracy was evaluated in terms of the auc area under the roc receiver operating characteristic curve performance metric results demonstrate that the ensemble technique performed better overall than any individual ranker and also possessed better robustness the empirical study also shows that variations among rankers learners and software projects significantly impacted the classification outcomes and that the ensemble method can smooth out performance 2011 world scientific publishing company defect prediction ensemble technique software metric selection software quality classification fine grained incremental learning and multi feature tossing graphs to improve bug triaging software bugs are inevitable and bug fixing is a difficult expensive and lengthy process one of the primary reasons why bug fixing takes so long is the difficulty of accurately assigning a bug to the most competent developer for that bug kind or bug class assigning a bug to a potential developer also known as bug triaging is a labor intensive time consuming and faultprone process if done manually moreover bugs frequently get reassigned to multiple developers before they are resolved a process known as bug tossing researchers have proposed automated techniques to facilitate bug triaging and reduce bug tossing using machine learning based prediction and tossing graphs while these techniques achieve good prediction accuracy for triaging and reduce tossing paths they are vulnerable to several issues outdated training sets inactive developers and imprecise singleattribute tossing graphs in this paper we improve triaging accuracy and reduce tossing path lengths by employing several techniques such as refined classification using additional attributes and intra fold updates during training a precise ranking function for recommending potential tossees in tossing graphs and multi feature tossing graphs we validate our approach on two large software projects mozilla and eclipse covering 856 259 bug reports and 21 cumulative years of development we demonstrate that our techniques can achieve up to 83 62 prediction accuracy in bug triaging moreover we reduce tossing path lengths to 1 5 2 tosses for most bugs which represents a reduction of up to 86 31 compared to original tossing paths our improvements have the potential to significantly reduce the bug fixing effort especially in the context of sizable projects with large numbers of testers and developers 2010 ieee proceedings 2010 ieee international conference on software maintenance icsm 2010 the proceedings contain 83 papers the topics discussed include using clone detection to identify bugs in concurrent software revisiting common bug prediction findings using effort aware models conversion of fast inter procedural static analysis to model checking cost drivers of software corrective maintenance an empirical study in two companies automatic test case selection and generation for regression testing of composite service based on extensible bpel flow graph software modularization operators automatically repairing test cases for evolving method declarations magister quality assurance of magic applications for software developers and end users improved size and effort estimation models for software maintenance log filtering and interpretation for root cause analysis history sensitive recovery of product line features and automatic verification of loop invariants using commercial off the shelf business intelligence software tools to support aircraft and automated test system maintenance environments the purpose of this paper is to provide information about the benefits using commercial off the shelf cots business intelligence software tools to support aircraft and automated test system maintenance environments aircraft and automated test system parametric and maintenance warehouse based data can be shared and used for predictive data mining exploitation which will enable better decision support for war fighters and back shop maintenance when utilizing common industry business intelligence predictive modeling processes engineering designers can create initial business intelligence aircraft and automated test system maintenance environment engineering cluster models this is a process of grouping together engineering data that have similar aggregate patterns by using these engineering cluster models produced earlier to develop and build more accurate predictive models predictive algorithms are utilized to make use of the cluster results to improve predictive accuracy common industry business intelligence decision trees and neural network models are developed to determine which algorithm produces the most accurate models as measured by comparing predictions with actual values over the testing set after an initial mining structure and mining model is built specifying the input and predictable attributes the analyst can easily add other mining models cots business intelligence software tools provide for a more cost effective support and predictive role for war fighter support personnel in a time of decreased defense spending having access to applicable engineering data at the time of need will decrease troubleshooting time on production aircraft and back shop maintenance increase the ability of the technical user to better understand the diagnostics reduce ambiguities which drive false removals of system components decrease misallocated spares and maintain increase knowledge management 2010 ieee a quantitative approach to software maintainability prediction software maintainability is one important aspect in the evaluation of software evolution of a software product due to the complexity of tracking maintenance behaviors it is difficult to accurately predict the cost and risk of maintenance after delivery of software products in an attempt to address this issue quantitatively software maintainability is viewed as an inevitable evolution process driven by maintenance behaviors given a health index at the time when a software product are delivered a hidden markov model hmm is used to simulate the maintenance behaviors shown as their possible occurrence probabilities and software metrics is the measurement of the quality of a software product and its measurement results of a product being delivered are combined to form the health index of the product the health index works as a weight on the process of maintenance behavior over time when the occurrence probabilities of maintenance behaviors reach certain number which is reckoned as the indication of the deterioration status of a software product the product can be regarded as being obsolete longer the time better the maintainability would be 2010 ieee hidden markov model software maintainability software metrics bug forecast a method for automatic bug prediction in this paper we present an approach and a toolset for automatic bug prediction during software development and maintenance the toolset extends the columbus source code quality framework which is able to integrate into the regular builds analyze the source code calculate different quality attributes like product metrics and bad code smells and monitor the changes of these attributes the new bug forecast toolset connects to the bug tracking and version control systems and assigns the reported and fixed bugs to the source code classes from the past it then applies machine learning methods to learn which values of which quality attributes typically characterized buggy classes based on this information it is able to predict bugs in current and future versions of the classes the toolset was evaluated on an industrial software system developed by a large software company called evosoft we studied the behavior of the toolset through a 1 5 year development period during which 128 snapshots of the software were analyzed the toolset reached an average bug prediction precision of 72 reaching many times 100 we concentrated on high precision as the primary purpose of the toolset is to aid software developers and testers in pointing out the classes which contain bugs with a high probability and keep the number of false positives relatively low 2010 springer verlag berlin heidelberg bad code smells bug prediction machine learning software product metrics predicting re opened bugs a case study on the eclipse project bug fixing accounts for a large amount of the software maintenance resources generally bugs are reported fixed verified and closed however in some cases bugs have to be re opened re opened bugs increase maintenance costs degrade the overall user perceived quality of the software and lead to unnecessary rework by busy practitioners in this paper we study and predict re opened bugs through a case study on the eclipse project we structure our study along 4 dimensions 1 the work habits dimension e g the weekday on which the bug was initially closed on 2 the bug report dimension e g the component in which the bug was found 3 the bug fix dimension e g the amount of time it took to perform the initial fix and 4 the team dimension e g the experience of the bug fixer our case study on the eclipse platform 3 0 project shows that the comment and description text the time it took to fix the bug and the component the bug was found in are the most important factors in determining whether a bug will be re opened based on these dimensions we create decision trees that predict whether a bug will be re opened after its closure using a combination of our dimensions we can build explainable prediction models that can achieve 62 9 precision and 84 5 recall when predicting whether a bug will be re opened 2010 ieee studies on the use of keel software for intelligent analyzing of bridge load bearing capacity the goal of this paper is to work out an effective computer method to assess the bridge structure condition that is expressed by means of load bearing parameter considering typical bridge damages in order to find the proper algorithm a survey of available regression methods using the keel software was made keel software is used to assess evolutionary algorithms for common data mining problems selected methods were tested using real data that were provided by experts and achieved results were compared to results given by the mybride expert tool all considered algorithms base either on the neural network or fuzzy rule learning approach first stage of the experiment focused on selecting a group of best methods taking into account mean squared error mse standard deviation stdev and mean response time mrt for each algorithm verified methods use normalized input data where the considered range is the interval 0 1 after this initial selection rbfn cor ga fuzzy gap fuzzy sap and thrift algorithms were promoted for further tests the aim of experiment s second stage was to compare previously selected methods to th e mybride expert tool in order this comparison was possible the structure of input data sets was reorganized to use the same datasets as the expert tool was tested on also root mean squared error rmse and mean absolute error mae are used to determine the quality of possible predictions the comparison between tested methods and the expert tool shows that only two out of five considered algorithms using non normalized input data give results that are similar to expert tool predictions these methods are fuzzy gap and thrift with mean rmse for all data sets respectively 12 4 and 7 1 whereas the reference rmse value is 3 5 presented results are very useful for planned implementation of intelligent tools for the bridge management systems 2010 taylor francis group london assessing object oriented software systems based on change impact simulation software changes are inevitable during software evolution and software change propagation intensely increases the difficulty of software maintenance in this paper we regard various change requirements as the combination of a series of atomic change requirement software modifications which are used to satisfy the atomic change requirement are considered as modifications of a random selected initial element and the ripple effects caused by the modifications then we propose a method for assessing the change propagation of object oriented software based on change impact simulation firstly the method to construct a software change propagation model and related software metric indicators are presented the rationale of this approach is that different strength of coupling has different probability of change propagation secondly an approach for getting the probability of change propagation setting is provided which is based on change history obtained from software version repositories and different dependence relationships finally the proposed systematic approach has been evaluated on a multi version medium sized open source object namely apache ant is a java based build tool which indicates the simplicity and rationality of our approach 2010 ieee change impact change propagation probability simulation approach software change using data mining in optimisation of building energy consumption and thermal comfort management performance monitoring using wireless sensors is now common practice in building operation and maintenance and generates a large amount of building specific data however it is difficult for occupants owners and operators to explore such data and understand underlying patterns this is especially true in buildings which involve complex interactions such as ventilation solar gains internal gains and thermal mass performance monitoring requires collecting data concerning energy consumption and ambient environmental conditions to model and optimise buildings energy consumption this paper details the use of data mining techniques in understanding building energy performance of geothermal solar and gas burning energy systems the paper is part of an outgoing research into optimisation of building performance under hybrid energy regimes the objective of the research presented in this paper is to predict comfort levels based on the heating ventilating and air conditioning hvac system performance and external environmental conditions a c4 5 classification methodology is used to analyse a combination of internal and external ambient conditions the mining algorithms are used to determine comfort constraints and the influence of external conditions on a building s internal user comfort to test the performance of classification and its use in prediction different offices one to the south and the other to the north of the building are used classification rules being developed are analysed for their application to modify control algorithms and to apply results to generalise hybrid system performance the results of this study can be generalised for an entire building or a set of buildings under a single energy network subject to the same constraints classification data mining energy hvac multidimension performance sensors applications of support vector mathine and unsupervised learning for predicting maintainability using object oriented metrics importance of software maintainability is increasing leading to development of new sophisticated techniques this paper presentes the applications of support vector machine and unsupervised learning in software maintainability prediction using object oriented metrics in this paper the software maintainability predictor is performed the dependent variable was maintenance effort the independent variable were five oo metrics decided clustering technique the results showed that the mean absolute relative error mare was 0 218 of the predictor therefore we found that svm and clustering technique were useful in constructing software maintainability predictor novel predictor can be used in the similar software developed in the same environment 2010 ieee maintainability object oriented metrics predict support vector machine unsupervised learning software change classification using hunk metrics change management is a challenging task in software maintenance changes are made to the software during its whole life some of these changes introduce errors in the code which result in failures software changes are composed of small code units called hunks dispersed in source code files in this paper we present a technique for classifying software changes based on hunk metrics we classify individual hunks as buggy or bug free thus we provide an approach for bug prediction at the smallest level of granularity we introduce a set of hunk metrics and build classification models based on these metrics classification models are built using logistic regression and random forests we evaluated the performance of our approach on 7 open source software projects our classification approach can classify hunks as buggy or bug free with 81 percent accuracy 77 percent buggy hunk precision and 67 percent buggy hunk recall on average most of the hunk metrics are significant predictors of bugs but the set of significant metrics varies among different projects counselor a data mining based time estimation for software maintenance measuring and estimating are fundamental activities for the success of any project in the software maintenance realm the lack of maturity or even a low level of interest in adopting effective maintenance techniques and related metrics have been pointed out as an important cause for the high costs involved in this paper data mining techniques are applied to provide a sound estimation for the time required to accomplish a maintenance task based on real world data regarding maintenance requests some regression models are built to predict the time required for each maintenance data on the team skill and the maintenance characteristics are mapped into values that predict better time estimations in comparison to the one predicted by the human expert a particular finding from this research is that the time prediction provided by a human expert works as an inductive bias that improves the overall prediction accuracy 2009 springer berlin heidelberg data mining informal reasoning software maintenance empirical evaluation of hunk metrics as bug predictors reducing the number of bugs is a crucial issue during software development and maintenance software process and product metrics are good indicators of software complexity these metrics have been used to build bug predictor models to help developers maintain the quality of software in this paper we empirically evaluate the use of hunk metrics as predictor of bugs we present a technique for bug prediction that works at smallest units of code change called hunks we build bug prediction models using random forests which is an efficient machine learning classifier hunk metrics are used to train the classifier and each hunk metric is evaluated for its bug prediction capabilities our classifier can classify individual hunks as buggy or bug free with 86 accuracy 83 buggy hunk precision and 77 buggy hunk recall we find that history based and change level hunk metrics are better predictors of bugs than code level hunk metrics springer verlag berlin heidelberg 2009 bug predictor code metrics empirical software engineering hunk metrics software faults fault detection and prediction in an open source software project software maintenance continues to be a time and resource intensive activity any efforts that help to address the maintenance bottleneck within the software lifecycle are welcome one area where such efforts are useful is in the identification of the parts of the source code of a software system that are most likely to contain faults and thus require changes we have carried out an empirical study where we have merged information from the cvs repository and the bugzilla database for an open source software project to investigate whether or not parts of the source code are faulty the number and severity of faults and the number and types of changes associated with parts of the system we present an analysis of this information showing that pareto s law holds and we evaluate the usefulness of the chidamber and kemerer metrics for identifying the fault prone classes in the system analysed acm 2009 empirical study fault prediction metrics open source regression software quality an empirical investigation of filter attribute selection techniques for software quality classification attribute selection is an important activity in data preprocessing for software quality modeling and other data mining problems the software quality models have been used to improve the fault detection process finding faulty components in a software system during early stages of software development process can lead to a more reliable final product and can reduce development and maintenance costs it has been shown in some studies that prediction accuracy of the models improves when irrelevant and redundant features are removed from the original data set in this study we investigated four filter attribute selection techniques automatic hybrid search ahs rough sets rs kolmogorov smirnov ks and probabilistic search ps and conducted the experiments by using them on a very large telecommunications software system in order to evaluate their classification performance on the smaller subsets of attributes selected using different approaches we built several classification models using five different classifiers the empirical results demonstrated that by applying an attribution selection approach we can build classification models with an accuracy comparable to that built with a complete set of attributes moreover the smaller subset of attributes has less than 15 percent of the complete set of attributes therefore the metrics collection model calibration model validation and model evaluation times of future software development efforts of similar systems can be significantly reduced in addition we demonstrated that our recently proposed attribute selection technique ks outperformed the other three attribute selection techniques 2009 ieee increasing diversity natural language measures for software fault prediction while challenging the ability to predict faulty modules of a program is valuable to a software project because it can reduce the cost of software development as well as software maintenance and evolution three language processing based measures are introduced and applied to the problem of fault prediction the first measure is based on the usage of natural language in a program s identifiers the second measure concerns the conciseness and consistency of identifiers the third measure referred to as the qalp score makes use of techniques from information retrieval to judge software quality the qalp score has been shown to correlate with human judgments of software quality two case studies consider the language processing measures applicability to fault prediction using two programs one open source one proprietary linear mixed effects regression models are used to identify relationships between defects and the measures results while complex show that language processing measures improve fault prediction especially when used in combination overall the models explain one third and two thirds of the faults in the two case studies consistent with other uses of language processing the value of the three measures increases with the size of the program module considered 2009 elsevier inc all rights reserved code comprehension empirical software engineering fault prediction information retrieval linear regression models an expert system for determining candidate software classes for refactoring in the lifetime of a software product development costs are only the tip of the iceberg nearly 90 of the cost is maintenance due to error correction adaptation and mainly enhancements as lehman and belady lehman m m belady l a 1985 program evolution processes of software change academic press professional state that software will become increasingly unstructured as it is changed one way to overcome this problem is refactoring refactoring is an approach which reduces the software complexity by incrementally improving internal software quality our motivation in this research is to detect the classes that need to be rafactored by analyzing the code complexity we propose a machine learning based model to predict classes to be refactored we use weighted naïve bayes with infogain heuristic as the learner and we conducted experiments with metric data that we collected from the largest gsm operator in turkey our results showed that we can predict 82 of the classes that need refactoring with 13 of manual inspection effort on the average 2008 elsevier ltd all rights reserved naive bayes refactor prediction refactoring software metrics software maintenance severity prediction for object oriented systems as the majority of faults are found in a few of its modules so there is a need to investigate the modules that are affected severely as compared to other modules and proper maintenance need to be done in time especially for the critical applications as neural networks which have been already applied in software engineering applications to build reliability growth models predict the gross change or reusability metrics neural networks are non linear sophisticated modeling techniques that are able to model complex functions neural network techniques are used when exact nature of input and outputs is not known a key feature is that they learn the relationship between input and output through training in this present work various neural network based techniques are explored and comparative analysis is performed for the prediction of level of need of maintenance by predicting level severity of faults present in nasa s public domain defect dataset the comparison of different algorithms is made on the basis of mean absolute error root mean square error and accuracy values it is concluded that generalized regression networks is the best algorithm for classification of the software components into different level of severity of impact of the faults the algorithm can be used to develop model that can be used for identifying modules that are heavily affected by the faults neural network software faults software metric application of treenet in predicting object oriented software maintainability a comparative study there is an increasing interest in more accurate prediction of software maintainability in order to better manage and control software maintenance recently treenet has been proposed as a novel advance in data mining that extends and improves the cart classification and regression trees model using stochastic gradient boosting this paper empirically investigates whether the treenet model yields improved prediction accuracy over the recently published object oriented software maintainability prediction models multivariate adaptive regression splines multivariate linear regression support vector regression artificial neural network and regression tree the results indicate that improved or at least competitive prediction accuracy has been achieved when applying the treenet model 2009 ieee software metrics suites for project landscapes many software metrics have been proposed over the past decades selecting a small custom suite of metrics is desirable for quality assessment and defect prediction in industrial practice since developers cannot easily cope with dozens of metrics in large software architectures structurally similar projects are subject to similar defect conditions and can be analysed by the same metrics suite a large java application developed at continentale insurance contains structurally similar subprojects for different insurance branches health life accident car property these branches are integrated in the it architecture in a technically uniform way we investigated which subsets of metrics are predictive but uncorrelated with each other and compared the results for structurally similar projects 2009 ieee experienced report on assessing and evaluating change impact analysis through a framework and associated models in our evolving computing environment with heterogenously distributed information systems products are continuously modified and changed during this process a change to one part will in most cases results in changes to other parts therefore in design and redesign for customization predicting this change presents a significant challenge changes are required to fix faults or to improve or update products this paper reports on the development of a change impact analysis factor adaptation model a fault and failure assumption model and the implementation of a generic change propagation framework for evaluating and assessing utility service provisioning in a grid service environment while implementing the framework data was collected for a period of 3 years which helped in predicting an immediate year the obtained results from our pre diction shows the framework its associated models and bayesian statistics as satisfying the criteria for a significant prediction accuracy in evaluating and assessing the effect of a change of service in a grid environment when compared to an unreported regression method bayesian statistics change impact analysis grid environment service maintenance service provisioning software metrics software maintenance severity prediction with soft computing approach as the majority of faults are found in a few of its modules so there is a need to investigate the modules that are affected severely as compared to other modules and proper maintenance need to be done on time especially for the critical applications in this paper we have explored the different predictor models to nasa s public domain defect dataset coded in perl programming language different machine learning algorithms belonging to the different learner categories of the weka project including mamdani based fuzzy inference system and neuro fuzzy based system have been evaluated for the modeling of maintenance severity or impact of fault severity the results are recorded in terms of accuracy mean absolute error mae and root mean squared error rmse the results show that neuro fuzzy based model provides relatively better prediction accuracy as compared to other models and hence can be used for the maintenance severity prediction of the software 2009 waset org accuracy fuzzy mae neuro fuzzy rmse software faults software metrics using random test selection to gain confidence in modified software this paper presents a method that addresses two practical issues concerning the use of random test selection for regression testing the number of random samples needed from the test suite to provide reliable results and the confidence levels of the predictions made by the random samples the method applies the chernoff bound which has been applied in various randomized algorithms to compute the error bound for random test selection the paper presents three example applications based on the method for regression testing the main benefits of the method are that it requires no distribution information about the test suite from which the samples are taken and the computation of the confidence level is independent of the size of the test suite the paper also presents the results of an empirical evaluation of the technique on a set of c programs which have been used in many testing experiments along with three of the gcc compilers the results demonstrate the effectiveness of the method and show its potential for regression testing on real world large scale applications 2008 ieee refactoring prediction using class complexity metrics in the lifetime of a software product development costs are only the tip of the iceberg nearly 90 of the cost is maintenance due to error correction adoptation and mainly enhancements as belady and lehman lehman and belady 1985 state that software will become increasingly unstructured as it is changed one way to overcome this problem is refactoring refactoring is an approach which reduces the software complexity by incrementally improving internal software quality our motivation in this research is to detect the classes that need to be rafactored by analyzing the code complexity we propose a machine learning based model to predict classes to be refactored we use weighted naïve bayes with infogain heuristic as the learner and we conducted experiments with metric data that we collected from the largest gsm operator in turkey our results showed that we can predict 82 of the classes that need refactoring with 13 of manual inspection effort on the average defect prediction naïve bayes refactor prediction refactoring software metrics weighted naïve bayes optimal successive mappings for classification in this paper we propose a new method of designing and constructing good mappings defined by kernel functions for classification task called optimal successive mappings osm kernel methods such as support vector machines svm could not provide satisfactory classification accuracy on some complicated data sets which are still not linearly separable in feature space it means kernels designed only by tuning kernel parameters cannot adapt well to classification of complicated data sets unlike tuning parameters osm learns and designs its kernel from training data through a sequence of two mappings and optimizing a criteria function after feature mapping of osm data in the feature space appear not only linearly separable but also intra class compact and extra class separate as the problem of optimizing the criteria function reduces to a generalized eigenvalue problem osm possesses non iterative and low complex properties comparative experiments demonstrate the effectiveness of our method 2008 ieee data classification kernel methods kernel optimization prediction of maintainability using software complexity analysis an extended frt in this paper a method is proposed for predicting software maintainability prediction of maintainability of a product is done by using its code complexity here a sample of 4 products is taken into consideration and both the absolute and relative complexity assessment are made over it the process of measuring the code complexity is done at testing phase we employ the fuzzy repertory table frt technique to acquire the necessary domain knowledge of testers from which the software complexity analysis is made regression analysis is then used to predict maintainability from the product s code complexity 2008 ieee fuzzy repertory table regression analysis software maintainability trapezoid number aode for source code metrics for improved software maintainability software metrics are collected at various phases of the whole software development process in order to assist in monitoring and controlling the software quality however software quality control is complicated because of the complex relationship between these metrics and the attributes of a software development process to solve this problem many excellent techniques have been introduced into software maintainability domain in this paper we propose a novel classification method aggregating one dependence estimators aode to support and enhance our understanding of software metrics and their relationship to software quality experiments show that performance of aode is much better than eight traditional classification methods and it is a promising method for software quality prediction furthermore we present a symmetrical uncertainty su based feature selection method to reduce source code metrics taking part in classification make these classifiers more efficient and keep their performances not undermined meanwhile our empirical study shows the promising capability of su for selecting relevant metrics and preserving original performances of the classifiers 2008 ieee an overview and case study of a statistical regression testing method for software maintenance we propose a statistical regression testing method for evaluating the reliability of software as part of the software maintenance process maintenance procedures take up more than half the time of the software development process in addition software reliability is an important factor in determining the dependability of a product regression tests are performed in order to conserve or improve software reliability as part of the software maintenance process however existing systematic testing methods based on regression tests are not necessarily appropriate for evaluating software reliability the statistical regression testing method is a means for compensating for the flaws of such existing methods in this method a model of how the user makes use of the software is defined by means of a markov chain this is known as the usage model and then test cases are generated at random according to a probability distribution based on this usage model in this paper we perform experiments applying the proposed method to a small scale client server program and demonstrate that the proposed method can be implemented in addition we clarify the effects and issues that may be anticipated when applying the method and establish how it may be used in practice 2007 wiley periodicals inc automated tests markov chains regression tests software reliability evaluation statistical tests modification analysis support at the requirements level modification analysis is part of most maintenance processes and includes among other activities early prediction of potential change impacts feasibility studies cost estimation etc existing impact analysis and regression testing techniques being source code based require at least some understanding of the system implementation in this research we present a novel approach that combines ucm with fca to assist decision makers in supporting modification analysis at the requirements level our approach provides support for determining the potential modification and re testing effort associated with a change without the need to analyze or comprehend source code we demonstrate the applicability of our approach on a telephony system case study copyright 2007 acm change impact analysis formal concept analysis regression testing use case maps on the prediction of the evolution of libre software projects libre free open source software development is a complex phenomenon many actors core developers casual contributors bug reporters patch submitters users etc in many cases volunteers interact in complex patterns without the constrains of formal hierarchical structures or organizational ties understanding this complex behavior with enough detail to build explanatory models suitable for prediction is an open challenge and few results have been published to date in this area therefore statistical non explanatory models such as the traditional regression model have a clear role and have been used in some evolution studies our pmposal goes in this direction but using a model that we have found more useful time series analysis data available from the source code management repository is used to compute the size of the software over its past life using this information to estimate the future evolution of the pmject in this paper we present this methodology and apply it to three large projects showing how in these cases predictions are more accurate than regression models and precise enough to estimate with little error their near future evolutions 2007 ieee modification analysis support at the requirements level modification analysis is part of most maintenance processes and includes among other activities early prediction of potential change impacts feasibility studies cost estimation etc existing impact analysis and regression testing techniques being source code based require at least some understanding of the system implementation in this research we present a novel approach that combines ucm with fca to assist decision makers in supporting modification analysis at the requirements level our approach provides support for determining the potential modification and re testing effort associated with a change without the need to analyze or comprehend source code we demonstrate the applicability of our approach on a telephony system case study 2007 acm isbn change impact analysis formal concept analysis regression testing use case maps a model to predict anti regressive effort in open source software accumulated changes on a software system are not uniformly distributed some elements are changed more often than others for optimal impact the limited time and effort for complexity control called anti regressive work should be applied to the elements of the system which are frequently changed and are complex based on this we propose a maintenance guidance model mgm which is tested against real world data mgm takes into account several dimensions of complexity size structural complexity and coupling results show that maintainers of the eight open source systems studied tend in general to prioritize their anti regressive work in line with the predictions given by our mgm even though divergences also exist mgm offers a history based alternative to existing approaches to the identification of elements for anti regressive work most of which use static code characteristics only 2007 ieee anti regressive work coupling empirical studies maintenance mccabe cyclomatic complexity metrics open source software evolution predicting object oriented software maintainability using multivariate adaptive regression splines accurate software metrics based maintainability prediction can not only enable developers to better identify the determinants of software quality and thus help them improve design or coding it can also provide managers with useful information to help them plan the use of valuable resources in this paper we employ a novel exploratory modeling technique multiple adaptive regression splines mars to build software maintainability prediction models using the metric data collected from two different object oriented systems the prediction accuracy of the mars models are evaluated and compared using multivariate linear regression models artificial neural network models regression tree models and support vector models the results suggest that for one system mars can predict maintainability more accurately than the other four typical modeling techniques and that for the other system mars is as accurate as the best modeling technique 2006 elsevier inc all rights reserved maintainability multiple adaptive regression splines object oriented prediction life cycle management distributed web based software development with evolutionary programming and stochastic optimization this article is an extension of the work presented earlier which compared and analyzed the economics of alternative maintenance plans the previous approach has been improved by an application of a genetic algorithm the system is introduced as a distributed web based software application the methodology based on combining evolutionary programming with monte carlo simulations is described and illustrated by a numerical example involving analysis of the optimal timing of new investments for refurbishment of a large steam generating unit copyright kth 2006 evolutionary algorithms genetic algorithms life cycle management stochastic optimization software defect prediction using regression via classification in this paper we apply a machine learning approach to the problem of estimating the number of defects called regression via classification rvc rvc initially automatically discretizes the number of defects into a number of fault classes then learns a model that predicts the fault class of a software system finally rvc transforms the class output of the model back into a numeric prediction this approach includes uncertainty in the models because apart from a certain number of faults it also outputs an associated interval of values within which this estimate lies with a certain confidence to evaluate this approach we perform a comparative experimental study of the effectiveness of several machine learning algorithms in a software dataset the data was collected by pekka forselious and involves applications maintained by a bank of finland 2006 ieee maximum profit mining and its application in software development while most software defects i e bugs are corrected and tested as part of the lengthy software development cycle enterprise software vendors often have to release software products before all reported defects are corrected due to deadlines and limited resources a small number of these defects will be escalated by customers and they must be resolved immediately by the software vendors at a very high cost in this paper we develop an escalation prediction ep system that mines historic defect report data and predict the escalation risk of the defects for maximum net profit more specifically we first describe a simple and general framework to convert the maximum net profit problem to cost sensitive learning we then apply and compare several well known cost sensitive learning approaches for ep our experiments suggest that the cost sensitive decision tree is the best method for producing the highest positive net profit and comprehensible results the ep system has been deployed successfully in the product group of an enterprise software vendor copyright 2006 acm cost sensitive learning data mining escalation prediction an application of bayesian network for predicting object oriented software maintainability as the number of object oriented software systems increases it becomes more important for organizations to maintain those systems effectively however currently only a small number of maintainability prediction models are available for object oriented systems this paper presents a bayesian network maintainability prediction model for an object oriented software system the model is constructed using object oriented metric data in li and henry s datasets which were collected from two different object oriented systems prediction accuracy of the model is evaluated and compared with commonly used regression based models the results suggest that the bayesian network model can predict maintainability more accurately than the regression based models for one system and almost as accurately as the best regression based model for the other system 2005 elsevier b v all rights reserved bayesian network maintainability object oriented systems regression regression tree quantifying software architectures an analysis of change propagation probabilities software architectures are an emerging discipline in software engineering as they play a central role in many modern software development paradigms quantifying software architectures is an important research agenda as it allows software architects to subjectively assess quality attributes and rationalize architecture related decisions in this paper we discuss the attribute of change propagation probability which reflects the likelihood that a change that arises in one component of the architecture propagates i e mandates changes to other components 2005 ieee change propagation probability software architectures software engineering software maintenance software metrics maintainability prediction a regression analysis of measures of evolving systems in order to build predictors of the maintainability of evolving software we first need a means for measuring maintainability as well as a training set of software modules for which the actual maintainability is known this paper describes our success at building such a predictor numerous candidate measures for maintainability were examined including a new compound measure two datasets were evaluated and used to build a maintainability predictor the resulting model maintainability prediction model mainpredmo was validated against three held out datasets we found that the model possesses predictive accuracy of 83 accurately predicts the maintainability of 83 of the modules a variant of mainpredmo also with accuracy of 83 is offered for interested researchers 2005 ieee comparing fault proneness estimation models over the last years software quality has become one of the most important requirement in the development of systems fault proneness estimation could play a key role in quality control of software products in this area much effort has been spent in defining metrics and identifying models for system assessment using these metrics to assess which parts of the system are more fault proneness is of primary importance this paper reports a research study begun with the analysis of more than 100 metrics and aimed at producing suitable models for fault proneness estimation and prediction of software modules files the objective has been to find a compromise between the fault proneness estimation rate and the size of the estimation model in terms of number of metrics used in the model itself to this end two different methodologies have been used compared and some synergies exploited the methodologies were the logistic regression and the discriminant analyses the corresponding models produced for fault proneness estimation and prediction have been based on metrics addressing different aspects of computer programming the comparison has produced satisfactory results in terms of fault proneness prediction the produced models have been cross validated by using data sets derived from source codes provided by two application scenarios 2005 ieee cross validation empirical validation fault proneness estimation fault proneness prediction maintenance reducing corrective maintenance effort considering module s history a software package evolves in time through various maintenance release steps whose effectiveness depends mainly on the number of faults left in the modules the testing phase is therefore critical to discover these faults the purpose of this paper is to show a criterion to estimate an optimal repartition of available testing time among software modules in a maintenance release in order to achieve this objective we have used fault prediction techniques based both on classical complexity metrics and an additional innovative factor related to the module s age in terms of release this method can actually diminish corrective maintenance effort while assuring a high reliability for the delivered software 2005 ieee economical aspects of software evolution software metrics tools and enabling technologies for evolution predicting the probability of change in object oriented systems of all merits of the object oriented paradigm flexibility is probably the most important in a world of constantly changing requirements and the most striking difference compared to previous approaches however it is rather difficult to quantify this aspect of quality this paper describes a probabilistic approach to estimate the change proneness of an object oriented design by evaluating the probability that each class of the system will be affected when new functionality is added or when existing functionality is modified it is obvious that when a system exhibits a large sensitivity to changes the corresponding design quality is questionable the extracted probabilities of change can be used to assist maintenance and to observe the evolution of stability through successive generations and identify a possible saturation level beyond which any attempt to improve the design without major refactoring is impossible the proposed model has been evaluated on two multiversion open source projects the process has been fully automated by a java program while statistical analysis has proved improved correlation between the extracted probabilities and actual changes in each of the classes in comparison to a prediction model that relies simply on past data 2005 ieee object oriented design methods object oriented programming product metrics quality analysis and evaluation assessing effort estimation models for corrective maintenance through empirical studies we present an empirical assessment and improvement of the effort estimation model for corrective maintenance adopted in a major international software enterprise our study was composed of two phases in the first phase we used multiple linear regression analysis to construct effort estimation models validated against real data collected from five corrective maintenance projects the model previously adopted by the subject company used as predictors the size of the system being maintained and the number of maintenance tasks while this model was not linear we show that a linear model including the same variables achieved better performances also we show that greater improvements in the model performances can be achieved if the types of the different maintenance tasks is taken into account in the second phase we performed a replicated assessment of the effort prediction models built in the previous phase on a new corrective maintenance project conducted by the subject company on a software system of the same type as the systems of the previous maintenance projects the data available for the new project were finer grained according to the indications devised in the first study this allowed to improve the confidence in our previous empirical analysis by confirming most of the hypotheses made the new data also provided other useful indications to better understand the maintenance process of the company in a quantitative way 2004 elsevier b v all rights reserved corrective software maintenance cost estimation models experimentation management measurement a model for corrective maintenance time prediction using neural network this paper presents the application of neural networks in predicting corrective maintenance time the paper aims to establish the viability of the usage of feedforward artificial neural network for predicting the time needed to correct errors associated with changes made to the software during maintenance for this purpose a neural model using four software measures is proposed the software measures used are complexity measures like cyclomatic complexity acc readability of source code rsc documentation quality doq and understandability of software uos the neural network used is sigmoidal feedforward artificial neural network it is found that the neural network of the type used can act as an efficient predictor of corrective maintenance time corrective maintenance sffanns software measures prediction of software development faults in pl sql files using neural network models database application constitutes one of the largest and most important software domains in the world some classes or modules in such applications are responsible for database operations structured query language sql is used to communicate with database middleware in these classes or modules it can be issued interactively or embedded in a host language this paper aims to predict the software development faults in pl sql files using sql metrics based on actual project defect data the sql metrics are empirically validated by analyzing their relationship with the probability of fault detection across pl sql files sql metrics were extracted from oracle pl sql code of a warehouse management database application system the faults were collected from the journal files that contain the documentation of all changes in source files the result demonstrates that these measures may be useful in predicting the fault concerning with database accesses in our study general regression neural network and ward neural network are used to evaluate the capability of this set of sql metrics in predicting the number of faults in database applications 2003 elsevier b v all rights reserved neural network software metrics software prediction structured query language metrics jackson networks and markov processes for resource allocation modeling measuring the productivities of engineers is one difficult task in software engineering the popular technique used to evaluate the quality and productivity of engineers during software development is that supervisors and managers have to monitor engineers and keep track their work activities every day managers might use their common sense that can be unfair and incorrect to evaluate performance of engineers hence two new models encouraged fault repair model and discouraged fault arrival model that we propose solve these issues a closed form derived from encouraged fault repair model is used to measure the productivities of engineers and a closed form derived discouraged fault arrival model to evaluate the performance of testers the limitation the fault repair rate luong 2001 is a constant is removed in the new model engineering resource configuration model that is a jackson network model with markov chain configuration factors causing workforce available or unavailable are included into the model jackson network model markov processes resource allocation model software maintenance model software queueing model software reliability modelling the process of incoming problem reports on released software products for big software developing companies it is important to know the amount of problems of a new software product that are expected to be reported in a period after the date of release on a weekly basis for each of a number of past releases weekly data are present on the number of such reports based on the type of data that is present we construct a stochastic model for the weekly number of problems to be reported the non parametric maximum likelihood estimator for the crucial model parameter the intensity of an inhomogeneous poisson process is defined moreover the expectation maximization algorithm is described which can be used to compute this estimate the method is illustrated using simulated data copyright 2004 john wiley sons ltd corrective software maintenance em algorithm inhomogeneous poisson process isotonic regression unimodality an information theory approach to studying software evolution information theory defines and mathematically characterizes several entropy measures including the well known shannon entropy based on these mathematical characterizations this paper rigorously chooses the entropy metrics which are suitable for measuring the information content of software systems by treating a software system as an information source the probabilities required for computing the entropy metrics are obtained using an empirical distribution of the symbols emitted from the source a preliminary case study is performed on a well known compiler implemented in a procedural programming language the results of the study validate the utility of the proposed metrics as indicators of software information content the metrics also represent a family of measures which satisfies different measurement requirements a second case study is performed on an object oriented graphics and multimedia software in addition to further validating the metrics the case study demonstrates their use in monitoring the evolution of a large software system it is shown that studying the evolution of the modules within the system reveals the different module behaviors that are concealed when the full system is viewed at the top level this understanding can be very useful to maintenance engineers because it guides them to the modules that undergo considerable information changes entropy information theory software engineering software evolution software maintenance software metrics analysis of software maintenance data using multi technique approach amount of software engineering data that is accumulated by software companies grows with enormous speed this data is a source of knowledge about different activities related to software development and maintenance many different techniques and tools have been developed and proposed for extracting knowledge and representing it in forms understandable by people these techniques are based on different principles and they process data differently this paper illustrates a multi technique approach to analysis of data a detailed case study of analyzing software maintenance data is presented different models are built analyzed and evaluated the first model is a bayesian network the second is a set of if then rules extracted from the data and the third one is built using a decision tree the emphasis of the analysis is put on two aspects how the models support understanding of a process represented by the data and how good prediction capabilities these models have determinants of software volatility a field study although technology advances have provided new tools for maintaining software maintenance costs remain the largest component of software life cycle cost a basic factor claimed to be one of the driving factors in the cost of maintenance is software volatility the objective of this research is to investigate the relationship between certain software attributes and software volatility in this study software volatility refers to the frequency or number of enhancements per unit of application over a specified time normalized however this metric is divided by the number of source lines of code sloc to obtain a measure that takes into account the size of the software application the research model is built on previous research concerning software volatility three factors are examined to determine their influence on software volatility normalized for sloc age software complexity and software complexity normalized for sloc in addition we introduce the notion that mean time between software enhancements moderates the relationship of age complexity and complexity normalized for sloc with software volatility a field study at a major corporation allowed for the collection of data from a 13 year time period these data are used to empirically test the hypotheses presented in this study as a moderator variable mean time between enhancements significantly contributes to the explanatory power of a prediction model for software volatility adjusted for sloc software administrators may wish to use the proposed model in their decision making plans to control for software costs copyright 2003 john wiley sons ltd development costs lines of code maintenance costs software metrics software quality software volatility an analogy based approach for predicting design stability of java classes 2003 ieee predicting stability in object oriented oo software i e the ease with which a software item evolves while preserving its design is a key feature for software maintenance in fact a well designed oo software must be able to evolve without violating the compatibility among versions provided that no major requirement reshuffling occurs stability like most quality factors is a complex phenomenon and its prediction is a real challenge we present an approach which relies on the case based reasoning cbr paradigm and thus overcomes the handicap of insufficient theoretical knowledge on stability the approach explores structural similarities between classes expressed as software metrics to guess their chances of becoming unstable in addition our stability model binds its value to the impact of changing requirements i e the degree of class responsibilities increase between versions quantified as the stress factor as a result the prediction mechanism favours the stability values for classes having strong structural analogies with a given test class as well as a similar stress impact our predictive model is applied on a testbed made up of the classes from four major version of the java api java object oriented modeling predictive models q factor software design software maintenance software metrics stability stress testing mining the maintenance history of a legacy software system a considerable amount of system maintenance experience can be found in bug tracking and source code configuration management systems data mining and machine learning techniques allow one to extract models from past experience that can be used in future predictions by mining the software change record one can therefore generate models that can be used in future maintenance activities in this paper we present an example of such a model that represents a relation between pairs of files and show how it can be extracted from the software update records of a real world legacy system we show how different sources of data can be used to extract sets of features useful in describing this model as well as how results are affected by these different feature sets and their combinations our best results were obtained from text based features i e those extracted from words in the problem reports as opposed to syntactic structures in the source code effort estimation for corrective software maintenance this paper reports on an empirical study aiming at constructing cost estimation models for corrective maintenance projects data available were collected from five maintenance projects currently carried out by a large software enterprise the resulting models constructed using multivariate linear regression techniques allow to estimate the costs of a project conducted according to the adopted maintenance processes model performances on future observations were achieved by taking into account different corrective maintenance task typologies each affecting the effort in a different way and assessed by means of a cross validation which guarantees a nearly unbiased estimate of the prediction error the constructed models are currently adopted by the subject company copyright 2002 acm d 2 8 software engineering management software maintenance cost estimation experimentation management measurement association analysis of software measures software measures metrics provide software engineers with an important means of quantifying essential features of software products and software processes such as software reliability maintenance reusability and alike software measures interact between themselves some of them may be deemed redundant software measures are used to construct detailed prediction models the objective of this study is to pursue an association analysis of software measures by revealing dependencies associations between them more specifically the introduced association analysis is carried out at the local level by studying dependencies between information granules of the software measures this approach is contrasted with a global level such as e g regression analysis we discuss the role of information granules as meaningful conceptual entities that facilitate analysis and give rise to a user friendly highly transparent environment associations fuzzy sets information granules rule based models rules software measures using code metrics to predict maintenance of legacy programs a case study this paper presents an empirical study on the correlation of simple code metrics and maintenance necessities the goal of the work is to provide a method for the estimation of maintenance in the initial stages of outsourcing maintenance projects when the maintenance contract is being prepared and there is very little available information on the software to be maintained the paper shows several positive results related with the mentioned goal code metrics maintenance prediction outsourcing modeling development effort in object oriented systems using design properties in the context of software cost estimation system size is widely taken as a main driver of system development effort but other structural design properties such as coupling cohesion and complexity have been suggested as additional cost factors in this paper using effort data from an object oriented development project we empirically investigate the relationship between class size and the development effort for a class and what additional impact structural properties such as class coupling have on effort this paper proposes a practical repeatable and accurate analysis procedure to investigate relationships between structural properties and development effort this is particularly important as it is necessary as for any empirical study to be able to replicate the analysis reported here more specifically we use poisson regression and regression trees to build cost prediction models from size and design measures and use these models to predict system development effort we also investigate a recently suggested technique to combine regression trees with regression analysis which aims at building more accurate models results indicate that fairly accurate predictions of class effort can be made based on simple measures of the class interface size alone mean mres below 30 percent effort predictions at the system level are even more accurate as using bootstrapping the estimated 95 percent confidence interval for mres is 3 to 23 percent but more sophisticated coupling and cohesion measures do not help to improve these predictions to a degree that would be practically significant however the use of hybrid models combining poisson regression and cart regression trees clearly improves the accuracy of the models as compared to using poisson regression alone cost estimation empirical validation object oriented measurement evolution of fault prone components in legacy systems a case study prediction of problematic software components is an important activity today for many organisations as they manage and maintain their legacy systems and the maintenance problems they cause this means that there is a need for methods and models to identify problematic components we apply a model for classification of software components as green yellow and red according to the number of times they required corrective maintenance over successive releases further we apply principal component analysis pca and box plots to investigate the causes for the code decay and structural changes the case study includes five system releases and 80 software components a large set of non fault prone components was identified the system did not contain any large structural changes which was indicated by the pca and the box plots most of the changes were small fault corrections a number of design improvement suggestions had been identified by the developers but not carried out overall the model was successful in identifying the most problematic components and provided information about the evolution of the system the strength of the model was the combination of both a short term view and a long term view fault prone components legacy systems prediction software evolution tracking empirical studies of a prediction model for regression test selection regression testing is an important activity that can account for a large proportion of the cost of software maintenance one approach to reducing the cost of regression testing is to employ a selective regression testing technique that 1 chooses a subset of a test suite that was used to test the software before the modifications then 2 uses this subset to test the modified software selective regression testing techniques reduce the cost of regression testing if the cost of selecting the subset from the test suite together with the cost of running the selected subset of test cases is less than the cost of rerunning the entire test suite rosenblum and weyuker recently proposed coverage based predictors for use in predicting the effectiveness of regression test selection strategies using the regression testing cost model of leung and white rosenblum and weyuker demonstrated the applicability of these predictors by performing a case study involving 31 versions of the kornshell to further investigate the applicability of the rosenblum weyuker rw predictor additional empirical studies have been performed the rw predictor was applied to a number of subjects using two different selective regression testing tools dejavu and testtube these studies support two conclusions first they show that there is some variability in the success with which the predictors work and second they suggest that these results can be improved by incorporating information about the distribution of modifications it is shown how the rw prediction model can be improved to provide such an accounting regression test selection regression testing selective retest software maintenance controlling overfitting in software quality models experiments with regression trees and classification in this day of faster cheaper better release cycles software developers must focus enhancement efforts on those modules that need improvement the most predictions of which modules are likely to have faults during operations is an important tool to guide such improvement efforts during maintenance tree based models are attractive because they readily model nonmonotonic relationships between a response variable and predictors however tree based models are vulnerable to overfitting where the model reflects the structure of the training data set too closely even though a model appears to be accurate on training data if overfitted it may be much less accurate when applied to a current data set to account for the severe consequences of misclassifying fault prone modules our measure of overfitting is based on expected costs of misclassification rather than the total number of misclassifications in this paper we apply a regression tree algorithm in the s plus system to classification of software modules by application of our classification rule that accounts for the preferred balance between misclassificalion rates we conducted a case study of a very large legacy telecommunications system and investigated two parameters of the regression tree algorithm we found here that minimum deviance was strongly related to overfitting and can be used to control it but the effect of minimum node size on overfitting is ambiguous classification fault prone modules overfitting regression trees s plus software maintenance software metrics software reliability modelling fault proneness statistically over a sequence of releases a case study many of today s software systems evolve through a series of releases that add new functionality and features in addition to the results of corrective maintenance as the systems evolve over time it is necessary to keep track of and manage their problematic components our focus is to track system evolution and to react before the systems become difficult to maintain to do the tracking we use a method based on a selection of statistical techniques in the case study we report here that had historical data available primarily on corrective maintenance we apply the method to four releases of a system consisting of 130 components in each release components are classified as fault prone if the number of defect reports written against them are above a certain threshold the outcome from the case study shows stabilizing principal components over the releases and classification trees with lower thresholds in their decision nodes also the variables used in the classification trees decision nodes are related to changes in the same files the discriminant functions use more variables than the classification trees and are more difficult to interpret box plots highlight the findings from the other analyses the results show that for a context of corrective maintenance principal components analysis together with classification trees are good descriptors for tracking software evolution classification trees code decay corrective maintenance fault prediction fault prone components principal components analysis software evolution maintainability model for industrial software systems using design level metrics software maintenance is a time consuming and expensive phase of a software product s life cycle this paper investigates the use of software design metrics to statistically estimate the maintainability of large software systems and to identify error prone modules a methodology for assessing evaluating and selecting software metrics for predicting software maintainability is presented in addition a linear prediction model based on a minimal set of design level software metrics is proposed the model is evaluated by applying it to industrial software systems software quality maintenance model we develop a quality control and prediction model for improving the quality of software delivered by development to maintenance this model identifies modules that require priority attention during development and maintenance the model also predicts during development the quality that will be delivered to maintenance we show that it is important to perform a marginal analysis when making a decision about how many metrics to include in a discriminant function if many metrics are added at once the contribution of individual metrics is obscured also the marginal analysis provides an effective rule for deciding when to stop adding metrics we also show that certain metrics are dominant in their effects on classifying quality and that additional metrics are not needed to increase the accuracy of classification data from the space shuttle flight software are used to illustrate the model process an examination of the effects of requirements changes on software maintenance releases requirements are the foundation of the software release process they provide the basis for estimating costs and schedules as well as developing design and testing specifications when requirements have been agreed on by both clients and maintenance management then adding to deleting from or modifying those existing requirements during the execution of the software maintenance process impacts the maintenance cost schedule and quality of the resulting product the basic problem is not the changing in itself but rather the inadequate approaches for dealing with changes in a way that minimizes and communicates the impact to all stakeholders using data collected from one organization on 44 software releases spanning seven products this paper presents two quantitative techniques for dealing with requirements change in a maintenance environment first exploratory data analysis helps one to understand the sources frequency and types of changes being made second a regression model helps managers communicate the cost and schedule effects of changing requirements to clients and other release stakeholders these two techniques can help an organization provide a focus for management action during the software maintenance process copyright 1999 john wiley sons ltd maintenance productivity maintenance release management requirements taxonomy requirements volatility risk schedule prediction which software modules have faults which will be discovered by customers software is the medium for implementing increasingly sophisticated features during the maintenance phase as successive releases are developed software quality models can predict which software modules are likely to have faults that hill be discovered by customers such models are key components of a system such as enhanced measurement for early risk assessment of latent defects emerald it is a sophisticated system of decision support tools used by software designers and managers at nortel to assess risk and improve software quality and reliability of legacy software systems this paper reports an approach to software quality modelling that is suitable for industrial systems such as emerald we conducted a case study of a large legacy telecommunications system in the maintenance phase to predict whether each module will be considered fault prone the case study is distinctive in the following respects 1 fault prone modules were defined in terms of faults discovered by customers which represent only a small fraction of the modules in the system 2 we developed models based on software product and process metrics that can make useful predictions at the end of the coding phase and at the time of release 3 the modelling approach is suitable for very large systems we anticipate that refinements of this case study s models will be incorporated into emerald a similar approach could be taken for other systems copyright 1999 john wiley sons ltd fault prone modules logistic regression process metrics product metrics software metrics software quality code decay analysis of legacy software through successive releases prediction of problematic software components is an important activity today for many organizations as they manage their legacy systems and the maintenance problems they cause this means that there is a need for methods and models to identify troublesome components we apply a model for classification of software components as green yellow and red according to the number of times they required corrective maintenance over successive releases further we apply a principal component and box plot analysis to investigate the causes for the code decay and try to characterize the releases the case study includes eight releases and 130 software components the outcome indicates a large number of healthy components as well as a small set of troublesome components requiring extensive repair repeatedly the analysis characterizes the releases and indicates that it is the relationship between components that causes many of the problems proceedings of the 1998 9th international symposium on software reliability engineering issre the proceedings contains 44 papers topics discussed include reliability prediction and estimation process improvement and software maintenance software testability standards reliability modelling and validation test planning and automation software reliability engineering mobile code security computer simulation fault diagnosis optimization code defect classification and metrics safety critical software and fault injection methods of measuring software reuse for the prediction of maintenance effort a major difficulty in evaluating the costs of reusing software is determining the amount of reused software artefacts in systems determining the amount of reuse in a system is important for software maintenance because reused software is likely to need less corrective maintenance than newly developed software reusing software can also decrease costs of testing and integration in this paper we describe some practical techniques for measuring the amount of software reuse using simple tools the goal is to provide accurate assessment of the state of existing software systems in order to assess quality and deploy resources efficiently the techniques for software developed on the unix system use the standard utilities find and diff software developed under configuration management by the sees utility is measured using the prs utility techniques are also given for measurement of the amount of reuse in software that was developed on personal computers each of the methods was used for reuse measurement at nasa s goddard space flight center the methods were applied to measure reuse in moderately large software systems used for ground centre control of spacecraft configuration management cots reuse categories reuse factors software metrics software reuse development and application of composite complexity models and a relative complexity metric in a software maintenance environment a great deal of effort is now being devoted to the study analysis prediction and minimization of expected software maintenance cost long before software is delivered to users or customers it had been estimated that on the average the effort spent on software maintenance is as costly as the effort spent on all other software stages ways to alleviate software maintenance complexity and high costs should originate in software design two aspects of maintenance deserve attention protocols for locating and rectifying defects and ensuring that no new defects are introduced in the development phase of the software process and development of protocols for increasing the quality and reducing the costs associated with modification enhancement and upgrading of software this article focuses on the second aspect and puts forward newly developed parsimonious models and a relative complexity metric for complexity measurement of software that were used to rank the modules in the system relative to each other significant success was achieved by use of the models and relative metric to identify maintenance prone modules 1995 experience with the accuracy of software maintenance task effort prediction models this paper reports experience from the development and use of eleven different software maintenance effort prediction models the models were developed applying regression analysis neural networks and pattern recognition and the prediction accuracy was measured and compared for each model type the most accurate predictions were achieved applying models based on multiple regression and on pattern recognition we suggest the use of prediction models as instruments to support the expert estimates and to analyse the impact of the maintenance variables on the maintenance process and product we believe that the pattern recognition based models evaluated i e the prediction models based on the optimized set reduction method show potential for such use 1995 ieee cost models neural network pattern recognition prediction models regression software maintenance software measurement object oriented metrics that predict maintainability software metrics have been studied in the procedural paradigm as a quantitative means of assessing the software development process as well as the quality of software products several studies have validated that various metrics are useful indicators of maintenance effort in the procedural paradigm however software metrics have rarely been studied in the object oriented paradigm very few metrics have been proposed to measure object oriented systems and the proposed ones have not been validated this research concentrates on several object oriented software metrics and the validation of these metrics with maintenance effort in two commercial systems statistical analyses of a prediction model incorporating 10 metrics were performed in addition a more compact model with fewer metrics is presented 1993